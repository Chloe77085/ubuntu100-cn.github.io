<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-Application Development and Execution Guide/Framework-Driven AI Sample Execution/llama_cpp" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.9.2">
<title data-rh="true">Llama.cpp | Ubuntu Manual CN</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://ubuntu100-cn.github.io/ubuntu100-cn.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://ubuntu100-cn.github.io/ubuntu100-cn.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://ubuntu100-cn.github.io/ubuntu100-cn.github.io/docs/Application Development and Execution Guide/Framework-Driven AI Sample Execution/llama_cpp"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Llama.cpp | Ubuntu Manual CN"><meta data-rh="true" name="description" content="您可以使用 llama.cpp 在 Dragonwing 开发板上运行各种大语言模型（LLM）和视觉语言模型（VLM）。在 llama.cpp 下运行的模型在GPU而非NPU上运行。您可以通过 GENIE 在NPU上运行部分模型子集。"><meta data-rh="true" property="og:description" content="您可以使用 llama.cpp 在 Dragonwing 开发板上运行各种大语言模型（LLM）和视觉语言模型（VLM）。在 llama.cpp 下运行的模型在GPU而非NPU上运行。您可以通过 GENIE 在NPU上运行部分模型子集。"><link data-rh="true" rel="icon" href="/ubuntu100-cn.github.io/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://ubuntu100-cn.github.io/ubuntu100-cn.github.io/docs/Application Development and Execution Guide/Framework-Driven AI Sample Execution/llama_cpp"><link data-rh="true" rel="alternate" href="https://ubuntu100-cn.github.io/ubuntu100-cn.github.io/docs/Application Development and Execution Guide/Framework-Driven AI Sample Execution/llama_cpp" hreflang="en"><link data-rh="true" rel="alternate" href="https://ubuntu100-cn.github.io/ubuntu100-cn.github.io/docs/Application Development and Execution Guide/Framework-Driven AI Sample Execution/llama_cpp" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"应用开发与执行指南","item":"https://ubuntu100-cn.github.io/ubuntu100-cn.github.io/docs/Application Development and Execution Guide/"},{"@type":"ListItem","position":2,"name":"Llama.cpp","item":"https://ubuntu100-cn.github.io/ubuntu100-cn.github.io/docs/Application Development and Execution Guide/Framework-Driven AI Sample Execution/llama_cpp"}]}</script><link rel="alternate" type="application/rss+xml" href="/ubuntu100-cn.github.io/blog/rss.xml" title="Ubuntu Manual CN RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/ubuntu100-cn.github.io/blog/atom.xml" title="Ubuntu Manual CN Atom Feed"><link rel="stylesheet" href="/ubuntu100-cn.github.io/assets/css/styles.a2f89494.css">
<script src="/ubuntu100-cn.github.io/assets/js/runtime~main.3e999906.js" defer="defer"></script>
<script src="/ubuntu100-cn.github.io/assets/js/main.667d0c46.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",t||(window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light")),document.documentElement.setAttribute("data-theme-choice",t||"system")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><link rel="preload" as="image" href="/ubuntu100-cn.github.io/img/logo.svg"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/ubuntu100-cn.github.io/"><div class="navbar__logo"><img src="/ubuntu100-cn.github.io/img/logo.svg" alt="My Site Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/ubuntu100-cn.github.io/img/logo.svg" alt="My Site Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">My Site</b></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/ubuntu100-cn.github.io/docs/Get familiar">Tutorial</a><a class="navbar__item navbar__link" href="/ubuntu100-cn.github.io/blog">Blog</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><a href="https://github.com/facebook/docusaurus" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/ubuntu100-cn.github.io/docs/Get familiar"><span title="设备规格" class="linkLabel_WmDU">设备规格</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/ubuntu100-cn.github.io/docs/intro"><span title="Tutorial Intro" class="linkLabel_WmDU">Tutorial Intro</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/ubuntu100-cn.github.io/docs/set-up-your-device"><span title="设备设置" class="linkLabel_WmDU">设备设置</span></a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" href="/ubuntu100-cn.github.io/docs/Update-Software/"><span title="更新软件" class="categoryLinkLabel_W154">更新软件</span></a><button aria-label="Collapse sidebar category &#x27;更新软件&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ubuntu100-cn.github.io/docs/Update-Software/3.1.upgrade-ubuntu"><span title="将Canonical Ubuntu升级到最新版本" class="linkLabel_WmDU">将Canonical Ubuntu升级到最新版本</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ubuntu100-cn.github.io/docs/Update-Software/3.2.Flash-using-Qualcomm-Launcher"><span title="使用Qualcomm Launcher刷写Canonical Ubuntu 24.04" class="linkLabel_WmDU">使用Qualcomm Launcher刷写Canonical Ubuntu 24.04</span></a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" href="/ubuntu100-cn.github.io/docs/peripherals-and-interfaces/"><span title="外设与接口" class="categoryLinkLabel_W154">外设与接口</span></a><button aria-label="Collapse sidebar category &#x27;外设与接口&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ubuntu100-cn.github.io/docs/peripherals-and-interfaces/1.40-pin-ls-connector"><span title="40 pin 连接器" class="linkLabel_WmDU">40 pin 连接器</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ubuntu100-cn.github.io/docs/peripherals-and-interfaces/usb"><span title="USB" class="linkLabel_WmDU">USB</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ubuntu100-cn.github.io/docs/peripherals-and-interfaces/csi"><span title="摄像头串行接口（CSI）" class="linkLabel_WmDU">摄像头串行接口（CSI）</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ubuntu100-cn.github.io/docs/peripherals-and-interfaces/hdmi"><span title="HDMI OUT" class="linkLabel_WmDU">HDMI OUT</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ubuntu100-cn.github.io/docs/peripherals-and-interfaces/displayport"><span title="DisplayPort" class="linkLabel_WmDU">DisplayPort</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ubuntu100-cn.github.io/docs/peripherals-and-interfaces/wifi-and-bluetooth"><span title="Wi-Fi 与蓝牙" class="linkLabel_WmDU">Wi-Fi 与蓝牙</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ubuntu100-cn.github.io/docs/peripherals-and-interfaces/fan"><span title="风扇" class="linkLabel_WmDU">风扇</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ubuntu100-cn.github.io/docs/peripherals-and-interfaces/led"><span title="LED" class="linkLabel_WmDU">LED</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ubuntu100-cn.github.io/docs/peripherals-and-interfaces/ethernet"><span title="以太网" class="linkLabel_WmDU">以太网</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ubuntu100-cn.github.io/docs/peripherals-and-interfaces/rtc-battery"><span title="RTC 电池接口" class="linkLabel_WmDU">RTC 电池接口</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ubuntu100-cn.github.io/docs/peripherals-and-interfaces/m.2-key-m-connector"><span title="M.2 Key M 接口" class="linkLabel_WmDU">M.2 Key M 接口</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ubuntu100-cn.github.io/docs/peripherals-and-interfaces/audio"><span title="音频" class="linkLabel_WmDU">音频</span></a></li></ul></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/ubuntu100-cn.github.io/docs/Prebuilt Sample Applications"><span title="预构建示例应用程序" class="linkLabel_WmDU">预构建示例应用程序</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/ubuntu100-cn.github.io/docs/QDemo"><span title="Qualcomm® Qdemo" class="linkLabel_WmDU">Qualcomm® Qdemo</span></a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--active" href="/ubuntu100-cn.github.io/docs/Application Development and Execution Guide/"><span title="应用开发与执行指南" class="categoryLinkLabel_W154">应用开发与执行指南</span></a><button aria-label="Collapse sidebar category &#x27;应用开发与执行指南&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="true" tabindex="0" href="/ubuntu100-cn.github.io/docs/Application Development and Execution Guide/Building AI Models/edge_impulse"><span title="构建 AI 模型" class="categoryLinkLabel_W154">构建 AI 模型</span></a></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ubuntu100-cn.github.io/docs/Application Development and Execution Guide/Building AI Models/edge_impulse"><span title="Edge Impulse" class="linkLabel_WmDU">Edge Impulse</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ubuntu100-cn.github.io/docs/Application Development and Execution Guide/Building AI Models/qualcomm_ai_hub"><span title="Qualcomm® AI Hub" class="linkLabel_WmDU">Qualcomm® AI Hub</span></a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" role="button" aria-expanded="true" tabindex="0" href="/ubuntu100-cn.github.io/docs/Application Development and Execution Guide/Framework-Driven AI Sample Execution/litert_tflite"><span title="基于框架的 AI 示例运行" class="categoryLinkLabel_W154">基于框架的 AI 示例运行</span></a></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ubuntu100-cn.github.io/docs/Application Development and Execution Guide/Framework-Driven AI Sample Execution/litert_tflite"><span title="LiteRT / TFLite" class="linkLabel_WmDU">LiteRT / TFLite</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ubuntu100-cn.github.io/docs/Application Development and Execution Guide/Framework-Driven AI Sample Execution/onnx"><span title="ONNX" class="linkLabel_WmDU">ONNX</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/ubuntu100-cn.github.io/docs/Application Development and Execution Guide/Framework-Driven AI Sample Execution/llama_cpp"><span title="Llama.cpp" class="linkLabel_WmDU">Llama.cpp</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ubuntu100-cn.github.io/docs/Application Development and Execution Guide/Framework-Driven AI Sample Execution/genie"><span title="Genie" class="linkLabel_WmDU">Genie</span></a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="true" tabindex="0" href="/ubuntu100-cn.github.io/docs/Application Development and Execution Guide/IMSDK/Update JSON Config"><span title="Qualcomm® IM SDK" class="categoryLinkLabel_W154">Qualcomm® IM SDK</span></a></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ubuntu100-cn.github.io/docs/Application Development and Execution Guide/IMSDK/Update JSON Config"><span title="配置示例应用程序" class="linkLabel_WmDU">配置示例应用程序</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ubuntu100-cn.github.io/docs/Application Development and Execution Guide/IMSDK/Customize-Sample"><span title="定制现有的示例应用程序" class="linkLabel_WmDU">定制现有的示例应用程序</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ubuntu100-cn.github.io/docs/Application Development and Execution Guide/IMSDK/Develop-app"><span title="开发你的第一个应用程序" class="linkLabel_WmDU">开发你的第一个应用程序</span></a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="true" tabindex="0" href="/ubuntu100-cn.github.io/docs/Application Development and Execution Guide/Robotics-Sample-Applications/Robotics Sample Applications"><span title="Qualcomm® Intelligent Robotics (QIR) SDK" class="categoryLinkLabel_W154">Qualcomm® Intelligent Robotics (QIR) SDK</span></a></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ubuntu100-cn.github.io/docs/Application Development and Execution Guide/Robotics-Sample-Applications/Robotics Sample Applications"><span title="机器人示例应用程序" class="linkLabel_WmDU">机器人示例应用程序</span></a></li></ul></li></ul></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/ubuntu100-cn.github.io/docs/Technologies"><span title="技术概览" class="linkLabel_WmDU">技术概览</span></a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/ubuntu100-cn.github.io/docs/Troubleshooting/troubleshooting"><span title="Troubleshooting" class="categoryLinkLabel_W154">Troubleshooting</span></a></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/ubuntu100-cn.github.io/docs/linux-kernel"><span title="Linux 内核" class="linkLabel_WmDU">Linux 内核</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/ubuntu100-cn.github.io/docs/datasheet"><span title="数据手册" class="linkLabel_WmDU">数据手册</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/ubuntu100-cn.github.io/docs/development-regulatory-notice"><span title="开发监管公告" class="linkLabel_WmDU">开发监管公告</span></a></li></ul></nav></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/ubuntu100-cn.github.io/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/ubuntu100-cn.github.io/docs/Application Development and Execution Guide/"><span>应用开发与执行指南</span></a></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">基于框架的 AI 示例运行</span></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">Llama.cpp</span></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>Llama.cpp</h1></header>
<p>您可以使用 <a href="https://github.com/ggml-org/llama.cpp" target="_blank" rel="noopener noreferrer" class="">llama.cpp</a> 在 Dragonwing 开发板上运行各种大语言模型（LLM）和视觉语言模型（VLM）。在 llama.cpp 下运行的模型在<em>GPU</em>而非<em>NPU上运行</em>。您可以通过 <a href="https://qc-ai-test.gitbook.io/qc-ai-test-docs/running-building-ai-models/genie" target="_blank" rel="noopener noreferrer" class="">GENIE</a> 在NPU上运行部分模型子集。</p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="构建-llamacpp">构建 llama.cpp<a href="#构建-llamacpp" class="hash-link" aria-label="Direct link to 构建 llama.cpp" title="Direct link to 构建 llama.cpp" translate="no">​</a></h2>
<p>您需要为 llama.cpp 构建一些依赖项。在开发板上打开终端，或建立 SSH 会话，然后执行以下操作：</p>
<p>1️⃣ 安装构建依赖项：</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">sudo apt update</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">sudo apt install -y cmake ninja-build curl libcurl4-openssl-dev</span><br></span></code></pre></div></div>
<p>2️⃣ 安装 OpenCL 头文件和 ICD 加载器库：</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">mkdir -p ~/dev/llm</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># Symlink the OpenCL shared library</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">sudo rm -f /usr/lib/libOpenCL.so</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">sudo ln -s /lib/aarch64-linux-gnu/libOpenCL.so.1.0.0 /usr/lib/libOpenCL.so</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># OpenCL headers</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">cd ~/dev/llm</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">git clone https://github.com/KhronosGroup/OpenCL-Headers</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">cd OpenCL-Headers</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">git checkout 5d52989617e7ca7b8bb83d7306525dc9f58cdd46</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">mkdir -p build &amp;&amp; cd build</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">cmake .. -G Ninja \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    -DBUILD_TESTING=OFF \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    -DOPENCL_HEADERS_BUILD_TESTING=OFF \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    -DOPENCL_HEADERS_BUILD_CXX_TESTS=OFF \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    -DCMAKE_INSTALL_PREFIX=&quot;$HOME/dev/llm/opencl&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">cmake --build . --target install</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># ICD Loader</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">cd ~/dev/llm</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">git clone https://github.com/KhronosGroup/OpenCL-ICD-Loader</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">cd OpenCL-ICD-Loader</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">git checkout 02134b05bdff750217bf0c4c11a9b13b63957b04</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">mkdir -p build &amp;&amp; cd build</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">cmake .. -G Ninja \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    -DCMAKE_BUILD_TYPE=Release \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    -DCMAKE_PREFIX_PATH=&quot;$HOME/dev/llm/opencl&quot; \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    -DCMAKE_INSTALL_PREFIX=&quot;$HOME/dev/llm/opencl&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">cmake --build . --target install</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># Symlink OpenCL headers</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">sudo rm -f /usr/include/CL</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">sudo ln -s ~/dev/llm/opencl/include/CL/ /usr/include/CL</span><br></span></code></pre></div></div>
<p>3️⃣ 使用 OpenCL 后端编译 llama.cpp：</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">cd ~/dev/llm</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># Clone repository</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">git clone https://github.com/ggml-org/llama.cpp</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">cd llama.cpp</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># We&#x27;ve tested this commit explicitly, you can try master if you want bleeding edge</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">git checkout f6da8cb86a28f0319b40d9d2a957a26a7d875f8c</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># Build</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">mkdir -p build</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">cd build</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">cmake .. -G Ninja \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    -DCMAKE_BUILD_TYPE=Release \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    -DBUILD_SHARED_LIBS=OFF \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    -DGGML_OPENCL=ON</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">ninja -j`nproc`</span><br></span></code></pre></div></div>
<p>4️⃣ 将 llama.cpp 路径添加到 PATH：</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">cd ~/dev/llm/llama.cpp/build/bin</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">echo &quot;&quot; &gt;&gt; ~/.bash_profile</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">echo &quot;# Begin llama.cpp&quot; &gt;&gt; ~/.bash_profile</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">echo &quot;export PATH=\$PATH:$PWD&quot; &gt;&gt; ~/.bash_profile</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">echo &quot;# End llama.cpp&quot; &gt;&gt; ~/.bash_profile</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">echo &quot;&quot; &gt;&gt; ~/.bash_profile</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># To use the llama.cpp files in your current session</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">source ~/.bash_profile</span><br></span></code></pre></div></div>
<p>5️⃣ 现在已完成 llama.cpp 的部署:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">llama-cli --version</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># ggml_opencl: selected platform: &#x27;QUALCOMM Snapdragon(TM)&#x27;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># ggml_opencl: device: &#x27;QUALCOMM Adreno(TM) 635 (OpenCL 3.0 Adreno(TM) 635)&#x27;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># ggml_opencl: OpenCL driver: OpenCL 3.0 QUALCOMM build: 0808.0.7 Compiler E031.49.02.00</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># ggml_opencl: vector subgroup broadcast support: true</span><br></span></code></pre></div></div>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="下载并量化模型">下载并量化模型<a href="#下载并量化模型" class="hash-link" aria-label="Direct link to 下载并量化模型" title="Direct link to 下载并量化模型" translate="no">​</a></h3>
<p>为了运行 GPU 加速的模型，你需要使用纯 4-bit 量化(<code>Q4_0</code>)模型，并且格式为 GGUF（llama.cpp 格式，<a href="https://github.com/ggml-org/llama.cpp/discussions/2948" target="_blank" rel="noopener noreferrer" class="">转换指南</a>)。可以选择已量化好的模型，或使用 <code>llama-quantize</code> 自行量化模型。例如，对于 Qwen2-1.5B-Instruct：</p>
<p>1️⃣从 HuggingFace 获取 fp16 格式的 <a href="https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF" target="_blank" rel="noopener noreferrer" class="">Qwen2-1.5B-Instruct</a> 并使用 <code>llama-quantize</code> 进行量化：</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain"># Download fp16 model</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">wget https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF/resolve/main/qwen2-1_5b-instruct-fp16.gguf</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># Quantize (pure Q4_0)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">llama-quantize --pure qwen2-1_5b-instruct-fp16.gguf qwen2-1_5b-instruct-q4_0-pure.gguf Q4_0</span><br></span></code></pre></div></div>
<p>2️⃣按照 llama.cpp 编译说明运行此模型。</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="使用-llama-cli-运行你的第一个-llm">使用 llama-cli 运行你的第一个 LLM<a href="#使用-llama-cli-运行你的第一个-llm" class="hash-link" aria-label="Direct link to 使用 llama-cli 运行你的第一个 LLM" title="Direct link to 使用 llama-cli 运行你的第一个 LLM" translate="no">​</a></h3>
<p>您现在可以通过<code>llama-cli</code> 运行LLM。它会自动将各层转移到 GPU：</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">llama-cli -m ./qwen2-1_5b-instruct-q4_0-pure.gguf -no-cnv --no-warmup -b 128 -c 2048 -s 11 -n 128 -p &quot;Knock knock, &quot; -fa off</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># ... You&#x27;ll see:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># load_tensors: offloaded 29/29 layers to GPU</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># ...</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># Knock knock, 11:59 pm ... rest of the story</span><br></span></code></pre></div></div>
<p>现在，您设备的 GPU 上已经运行有 LLM。</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="使用-llama-server-部署-llm">使用 llama-server 部署 LLM<a href="#使用-llama-server-部署-llm" class="hash-link" aria-label="Direct link to 使用 llama-server 部署 LLM" title="Direct link to 使用 llama-server 部署 LLM" translate="no">​</a></h3>
<p>接下来可以使用<code>llama-server</code>启动带有聊天界面的 Web 服务器，该服务器同时提供OpenAI兼容的会话补全API。</p>
<p>1️⃣ 首先，找到开发板的IP地址：</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">ifconfig | grep -Eo &#x27;inet (addr:)?([0-9]*\.){3}[0-9]*&#x27; | grep -Eo &#x27;([0-9]*\.){3}[0-9]*&#x27; | grep -v &#x27;127.0.0.1&#x27;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># ... Example:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># 192.168.1.253</span><br></span></code></pre></div></div>
<p>2️⃣ 通过以下方式启动服务器：</p>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">llama-server -m ./qwen2-1_5b-instruct-q4_0-pure.gguf --no-warmup -b 128 -c 2048 -s 11 -n 128 --host 0.0.0.0 --port 9876</span><br></span></code></pre></div></div>
<p>3️⃣ 在您的计算机上，打开 Web 浏览器并前往 <code>http://192.168.1.253:9876</code> （将 IP 地址替换为您在 1 中找到的 IP 地址）：</p>
<p><img decoding="async" loading="lazy" src="https://3580193864-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxM5xrbdbelLSl7uN8oac%2Fuploads%2Fgit-blob-1c1d0f018f51abf311522cf4f398bcc4b69fb102%2Fllamacpp1.png?alt=media" alt="" title="使用 llama-server 部署 LLM" class="img_ev3q">4️⃣ 您还可以通过 OpenAI 会话补全 API 以编程方式访问此服务器。例如，从 Python：<br>
<!-- -->a.创建一个新的虚拟环境 （venv） 并安装<code>requests</code>：</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">python3 -m venv .venv-chat</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">source .venv/bin/activate</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">pip3 install requests</span><br></span></code></pre></div></div>
<p>b. 创建一个新文件<code>chat.py</code>：</p>
<div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token keyword" style="color:#00009f">import</span><span class="token plain"> requests</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># if running from your own computer, replace localhost with the IP address of your development board</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">url </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;http://localhost:9876/v1/chat/completions&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">payload </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token string" style="color:#e3116c">&quot;messages&quot;</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">[</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token punctuation" style="color:#393A34">{</span><span class="token string" style="color:#e3116c">&quot;role&quot;</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;system&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;content&quot;</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;You are a helpful assistant.&quot;</span><span class="token punctuation" style="color:#393A34">}</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token punctuation" style="color:#393A34">{</span><span class="token string" style="color:#e3116c">&quot;role&quot;</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;user&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;content&quot;</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;Explain Qualcomm in one sentence.&quot;</span><span class="token punctuation" style="color:#393A34">}</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token punctuation" style="color:#393A34">]</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token string" style="color:#e3116c">&quot;temperature&quot;</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">0.7</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token string" style="color:#e3116c">&quot;max_tokens&quot;</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">200</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token punctuation" style="color:#393A34">}</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">response </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> requests</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">post</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">url</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> headers</span><span class="token operator" style="color:#393A34">=</span><span class="token punctuation" style="color:#393A34">{</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;Content-Type&quot;</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;application/json&quot;</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">}</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> json</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">payload</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">print</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">response</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">json</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">)</span><br></span></code></pre></div></div>
<p>c. 运行<code>chat.py</code>：</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">python3 chat.py</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># ...</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># {&#x27;choices&#x27;: [{&#x27;finish_reason&#x27;: &#x27;stop&#x27;, &#x27;index&#x27;: 0, &#x27;message&#x27;: {&#x27;role&#x27;: &#x27;assistant&#x27;, &#x27;content&#x27;: &#x27;Qualcomm is a leading global technology company that designs, develops, licenses, and markets semiconductor-based products and mobile platform technologies to major telecommunications and consumer electronics manufacturers worldwide.&#x27;}}], &#x27;created&#x27;: 1757073340, &#x27;model&#x27;: &#x27;gpt-3.5-turbo&#x27;, &#x27;system_fingerprint&#x27;: &#x27;b6362-f6da8cb8&#x27;, &#x27;object&#x27;: &#x27;chat.completion&#x27;, &#x27;usage&#x27;: {&#x27;completion_tokens&#x27;: 34, &#x27;prompt_tokens&#x27;: 26, &#x27;total_tokens&#x27;: 60}, &#x27;id&#x27;: &#x27;chatcmpl-3O7l005WG1DzN191FTNomJNweHMoH8Is&#x27;, &#x27;timings&#x27;: {&#x27;prompt_n&#x27;: 12, &#x27;prompt_ms&#x27;: 303.581, &#x27;prompt_per_token_ms&#x27;: 25.298416666666668, &#x27;prompt_per_second&#x27;: 39.52816546490064, &#x27;predicted_n&#x27;: 34, &#x27;predicted_ms&#x27;: 4052.23, &#x27;predicted_per_token_ms&#x27;: 119.18323529411765, &#x27;predicted_per_second&#x27;: 8.390441806116632}}</span><br></span></code></pre></div></div>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="部署多模态llm">部署多模态LLM<a href="#部署多模态llm" class="hash-link" aria-label="Direct link to 部署多模态LLM" title="Direct link to 部署多模态LLM" translate="no">​</a></h3>
<p>您同样可以部署多模态大语言模型。例如 <a href="https://huggingface.co/ggml-org/SmolVLM-500M-Instruct-GGUF" target="_blank" rel="noopener noreferrer" class="">SmolVLM-500M-Instruct-GGUF</a>。下载 Q4_0 量化权重文件（或自行量化），并下载 CLIP 编码器<code>mmproj-*.gguf</code>文件。例如：</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain"># Download weights</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">wget https://huggingface.co/ggml-org/SmolVLM-500M-Instruct-GGUF/resolve/main/SmolVLM-500M-Instruct-f16.gguf</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">wget https://huggingface.co/ggml-org/SmolVLM-500M-Instruct-GGUF/resolve/main/mmproj-SmolVLM-500M-Instruct-f16.gguf</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># Quantize model (mmproj- models are not quantizable via llama-quantize, see below)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">llama-quantize --pure SmolVLM-500M-Instruct-f16.gguf SmolVLM-500M-Instruct-q4_0-pure.gguf Q4_0</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># Server the model</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">llama-server -m ./SmolVLM-500M-Instruct-q4_0-pure.gguf --mmproj ./mmproj-SmolVLM-500M-Instruct-f16.gguf --no-warmup -b 128 -c 2048 -s 11 -n 128 --host 0.0.0.0 --port 9876</span><br></span></code></pre></div></div>
<p><img decoding="async" loading="lazy" src="https://3580193864-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxM5xrbdbelLSl7uN8oac%2Fuploads%2Fgit-blob-272f54a74290f52156033bda8b2d480621ae78ab%2Fllamacpp2.png?alt=media" alt="" title="使用 llama-server 部署多模态 LLM" class="img_ev3q"></p>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg></span>info</div><div class="admonitionContent_BuS1"><p><strong>CLIP 模型仍为 fp16:</strong> <code>mmproj</code> 模型仍然是 fp16；因此处理图像会很慢。<a href="https://github.com/ggml-org/llama.cpp/pull/11644" target="_blank" rel="noopener noreferrer" class="">旧版本 llama.cpp</a> 中，有量化 CLIP 编码器的代码。</p></div></div>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="tips--tricks">Tips &amp; tricks<a href="#tips--tricks" class="hash-link" aria-label="Direct link to Tips &amp; tricks" title="Direct link to Tips &amp; tricks" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="比较-cpu-性能">比较 CPU 性能<a href="#比较-cpu-性能" class="hash-link" aria-label="Direct link to 比较 CPU 性能" title="Direct link to 比较 CPU 性能" translate="no">​</a></h3>
<p>把<code>-ngl 0</code>添加到<code>llama-*</code>命令以跳过将层转移到 GPU 的操作。模型将在 CPU 上运行，您可以将其与 GPU 性能进行比较。</p>
<p>例如， RB3 Gen 2 Vision Kit 上的 Qwen2-1.5B-Instruct Q4_0：</p>
<p><strong>GPU:</strong></p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">llama-cli -m ./qwen2-1_5b-instruct-q4_0-pure.gguf -no-cnv --no-warmup -b 128 -c 2048 -s 11 -n 128 -p &quot;Knock knock, &quot; -fa off</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># llama_perf_sampler_print:    sampling time =     225.78 ms /   133 runs   (    1.70 ms per token,   589.06 tokens per second)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># llama_perf_context_print:        load time =    5338.13 ms</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># llama_perf_context_print: prompt eval time =     201.32 ms /     5 tokens (   40.26 ms per token,    24.84 tokens per second)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># llama_perf_context_print:        eval time =   13214.35 ms /   127 runs   (  104.05 ms per token,     9.61 tokens per second)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># llama_perf_context_print:       total time =   18958.06 ms /   132 tokens</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># llama_perf_context_print:    graphs reused =        122</span><br></span></code></pre></div></div>
<p><strong>CPU:</strong></p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">llama-cli -m ./qwen2-1_5b-instruct-q4_0-pure.gguf -no-cnv --no-warmup -b 128 -ngl 99 -c 2048 -s 11 -n 128 -p &quot;Knock knock, &quot; -fa off -ngl 0</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># llama_perf_sampler_print:    sampling time =      23.47 ms /   133 runs   (    0.18 ms per token,  5666.08 tokens per second)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># llama_perf_context_print:        load time =     677.25 ms</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># llama_perf_context_print: prompt eval time =     253.39 ms /     5 tokens (   50.68 ms per token,    19.73 tokens per second)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># llama_perf_context_print:        eval time =   17751.29 ms /   127 runs   (  139.77 ms per token,     7.15 tokens per second)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># llama_perf_context_print:       total time =   18487.26 ms /   132 tokens</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># llama_perf_context_print:    graphs reused =        122</span><br></span></code></pre></div></div>
<p>这里 GPU 的令牌处理速度比 CPU 快约 33%。</p></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col noPrint_WFHX"><a href="https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/7.Application Development and Execution Guide/2.Framework-Driven AI Sample Execution/5.llama_cpp.md" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_JAkA"></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/ubuntu100-cn.github.io/docs/Application Development and Execution Guide/Framework-Driven AI Sample Execution/onnx"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">ONNX</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/ubuntu100-cn.github.io/docs/Application Development and Execution Guide/Framework-Driven AI Sample Execution/genie"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Genie</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#构建-llamacpp" class="table-of-contents__link toc-highlight">构建 llama.cpp</a><ul><li><a href="#下载并量化模型" class="table-of-contents__link toc-highlight">下载并量化模型</a></li><li><a href="#使用-llama-cli-运行你的第一个-llm" class="table-of-contents__link toc-highlight">使用 llama-cli 运行你的第一个 LLM</a></li><li><a href="#使用-llama-server-部署-llm" class="table-of-contents__link toc-highlight">使用 llama-server 部署 LLM</a></li><li><a href="#部署多模态llm" class="table-of-contents__link toc-highlight">部署多模态LLM</a></li></ul></li><li><a href="#tips--tricks" class="table-of-contents__link toc-highlight">Tips &amp; tricks</a><ul><li><a href="#比较-cpu-性能" class="table-of-contents__link toc-highlight">比较 CPU 性能</a></li></ul></li></ul></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Docs</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/ubuntu100-cn.github.io/docs/intro">Tutorial</a></li></ul></div><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://stackoverflow.com/questions/tagged/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Stack Overflow<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a href="https://discordapp.com/invite/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Discord<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a href="https://x.com/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">X<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div><div class="theme-layout-footer-column col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/ubuntu100-cn.github.io/blog">Blog</a></li><li class="footer__item"><a href="https://github.com/facebook/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 My Project, Inc. Built with Docusaurus.</div></div></div></footer></div>
</body>
</html>