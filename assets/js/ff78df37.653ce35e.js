"use strict";(globalThis.webpackChunkmy_website=globalThis.webpackChunkmy_website||[]).push([[3786],{1470:(e,n,t)=>{t.d(n,{A:()=>T});var i=t(6540),s=t(4164),r=t(7559),a=t(3104),l=t(6347),o=t(205),d=t(7485),p=t(1682),c=t(679);function u(e){return i.Children.toArray(e).filter(e=>"\n"!==e).map(e=>{if(!e||(0,i.isValidElement)(e)&&function(e){const{props:n}=e;return!!n&&"object"==typeof n&&"value"in n}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)})?.filter(Boolean)??[]}function _(e){const{values:n,children:t}=e;return(0,i.useMemo)(()=>{const e=n??function(e){return u(e).map(({props:{value:e,label:n,attributes:t,default:i}})=>({value:e,label:n,attributes:t,default:i}))}(t);return function(e){const n=(0,p.XI)(e,(e,n)=>e.value===n.value);if(n.length>0)throw new Error(`Docusaurus error: Duplicate values "${n.map(e=>e.value).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e},[n,t])}function m({value:e,tabValues:n}){return n.some(n=>n.value===e)}function h({queryString:e=!1,groupId:n}){const t=(0,l.W6)(),s=function({queryString:e=!1,groupId:n}){if("string"==typeof e)return e;if(!1===e)return null;if(!0===e&&!n)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return n??null}({queryString:e,groupId:n});return[(0,d.aZ)(s),(0,i.useCallback)(e=>{if(!s)return;const n=new URLSearchParams(t.location.search);n.set(s,e),t.replace({...t.location,search:n.toString()})},[s,t])]}function g(e){const{defaultValue:n,queryString:t=!1,groupId:s}=e,r=_(e),[a,l]=(0,i.useState)(()=>function({defaultValue:e,tabValues:n}){if(0===n.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(e){if(!m({value:e,tabValues:n}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${e}" but none of its children has the corresponding value. Available values are: ${n.map(e=>e.value).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return e}const t=n.find(e=>e.default)??n[0];if(!t)throw new Error("Unexpected error: 0 tabValues");return t.value}({defaultValue:n,tabValues:r})),[d,p]=h({queryString:t,groupId:s}),[u,g]=function({groupId:e}){const n=function(e){return e?`docusaurus.tab.${e}`:null}(e),[t,s]=(0,c.Dv)(n);return[t,(0,i.useCallback)(e=>{n&&s.set(e)},[n,s])]}({groupId:s}),f=(()=>{const e=d??u;return m({value:e,tabValues:r})?e:null})();(0,o.A)(()=>{f&&l(f)},[f]);return{selectedValue:a,selectValue:(0,i.useCallback)(e=>{if(!m({value:e,tabValues:r}))throw new Error(`Can't select invalid tab value=${e}`);l(e),p(e),g(e)},[p,g,r]),tabValues:r}}var f=t(2303);const x={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};var b=t(4848);function j({className:e,block:n,selectedValue:t,selectValue:i,tabValues:r}){const l=[],{blockElementScrollPositionUntilNextRender:o}=(0,a.a_)(),d=e=>{const n=e.currentTarget,s=l.indexOf(n),a=r[s].value;a!==t&&(o(n),i(a))},p=e=>{let n=null;switch(e.key){case"Enter":d(e);break;case"ArrowRight":{const t=l.indexOf(e.currentTarget)+1;n=l[t]??l[0];break}case"ArrowLeft":{const t=l.indexOf(e.currentTarget)-1;n=l[t]??l[l.length-1];break}}n?.focus()};return(0,b.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,s.A)("tabs",{"tabs--block":n},e),children:r.map(({value:e,label:n,attributes:i})=>(0,b.jsx)("li",{role:"tab",tabIndex:t===e?0:-1,"aria-selected":t===e,ref:e=>{l.push(e)},onKeyDown:p,onClick:d,...i,className:(0,s.A)("tabs__item",x.tabItem,i?.className,{"tabs__item--active":t===e}),children:n??e},e))})}function v({lazy:e,children:n,selectedValue:t}){const r=(Array.isArray(n)?n:[n]).filter(Boolean);if(e){const e=r.find(e=>e.props.value===t);return e?(0,i.cloneElement)(e,{className:(0,s.A)("margin-top--md",e.props.className)}):null}return(0,b.jsx)("div",{className:"margin-top--md",children:r.map((e,n)=>(0,i.cloneElement)(e,{key:n,hidden:e.props.value!==t}))})}function y(e){const n=g(e);return(0,b.jsxs)("div",{className:(0,s.A)(r.G.tabs.container,"tabs-container",x.tabList),children:[(0,b.jsx)(j,{...n,...e}),(0,b.jsx)(v,{...n,...e})]})}function T(e){const n=(0,f.A)();return(0,b.jsx)(y,{...e,children:u(e.children)},String(n))}},2491:(e,n,t)=>{t.d(n,{A:()=>i});const i=t.p+"assets/images/IC-LiteRT-ce50f98bc8865d04a94ebdc42ab6106d.jpeg"},2531:(e,n,t)=>{t.d(n,{A:()=>i});const i=t.p+"assets/images/IC-LiteRT-NPU-338129fd2a6187b7ac81956600ccfd7f.jpeg"},3244:(e,n,t)=>{t.d(n,{A:()=>i});const i=t.p+"assets/images/OpenCV-OD-66beef4f3ce80e7fddf5a740b7706421.png"},5450:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>p,contentTitle:()=>d,default:()=>_,frontMatter:()=>o,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"Application Development and Execution Guide/Framework-Driven AI Sample Execution/litert_tflite","title":"LiteRT / TFLite","description":"LiteRT\uff08\u524d\u8eab\u4e3aTensorFlow Lite\uff09\uff0c\u662f\u8c37\u6b4c\u4e13\u4e3a\u8bbe\u5907\u7aefAI\u6253\u9020\u7684\u9ad8\u6027\u80fd\u63a8\u7406\u8fd0\u884c\u65f6\u3002\u901a\u8fc7\u96c6\u6210\u5728AI Engine Direct\u4e2d\u7684LiteRT\u59d4\u6258\uff0c\u60a8\u53ea\u9700\u4e00\u884c\u4ee3\u7801\u5373\u53ef\u5728Dragonwing\u8bbe\u5907\u7684NPU\u4e0a\u8fd0\u884c\u73b0\u6709\u7684\u91cf\u5316LiteRT\u6a21\u578b\uff08\u652f\u6301Python\u4e0eC++\uff09\u3002","source":"@site/docs/7.Application Development and Execution Guide/2.Framework-Driven AI Sample Execution/3.litert_tflite.md","sourceDirName":"7.Application Development and Execution Guide/2.Framework-Driven AI Sample Execution","slug":"/Application Development and Execution Guide/Framework-Driven AI Sample Execution/litert_tflite","permalink":"/ubuntu100-cn.github.io/docs/Application Development and Execution Guide/Framework-Driven AI Sample Execution/litert_tflite","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/7.Application Development and Execution Guide/2.Framework-Driven AI Sample Execution/3.litert_tflite.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Qualcomm\xae AI Hub","permalink":"/ubuntu100-cn.github.io/docs/Application Development and Execution Guide/Building AI Models/qualcomm_ai_hub"},"next":{"title":"ONNX","permalink":"/ubuntu100-cn.github.io/docs/Application Development and Execution Guide/Framework-Driven AI Sample Execution/onnx"}}');var s=t(4848),r=t(8453),a=t(1470),l=t(9365);const o={},d="LiteRT / TFLite",p={},c=[{value:"\u91cf\u5316\u6a21\u578b",id:"\u91cf\u5316\u6a21\u578b",level:2},{value:"\u5728 NPU \u4e0a\u8fd0\u884c\u6a21\u578b\uff08Python\uff09",id:"\u5728-npu-\u4e0a\u8fd0\u884c\u6a21\u578bpython",level:2},{value:"\u5728 NPU \u4e0a\u8fd0\u884c\u6a21\u578b\uff08C++\uff09",id:"\u5728-npu-\u4e0a\u8fd0\u884c\u6a21\u578bc",level:2},{value:"Python \u793a\u4f8b",id:"python-\u793a\u4f8b",level:3},{value:"Vision Transformers",id:"vision-transformers",level:3},{value:"\u57fa\u4e8eGTK\u7684\u56fe\u50cf\u5206\u7c7b\u5e94\u7528",id:"\u57fa\u4e8egtk\u7684\u56fe\u50cf\u5206\u7c7b\u5e94\u7528",level:3},{value:"\u4e0b\u8f7d TFlite \u6a21\u578b",id:"\u4e0b\u8f7d-tflite-\u6a21\u578b",level:3},{value:"\u8f85\u52a9\u51fd\u6570",id:"\u8f85\u52a9\u51fd\u6570",level:3},{value:"\u53c2\u8003\u4ee3\u7801",id:"\u53c2\u8003\u4ee3\u7801",level:3},{value:"Object Detection with OpenCV &amp; Wayland Display",id:"object-detection-with-opencv--wayland-display",level:3},{value:"\u53c2\u8003\u4ee3\u7801",id:"\u53c2\u8003\u4ee3\u7801-1",level:3}];function u(e){const n={a:"a",admonition:"admonition",br:"br",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",img:"img",li:"li",mdxAdmonitionTitle:"mdxAdmonitionTitle",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"litert--tflite",children:"LiteRT / TFLite"})}),"\n",(0,s.jsx)(n.p,{children:"LiteRT\uff08\u524d\u8eab\u4e3aTensorFlow Lite\uff09\uff0c\u662f\u8c37\u6b4c\u4e13\u4e3a\u8bbe\u5907\u7aefAI\u6253\u9020\u7684\u9ad8\u6027\u80fd\u63a8\u7406\u8fd0\u884c\u65f6\u3002\u901a\u8fc7\u96c6\u6210\u5728AI Engine Direct\u4e2d\u7684LiteRT\u59d4\u6258\uff0c\u60a8\u53ea\u9700\u4e00\u884c\u4ee3\u7801\u5373\u53ef\u5728Dragonwing\u8bbe\u5907\u7684NPU\u4e0a\u8fd0\u884c\u73b0\u6709\u7684\u91cf\u5316LiteRT\u6a21\u578b\uff08\u652f\u6301Python\u4e0eC++\uff09\u3002"}),"\n",(0,s.jsx)(n.h2,{id:"\u91cf\u5316\u6a21\u578b",children:"\u91cf\u5316\u6a21\u578b"}),"\n",(0,s.jsxs)(n.p,{children:["NPU\u4ec5\u652f\u6301uint8/int8\u91cf\u5316\u6a21\u578b\u3002\u4e0d\u652f\u6301\u7684\u6a21\u578b\u6216\u4e0d\u652f\u6301\u7684\u5c42\u5c06\u81ea\u52a8\u79fb\u56de CPU\u3002\u53ef\u4ee5\u4f7f\u7528 ",(0,s.jsx)(n.a,{href:"https://www.tensorflow.org/model_optimization/guide/quantization/training_comprehensive_guide",children:"quantization-aware training"})," \u6216 ",(0,s.jsx)(n.a,{href:"https://ai.google.dev/edge/litert/models/post_training_quantization",children:"post-training quantization"})," \u6765\u91cf\u5316 LiteRT \u6a21\u578b\u3002\u8bf7\u786e\u4fdd\u9075\u5faa\u201c\u5b8c\u5168\u6574\u578b\u91cf\u5316\u201d\u7684\u64cd\u4f5c\u6b65\u9aa4\u3002"]}),"\n",(0,s.jsxs)(n.admonition,{type:"info",children:[(0,s.jsx)(n.mdxAdmonitionTitle,{}),(0,s.jsxs)(n.p,{children:["**\u4e0d\u60f3\u81ea\u5df1\u91cf\u5316\u6a21\u578b\uff1f**\u53ef\u4ee5\u4ece ",(0,s.jsx)(n.a,{href:"https://aihub.qualcomm.com",children:"Qualcomm AI Hub"})," \u4e0b\u8f7d\u4e00\u7cfb\u5217\u7684\u91cf\u5316\u6a21\u578b\uff0c\u6216\u4f7f\u7528 ",(0,s.jsx)(n.a,{href:"https://qc-ai-test.gitbook.io/qc-ai-test-docs/running-building-ai-models/edge-impulse",children:"Edge Impulse"})," \u6765\u91cf\u5316\u5df2\u6709\u7684\u6216\u5168\u65b0\u7684\u6a21\u578b\u3002"]})]}),"\n",(0,s.jsx)(n.h2,{id:"\u5728-npu-\u4e0a\u8fd0\u884c\u6a21\u578bpython",children:"\u5728 NPU \u4e0a\u8fd0\u884c\u6a21\u578b\uff08Python\uff09"}),"\n",(0,s.jsx)(n.p,{children:"\u8981\u5c06\u6a21\u578b\u5378\u8f7d\u5230 NPU\uff0c\u53ea\u9700\u52a0\u8f7d LiteRT \u59d4\u6258\uff0c\u5e76\u5c06\u5176\u4f20\u9012\u5230\u89e3\u91ca\u5668\u4e2d\u3002\u4f8b\u5982\uff1a"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-py",children:'from ai_edge_litert.interpreter import Interpreter, load_delegate\n\nqnn_delegate = load_delegate("libQnnTFLiteDelegate.so", options={"backend_type": "htp"})\ninterpreter = Interpreter(\n    model_path=...,\n    experimental_delegates=[qnn_delegate]\n)\n'})}),"\n",(0,s.jsx)(n.h2,{id:"\u5728-npu-\u4e0a\u8fd0\u884c\u6a21\u578bc",children:"\u5728 NPU \u4e0a\u8fd0\u884c\u6a21\u578b\uff08C++\uff09"}),"\n",(0,s.jsx)(n.p,{children:"\u8981\u5c06\u6a21\u578b\u5378\u8f7d\u5230 NPU\uff0c\u9996\u5148\u9700\u8981\u6dfb\u52a0\u4ee5\u4e0b\u7f16\u8bd1\u6807\u5fd7\uff1a"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-makefile",children:"CFLAGS += -I${QNN_SDK_ROOT}/include\nLDFLAGS += -L${QNN_SDK_ROOT}/lib/aarch64-ubuntu-gcc9.4 -lQnnTFLiteDelegate\n"})}),"\n",(0,s.jsx)(n.p,{children:"\u7136\u540e\uff0c\u5b9e\u4f8b\u5316 LiteRT \u59d4\u6258\u5e76\u5c06\u5176\u4f20\u9012\u7ed9 LiteRT \u89e3\u91ca\u5668\uff1a"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-c",children:'// == Includes ==\n#include "QNN/TFLiteDelegate/QnnTFLiteDelegate.h"\n\n// == Application code ==\n\n// Get your interpreter...\ntflite::Interpreter *interpreter = ...;\n\n// Create QNN Delegate options structure.\nTfLiteQnnDelegateOptions options = TfLiteQnnDelegateOptionsDefault();\n\n// Set the mandatory backend_type option. All other options have default values.\noptions.backend_type = kHtpBackend;\n\n// Instantiate delegate. Must not be freed until interpreter is freed.\nTfLiteDelegate* delegate = TfLiteQnnDelegateCreate(&options);\n\nTfLiteStatus status = interpreter->ModifyGraphWithDelegate(delegate);\n// Check that status == kTfLiteOk\n'})}),"\n",(0,s.jsx)(n.h3,{id:"python-\u793a\u4f8b",children:"Python \u793a\u4f8b"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"\u524d\u63d0\u6761\u4ef6"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Ubuntu \u64cd\u4f5c\u7cfb\u7edf"})," \u5df2\u5237\u5165\u3002"]}),"\n",(0,s.jsxs)(n.li,{children:["\u5177\u6709\u9002\u5f53\u6743\u9650\u7684",(0,s.jsx)(n.strong,{children:"\u7ec8\u7aef\u8bbf\u95ee"}),"\u3002"]}),"\n",(0,s.jsxs)(n.li,{children:["\u5982\u679c\u60a8\u4e4b\u524d\u6ca1\u6709\u5b89\u88c5\u8fc7 PPA \u5305\uff0c\u8bf7\u6309\u7167\u4ee5\u4e0b\u6b65\u9aa4\u8fdb\u884c\u5b89\u88c5\u3002","\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-shell",children:"  git clone -b ubuntu_setup --single-branch https://github.com/rubikpi-ai/rubikpi-script.git \n  cd rubikpi-script  \n  ./install_ppa_pkgs.sh  \n"})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\u5728\u5f00\u53d1\u677f\u4e0a\u6253\u5f00\u7ec8\u7aef\uff0c\u6216\u5efa\u7acb SSH \u4f1a\u8bdd\uff0c\u7136\u540e\u6267\u884c\u4ee5\u4e0b\u64cd\u4f5c\uff1a",(0,s.jsx)(n.br,{}),"\n","\u521b\u5efa\u4e00\u4e2a\u65b0\u7684\u865a\u62df\u73af\u5883\uff08venv\uff09\uff0c\u5e76\u5b89\u88c5 LiteRT \u8fd0\u884c\u65f6\u548c Pillow\uff1a","\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"python3 -m venv .venv-litert-demo --system-site-packages\nsource .venv-litert-demo/bin/activate\npip3 install ai-edge-litert==1.3.0 Pillow\npip3 install opencv-python\n"})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\u5b89\u88c5\u5fc5\u8981\u7684 python3 \u548c gtk \u5305\u3002","\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"sudo apt install python3-gi python3-gi-cairo gir1.2-gtk-3.0\nsudo apt install python3-venv python3-full\nsudo apt install -y pkg-config cmake libcairo2-dev\nsudo apt install libgirepository1.0-dev gir1.2-glib-2.0\nsudo apt install build-essential python3-dev python3-pip pkg-config meson\n"})}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(a.A,{children:[(0,s.jsxs)(l.A,{value:"Example1",label:"Vision Transformers",children:[(0,s.jsx)(n.h3,{id:"vision-transformers",children:"Vision Transformers"}),(0,s.jsxs)(n.p,{children:["\u4ee5\u4e0b\u8bf4\u660e\u5982\u4f55\u4f7f\u7528 LiteRT \u59d4\u6258\u5728 CPU \u548c NPU \u4e0a\u8fd0\u884c Vision Transformer \u6a21\u578b\uff08\u4ece ",(0,s.jsx)(n.a,{href:"https://aihub.qualcomm.com/models/vit",children:"AI Hub"})," \u4e0b\u8f7d)\u3002"]}),(0,s.jsxs)(n.p,{children:["1\ufe0f\u20e3 \u521b\u5efa ",(0,s.jsx)(n.code,{children:"inference_vit.py"})," \u5e76\u6dfb\u52a0\u4ee5\u4e0b\u53c2\u8003\u4ee3\u7801\uff1a"]}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-py",children:'import numpy as np\nfrom ai_edge_litert.interpreter import Interpreter, load_delegate\nfrom PIL import Image\nimport os, time, sys\nimport urllib.request\n\ndef curr_ms():\n    return round(time.time() * 1000)\n\nuse_npu = True if len(sys.argv) >= 2 and sys.argv[1] == \'--use-npu\' else False\n\n# Path to your quantized TFLite model and test image (will be download automatically)\nMODEL_PATH = "vit-vit-w8a8.tflite"\nIMAGE_PATH = "boa-constrictor.jpg"\nLABELS_PATH = "vit-vit-labels.txt"\n\nif not os.path.exists(MODEL_PATH):\n    print("Downloading model...")\n    model_url = \'https://cdn.edgeimpulse.com/qc-ai-docs/models/vit-vit-w8a8.tflite\'\n    urllib.request.urlretrieve(model_url, MODEL_PATH)\n\nif not os.path.exists(LABELS_PATH):\n    print("Downloading labels...")\n    labels_url = \'https://cdn.edgeimpulse.com/qc-ai-docs/models/vit-vit-labels.txt\'\n    urllib.request.urlretrieve(labels_url, LABELS_PATH)\n\nif not os.path.exists(IMAGE_PATH):\n    print("Downloading image...")\n    image_url = \'https://cdn.edgeimpulse.com/qc-ai-docs/examples/boa-constrictor.jpg\'\n    urllib.request.urlretrieve(image_url, IMAGE_PATH)\n\nwith open(LABELS_PATH, \'r\') as f:\n    labels = [line for line in f.read().splitlines() if line.strip()]\n\nexperimental_delegates = []\nif use_npu:\n    experimental_delegates = [load_delegate("libQnnTFLiteDelegate.so", options={"backend_type": "htp"})]\n\n# Load TFLite model and allocate tensors\ninterpreter = Interpreter(\n    model_path=MODEL_PATH,\n    experimental_delegates=experimental_delegates\n)\ninterpreter.allocate_tensors()\n\n# Get input and output tensor details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Load and preprocess image\ndef load_image(path, input_shape):\n    # Expected input shape: [1, height, width, channels]\n    _, height, width, channels = input_shape\n\n    img = Image.open(path).convert("RGB").resize((width, height))\n    img_np = np.array(img, dtype=np.uint8)  # quantized models expect uint8\n    img_np = np.expand_dims(img_np, axis=0)\n    return img_np\n\ninput_shape = input_details[0][\'shape\']\ninput_data = load_image(IMAGE_PATH, input_shape)\n\n# Set tensor and run inference\ninterpreter.set_tensor(input_details[0][\'index\'], input_data)\n\n# Run once to warmup\ninterpreter.invoke()\n\n# Then run 10x\nstart = curr_ms()\nfor i in range(0, 10):\n    interpreter.invoke()\nend = curr_ms()\n\n# Get prediction\nq_output = interpreter.get_tensor(output_details[0][\'index\'])\nscale, zero_point = output_details[0][\'quantization\']\nf_output = (q_output.astype(np.float32) - zero_point) * scale\n\n# Image classification models in AI Hub miss a Softmax() layer at the end of the model, so add it manually\ndef softmax(x, axis=-1):\n    # subtract max for numerical stability\n    x_max = np.max(x, axis=axis, keepdims=True)\n    e_x = np.exp(x - x_max)\n    return e_x / np.sum(e_x, axis=axis, keepdims=True)\n\n# show top-5 predictions\nscores = softmax(f_output[0])\ntop_k = scores.argsort()[-5:][::-1]\nprint("\\nTop-5 predictions:")\nfor i in top_k:\n    print(f"Class {labels[i]}: score={scores[i]}")\n\nprint(\'\')\nprint(f\'Inference took (on average): {(end - start) / 10}ms. per image\')\n'})}),(0,s.jsx)(n.p,{children:"2\ufe0f\u20e3 \u5728 CPU \u4e0a\u8fd0\u884c\u6a21\u578b\uff1a"}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"python3 inference_vit.py\n\n# INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n#\n# Top-5 predictions:\n# Class boa constrictor: score=0.6264431476593018\n# Class rock python: score=0.047579940408468246\n# Class night snake: score=0.006721484009176493\n# Class mouse: score=0.0022421202156692743\n# Class pick: score=0.001942973816767335\n#\n# Inference took (on average): 391.1ms. per image\n"})}),(0,s.jsx)(n.p,{children:"3\ufe0f\u20e3 \u5728 NPU \u4e0a\u8fd0\u884c\u6a21\u578b\uff1a"}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"python3 inference_vit.py --use-npu\n\n# INFO: TfLiteQnnDelegate delegate: 1382 nodes delegated out of 1633 nodes with 27 partitions.\n#\n# INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n#\n# Top-5 predictions:\n# Class boa constrictor: score=0.6113042235374451\n# Class rock python: score=0.038359832018613815\n# Class night snake: score=0.011630792170763016\n# Class mouse: score=0.002294909441843629\n# Class lens cap: score=0.0018960189772769809\n#\n# Inference took (on average): 132.7ms. per image\n"})}),(0,s.jsx)(n.p,{children:"\u6b63\u5982\u6240\u89c1\uff0c\u8be5\u6a21\u578b\u5728 NPU \u4e0a\u8fd0\u884c\u901f\u5ea6\u660e\u663e\u66f4\u5feb\uff0c\u4f46\u6a21\u578b\u7684\u8f93\u51fa\u7565\u6709\u53d8\u5316\u3002\u6b64\u5916\uff0c\u6b64\u6a21\u578b\u5e76\u975e\u6240\u6709\u5c42\u90fd\u53ef\u4ee5\u5728 NPU \u4e0a\u8fd0\u884c\uff08\u201c1633 \u4e2a\u8282\u70b9\u4e2d\u59d4\u6258\u4e86 1382 \u4e2a\u8282\u70b9\uff0c\u5171 27 \u4e2a\u5206\u533a\u201d\uff09\u3002"})]}),(0,s.jsxs)(l.A,{value:"Example2",label:"Image Classification",children:[(0,s.jsx)(n.h3,{id:"\u57fa\u4e8egtk\u7684\u56fe\u50cf\u5206\u7c7b\u5e94\u7528",children:"\u57fa\u4e8eGTK\u7684\u56fe\u50cf\u5206\u7c7b\u5e94\u7528"}),(0,s.jsx)(n.p,{children:"\u4ee5\u4e0b\u4ecb\u7ecd\u5982\u4f55\u901a\u8fc7\u57fa\u4e8eGTK\u7684\u684c\u9762\u5e94\u7528\u7a0b\u5e8f\uff0c\u8fd0\u7528AI Engine Direct\u7684LiteRT\u59d4\u6258\uff0c\u5728CPU\u548cNPU\u4e0a\u8fd0\u884c\u4eceAI Hub\u4e0b\u8f7d\u7684\u56fe\u50cf\u5206\u7c7b\u6a21\u578b\u3002"}),(0,s.jsx)(n.p,{children:"GoogLeNet_w8a8.tflite\u6a21\u578b\u6765\u81eaAI-Hub\uff0c\u5229\u7528\u4e86TensorFlow Lite\u4e0eQNN\u4ee3\u7406\u52a0\u901f\u6280\u672f\uff0c\u5b9e\u73b0\u9ad8\u6548\u7684\u8bbe\u5907\u7aef\u63a8\u7406\u3002"}),(0,s.jsxs)(n.p,{children:["\u8bf7\u6309\u4ee5\u4e0b\u6b65\u9aa4\u521b\u5efa\u56fe\u50cf\u5206\u7c7b\u5e94\u7528\u7a0b\u5e8f\u3002",(0,s.jsx)(n.br,{}),"\n","\u6982\u8ff0",(0,s.jsx)(n.br,{}),"\n","\xb7\u7c7b\u578b\uff1a\u684c\u9762 GUI \u5e94\u7528\u7a0b\u5e8f",(0,s.jsx)(n.br,{}),"\n","\xb7\u529f\u80fd\uff1a\u4f7f\u7528 TFLite \u8fdb\u884c\u56fe\u50cf\u5206\u7c7b",(0,s.jsx)(n.br,{}),"\n","\u2022\u6a21\u5f0f\uff1aCPU \u548c QNN \u59d4\u6258",(0,s.jsx)(n.br,{}),"\n","\u2022\u63a5\u53e3\uff1a\u57fa\u4e8e GTK \u7684 GUI",(0,s.jsx)(n.br,{}),"\n","\u2022\u8f93\u51fa\uff1a\u5e26\u6709\u7f6e\u4fe1\u5ea6\u6761\u7684\u6700\u4f73\u9884\u6d4b"]}),(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"\u73af\u5883\u8bbe\u7f6e\u548c\u5bfc\u5165"}),(0,s.jsx)(n.br,{}),"\n","\u5728\u6b64\u6b65\u9aa4\u4e2d\uff0c\u811a\u672c\u8bbe\u7f6e\u4e0e\u663e\u793a\u76f8\u5173\u7684\u73af\u5883\u53d8\u91cf\uff08\u9488\u5bf9 Linux \u7cfb\u7edf\uff09\u5e76\u5bfc\u5165\u5fc5\u8981\u7684\u5e93\uff0c\u5982 OpenCV\u3001NumPy\u3001GTK \u548c TensorFlow Lite\u3002"]}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import cv2, numpy as np, os, time\nfrom gi.repository import Gtk, GLib, GdkPixbuf\nimport ai_edge_litert.interpreter as tflite\n"})}),(0,s.jsx)(n.admonition,{type:"note",children:(0,s.jsx)(n.p,{children:"GTK \u7528\u4e8e GUI\uff0cOpenCV \u7528\u4e8e\u56fe\u50cf\u5904\u7406\uff0cTensorFlow Lite \u7528\u4e8e\u63a8\u7406\u3002"})}),(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"\u914d\u7f6e\u5e38\u91cf"}),(0,s.jsx)(n.br,{}),"\n","\u8fd9\u4e9b\u5e38\u91cf\u5b9a\u4e49\u4e86\u6a21\u578b\u3001\u6807\u7b7e\u6587\u4ef6\u548c\u59d4\u6258\u5e93\u7684\u8def\u5f84\u3002"]}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'TF_MODEL = "/home/ubuntu/GoogLeNet_w8a8.tflite"\nLABELS = "/etc/labels/imagenet_labels.txt"\nDELEGATE_PATH = "libQnnTFLiteDelegate.so"\nDEVICE_OS = "Ubuntu"\n'})}),(0,s.jsx)(n.h3,{id:"\u4e0b\u8f7d-tflite-\u6a21\u578b",children:"\u4e0b\u8f7d TFlite \u6a21\u578b"}),(0,s.jsx)(n.p,{children:"\u8be5\u811a\u672c\u68c0\u67e5\u672c\u5730\u662f\u5426\u5b58\u5728 TensorFlow Lite \u6a21\u578b\u6587\u4ef6\uff0c\u5982\u679c\u4e0d\u5b58\u5728\uff0c\u5219\u4ece\u6307\u5b9a\u7684 Hugging Face URL \u4e0b\u8f7d\u3002"}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import urllib.request\nif not os.path.exists(TF_MODEL):\n   print(\"Downloading model...\")\n   model_url = 'https://huggingface.co/qualcomm/GoogLeNet/resolve/main/GoogLeNet_w8a8.tflite'\n   urllib.request.urlretrieve(model_url, TF_MODEL)\n"})}),(0,s.jsx)(n.h3,{id:"\u8f85\u52a9\u51fd\u6570",children:"\u8f85\u52a9\u51fd\u6570"}),(0,s.jsx)(n.p,{children:"\u6b64\u6b65\u9aa4\u4f7f\u7528 LiteRT \u548c GTK \u8bbe\u7f6e\u56fe\u50cf\u5206\u7c7b\u7684\u6838\u5fc3\u903b\u8f91\u548c\u754c\u9762\u3002"}),(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Softmax \u8ba1\u7b97"}),(0,s.jsx)(n.br,{}),"\n","\u5728\u5c06 logits \u8f6c\u6362\u4e3a\u6982\u7387\u65f6\uff0c\u786e\u4fdd\u6570\u503c\u7684\u7a33\u5b9a\u6027\u3002"]}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"Pythondef stable_softmax(logits):    \nlogits = logits.astype(np.float32)    \nshifted_logits = np.clip(logits - np.max(logits), -500, 500)    \nexp_scores = np.exp(shifted_logits)    \nreturn exp_scores / np.sum(exp_scores)\n"})}),(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"\u6807\u7b7e\u52a0\u8f7d\u5668"}),(0,s.jsx)(n.br,{}),"\n","\u4ece\u6587\u672c\u6587\u4ef6\u52a0\u8f7d\u7c7b\u6807\u7b7e\u3002"]}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"Pythondef load_labels(label_path):    \nwith open(label_path, 'r') as f: \n       return [line.strip() for line in f.readlines()]\n"})}),(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"\u56fe\u50cf\u9884\u5904\u7406"}),(0,s.jsx)(n.br,{}),"\n","\u4e3a\u6a21\u578b\u8f93\u5165\u51c6\u5907\u56fe\u50cf\uff1a\u8c03\u6574\u5927\u5c0f\u3001\u989c\u8272\u8f6c\u6362\u548c\u91cd\u5851\u3002"]}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"Pythondef preprocess_image(image_path, input_shape, input_dtype):   \nimg = cv2.imread(image_path)    \nimg = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)    \nimg = cv2.resize(img, (input_shape[2], input_shape[1]))    \nimg = img.astype(input_dtype)    \nreturn np.expand_dims(img, axis=0)\n"})}),(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"\u63a8\u7406\u6267\u884c"}),(0,s.jsx)(n.br,{}),"\n","\u8be5\u51fd\u6570\u5c06\uff1a",(0,s.jsx)(n.br,{}),"\n","\u2022\t\u52a0\u8f7d\u6a21\u578b\uff08\u5e26\u6216\u4e0d\u5e26\u59d4\u6258\uff09",(0,s.jsx)(n.br,{}),"\n","\u2022\t\u51c6\u5907\u8f93\u5165",(0,s.jsx)(n.br,{}),"\n","\u2022\t\u8fd0\u884c\u63a8\u7406",(0,s.jsx)(n.br,{}),"\n","\u2022\t\u5e94\u7528 softmax",(0,s.jsx)(n.br,{}),"\n","\u2022\t\u8fd4\u56de\u5177\u6709\u7f6e\u4fe1\u5ea6\u5206\u6570\u7684\u524d 4 \u4e2a\u9884\u6d4b"]}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"def runInference(image, use_delegate):\n    if use_delegate:\n        try:\n            delegate = tflite.load_delegate(DELEGATE_PATH, {'backend_type': 'htp'})\n            model = tflite.Interpreter(model_path=TF_MODEL, experimental_delegates=[delegate])\n        except:\n            model = tflite.Interpreter(model_path=TF_MODEL)\n    else:\n        model = tflite.Interpreter(model_path=TF_MODEL)\n    model.allocate_tensors()\n    input_details = model.get_input_details()\n    input_data = preprocess_image(image, input_details[0]['shape'], input_details[0]['dtype'])\n    model.set_tensor(input_details[0]['index'], input_data)\n    start_time = time.time()\n    model.invoke()\n    inference_time = time.time() - start_time\n    output_data = model.get_tensor(model.get_output_details()[0]['index'])\n    probabilities = stable_softmax(output_data[0])\n    labels = load_labels(LABELS)\n    top_indices = np.argsort(probabilities)[::-1][:4]\n    results = [(labels[i], probabilities[i] * 100) for i in top_indices]\n    return results, inference_time\n"})}),(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"GTK GUI \u7ec4\u4ef6"})}),(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"\u6587\u4ef6\u6d4f\u89c8\u5668\u5bf9\u8bdd\u6846"}),(0,s.jsx)(n.br,{}),"\n","\u5141\u8bb8\u7528\u6237\u9009\u62e9\u56fe\u50cf\u6587\u4ef6\u3002"]}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'class FileBrowser(Gtk.FileChooserDialog):\n    def __init__(self):\n        super().__init__(title="Choose an image", action=Gtk.FileChooserAction.OPEN)\n        self.add_buttons(Gtk.STOCK_CANCEL, Gtk.ResponseType.CANCEL, Gtk.STOCK_OPEN, Gtk.ResponseType.OK)\n    def run_and_get_file(self):\n        if self.run() == Gtk.ResponseType.OK:\n            return self.get_filename()\n        self.destroy()\n'})}),(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"\u4e3b\u7a97\u53e3"}),(0,s.jsx)(n.br,{}),"\n","GUI \u5305\u62ec\uff1a",(0,s.jsx)(n.br,{}),"\n","\u2022\t\u56fe\u50cf\u663e\u793a\u533a\u57df",(0,s.jsx)(n.br,{}),"\n","\u2022\t\u7528\u4e8e\u9009\u62e9 CPU \u6216\u59d4\u6258\u7684\u5355\u9009\u6309\u94ae",(0,s.jsx)(n.br,{}),"\n","\u2022\t\u7528\u4e8e\u9009\u62e9\u548c\u91cd\u65b0\u5904\u7406\u56fe\u50cf\u7684\u6309\u94ae",(0,s.jsx)(n.br,{}),"\n","\u2022\t\u5e26\u6709\u6807\u7b7e\u548c\u8fdb\u5ea6\u6761\u7684\u7ed3\u679c\u663e\u793a"]}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'class MainWindow(Gtk.Window):\n    def __init__(self):\n        super().__init__(title="Image Classification")\n        self.set_default_size(800, 600)\n        self.imageFilepath = ""\n        ...\n'})}),(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"\u56fe\u50cf\u5904\u7406\u4e0e\u663e\u793a"}),(0,s.jsx)(n.br,{}),"\n","\u6b64\u65b9\u6cd5\uff1a",(0,s.jsx)(n.br,{}),"\n","\u2022\t\u8c03\u6574\u5927\u5c0f\u5e76\u663e\u793a\u56fe\u50cf",(0,s.jsx)(n.br,{}),"\n","\u2022\t\u8fd0\u884c\u63a8\u7406",(0,s.jsx)(n.br,{}),"\n","\u2022\t\u663e\u793a\u5e26\u6709\u8fdb\u5ea6\u6761\u548c\u63a8\u7406\u65f6\u95f4\u7684\u7ed3\u679c"]}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"def process_file(self, filepath):\n    pixbuf = GdkPixbuf.Pixbuf.new_from_file(filepath)\n    new_width, new_height = resizeImage(pixbuf)\n    scaled_pixbuf = pixbuf.scale_simple(new_width, new_height, GdkPixbuf.InterpType.BILINEAR)\n    self.image.set_from_pixbuf(scaled_pixbuf)\n\n    results, inference_time = runInference(filepath, self.use_delegate())\n    ...\n"})}),(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"\u5e94\u7528\u7a0b\u5e8f\u5165\u53e3\u70b9"}),(0,s.jsx)(n.br,{}),"\n","\u521d\u59cb\u5316\u5e76\u542f\u52a8 GTK \u5e94\u7528\u7a0b\u5e8f\u3002"]}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'def main():\n    app = MainWindow()\n    app.connect("destroy", Gtk.main_quit)\n    app.show_all()\n    Gtk.main()\nif __name__ == "__main__":\n    success, _ = Gtk.init_check()\n    if not success:\n        print("GTK could not be initialized.")\n        exit(1)\n    main()\n'})}),(0,s.jsx)(n.h3,{id:"\u53c2\u8003\u4ee3\u7801",children:"\u53c2\u8003\u4ee3\u7801"}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# -----------------------------------------------------------------------------\n#\n# Copyright (c) Qualcomm Technologies, Inc. and/or its subsidiaries.\n# SPDX-License-Identifier: BSD-3-Clause\n#\n# -----------------------------------------------------------------------------\nimport cv2\nimport gi\nimport numpy as np\nimport os\nos.environ[\'xDG_RUNTIME_DIR\'] = \'/run/user/1000/\'\nos.environ[\'WAYLAND_DISPLAY\'] = \'wayland-1\'\nos.environ[\'DISPLAY\'] = \':0\'\nimport time\nimport urllib.request\ngi.require_version("Gtk", "3.0")\nfrom gi.repository import Gtk, GLib, GdkPixbuf\n\n# ========= Constants =========\nTF_MODEL = "/home/ubuntu/GoogLeNet_w8a8.tflite"\nLABELS = "/etc/labels/imagenet_labels.txt"\nDELEGATE_PATH = "libQnnTFLiteDelegate.so"\nDEVICE_OS="Ubuntu"\nUNAME = os.uname().nodename\n\nimport ai_edge_litert.interpreter as tflite\n\nif not os.path.exists(TF_MODEL):\n    print("Downloading model...")\n    model_url = \'https://huggingface.co/qualcomm/GoogLeNet/resolve/main/GoogLeNet_w8a8.tflite\'\n    urllib.request.urlretrieve(model_url, TF_MODEL)\n\n# ========= Helper Functions =========\ndef stable_softmax(logits):\n    # Convert logits to float64 for higher precision\n    logits = logits.astype(np.float32)\n    \n    # Subtract the maximum logit to prevent overflow\n    shifted_logits = logits - np.max(logits)\n    \n    # Clip the shifted logits to a safe range to prevent overflow in exp\n    shifted_logits = np.clip(shifted_logits, -500, 500)\n    \n    # Calculate the exponentials and normalize\n    exp_scores = np.exp(shifted_logits)\n    probabilities = exp_scores / np.sum(exp_scores)\n    \n    return probabilities\n\n# Load labels from file\ndef load_labels(label_path):\n    with open(label_path, \'r\') as f:\n        return [line.strip() for line in f.readlines()]\n\ndef resizeImage(pixbuf):\n    original_width = pixbuf.get_width()\n    original_height = pixbuf.get_height()\n\n    # Target display size\n    max_width = 800\n    max_height = 600\n\n    # Calculate new size preserving aspect ratio\n    scale = min(max_width / original_width, max_height / original_height)\n    new_width = int(original_width * scale)\n    new_height = int(original_height * scale)\n\n    return new_width, new_height\n\n# Load and preprocess input image\ndef preprocess_image(image_path, input_shape, input_dtype):\n    # Read the image using OpenCV\n    img = cv2.imread(image_path)\n    if img is None:\n        raise ValueError(f"Failed to load image at {image_path}")\n    # Convert BGR to RGB\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    # Resize the image to the desired input shape\n    img = cv2.resize(img, (input_shape[2], input_shape[1]))\n    # Convert to the desired data type\n    img = img.astype(input_dtype)\n    # Add batch dimension\n    img = np.expand_dims(img, axis=0)\n    \n    return img\n\n# ====== Inference Function ======\ndef runInference(image, use_delegate):\n    results = []    \n    print(f"Running on {DEVICE_OS} using Delegate:{use_delegate}")\n    if use_delegate:\n        try:\n            # Load the QNN delegate library\n            delegate_options = { \'backend_type\': \'htp\' }\n            delegate = tflite.load_delegate(DELEGATE_PATH, delegate_options)\n            \n            # Load the TFLite model\n            model = tflite.Interpreter(model_path=TF_MODEL, experimental_delegates=[delegate])\n            print("INFO: Loaded QNN delegate with HTP backend")\n        except Exception as e:\n            print(f"WARNING: Failed to load QNN delegate: {e}")\n            print("INFO: Continuing without QNN delegate")\n            model = tflite.Interpreter(model_path=TF_MODEL)   \n    else:\n        model = tflite.Interpreter(model_path=TF_MODEL)  \n   \n    model.allocate_tensors()\n\n    # Get and Prepare input \n    input_details = model.get_input_details()\n    input_shape = input_details[0][\'shape\']\n    input_dtype = input_details[0][\'dtype\']\n    input_data = preprocess_image(image, input_shape, input_dtype)\n    \n    # Load input data to input tensor\n    model.set_tensor(input_details[0][\'index\'], input_data)\n    model.get_signature_list()\n    \n    # Run inference\n    try:\n        start_time = time.time()\n        model.invoke()\n        end_time = time.time()\n        print("Interpreter invoked successfully.")\n    except Exception as e:\n        print(f"Error during model invocation: {e}")\n        return []\n\n    # Calculate and print duration\n    inference_time = end_time - start_time\n\n    # Prepare output tensor details\n    output_details = model.get_output_details()\n\n    # Load output data to output tensor\n    output_data = model.get_tensor(output_details[0][\'index\'])\n\n    # Load labels and get prediction\n    labels = load_labels(LABELS)\n    predicted_index = np.argmax(output_data)\n    predicted_label = labels[predicted_index]\n    print("Predicted index:", predicted_index)\n    print("Predicted label:", predicted_label)\n    \n    # Add Softmax function\n    logits = output_data[0]\n    probabilities = stable_softmax(logits)\n\n    # Get top 4 predictions\n    top_k = 4\n    top_indices = np.argsort(probabilities)[::-1][:top_k]\n    for i in top_indices:\n        result = (labels[i], probabilities[i] * 100)\n        results.append(result)\n\n    return results, inference_time\n\n# ====== GTK GUI Classes ======\nclass FileBrowser(Gtk.FileChooserDialog):\n    def __init__(self):\n        super().__init__(title="Choose an image", action=Gtk.FileChooserAction.OPEN)\n        self.add_buttons(Gtk.STOCK_CANCEL, Gtk.ResponseType.CANCEL, Gtk.STOCK_OPEN, Gtk.ResponseType.OK)\n\n    def run_and_get_file(self):\n        response = super().run()\n        if response == Gtk.ResponseType.OK:\n            print("Selected file:", self.get_filename())\n            self.selected_file = self.get_filename()            \n        self.destroy()\n        return self.selected_file\n\nclass MainWindow(Gtk.Window):\n    def __init__(self):\n        super().__init__(title="Image Classification")\n        self.set_default_size(800, 600)\n        self.imageFilepath = ""\n        # Main layout\n        self.mainBox = Gtk.Box(orientation=Gtk.Orientation.HORIZONTAL, spacing=10)\n        self.mainBox.set_margin_top(10)\n        self.mainBox.set_margin_bottom(10)\n        self.mainBox.set_margin_start(10)\n        self.mainBox.set_margin_end(10)\n        self.add(self.mainBox)\n        \n        # Main Window Image setup with fallback\n        self.image = Gtk.Image()\n        try:\n            MAIN_IMAGE = "MainWindowPic.jpg"\n            self.image.set_from_file(MAIN_IMAGE)         \n        except Exception as e:\n            print("Error loading main image:", e)\n            self.image.set_from_icon_name("image-missing", Gtk.IconSize.DIALOG)\n\n        self.mainBox.pack_start(self.image, True, True, 0)\n\n        # Set up a new box to add results and and file button\n        self.infoBox = Gtk.Box(orientation=Gtk.Orientation.VERTICAL, spacing=10)\n        \n        # Radio button to select Delegate\n        delegate_label = Gtk.Label(label="Select Inference Mode:")\n        self.infoBox.pack_start(delegate_label, False, False, 10)\n\n        self.cpu_radio = Gtk.RadioButton.new_with_label_from_widget(None, "CPU")\n        self.delegate_radio = Gtk.RadioButton.new_with_label_from_widget(self.cpu_radio, "Delegate")\n\n        self.infoBox.pack_start(self.cpu_radio, False, False, 0)\n        self.infoBox.pack_start(self.delegate_radio, False, False, 0)\n        \n        # Radio button signal\n        self.cpu_radio.connect("toggled", self.on_radio_toggled)\n        self.delegate_radio.connect("toggled", self.on_radio_toggled)\n\n        # Open file button\n        open_button = Gtk.Button(label="Select Image")\n        open_button.connect("clicked", self.on_open_file_clicked)\n        self.infoBox.pack_start(open_button, False, True, 10)\n\n        # Reprocess Image\n        reprocess_button = Gtk.Button(label="Reprocess Image")\n        reprocess_button.connect("clicked", self.on_reprocess_image_clicked)\n        self.infoBox.pack_start(reprocess_button, False, True, 10)\n\n        # Classification results\n        self.results = Gtk.Box(orientation=Gtk.Orientation.VERTICAL, spacing=10)\n        self.infoBox.pack_start(self.results, True, True, 0)\n        self.mainBox.pack_start(self.infoBox, True, True, 0)\n\n    def use_delegate(self):\n        return self.delegate_radio.get_active()\n\n    def on_radio_toggled(self, button):\n        if button.get_active():\n            print(f"Selected option: {button.get_label()}")\n\n    def process_file(self, filepath): \n        try:\n            # Resize Image\n            pixbuf = GdkPixbuf.Pixbuf.new_from_file(filepath)\n            new_width, new_height = resizeImage(pixbuf)\n            scaled_pixbuf = pixbuf.scale_simple(new_width, new_height, GdkPixbuf.InterpType.BILINEAR)\n            \n            # Replace the image with new image\n            self.image.set_from_pixbuf(scaled_pixbuf)\n           \n            # Run Inference\n            use_delegate = self.use_delegate()\n            print("delegate: " , use_delegate)\n            options, inference_time = runInference(filepath, use_delegate)\n\n            # Clear result box\n            for child in self.results.get_children():\n                self.results.remove(child)\n            \n            # Set up predictions\n            for label, percent in options:\n                textBox = Gtk.Box(orientation=Gtk.Orientation.HORIZONTAL, spacing=10)\n                barBox = Gtk.Box(orientation=Gtk.Orientation.HORIZONTAL, spacing=10)\n                text = Gtk.Label(label=label, xalign=0)\n                text.set_size_request(100, -1) \n                \n                bar = Gtk.ProgressBar()\n                bar.set_fraction(percent / 100.0)\n                bar.set_text(f"{percent:.2f}%")\n                bar.set_show_text(True)\n                \n                textBox.pack_start(text, False, False, 0)\n                barBox.pack_start(bar, True, True, 0)\n            \n                self.results.pack_start(textBox, False, False, 0)\n                self.results.pack_start(barBox, False, False, 0)\n                self.results.show_all()\n            \n            # Add inference time label\n            time_label = Gtk.Label(label=f"Inference Time : {inference_time * 1000:.2f} ms")\n            self.results.pack_start(time_label, False, False, 50)\n            self.results.show_all()\n        except Exception as e:\n            print("Error reading file:", e)\n\n    def on_open_file_clicked(self, widget):\n        dialog = FileBrowser()\n        selected_file = dialog.run_and_get_file()\n        self.imageFilepath = selected_file\n        if selected_file:\n            self.process_file(selected_file)\n \n    def on_reprocess_image_clicked(self, widget):\n        self.process_file(self.imageFilepath)\n\n    def on_destroy(self, widget):\n        Gtk.main_quit()\n\n# === Main Entry Point ===\ndef main():\n    app = MainWindow()\n    app.connect("destroy", Gtk.main_quit)\n    app.show_all()\n    Gtk.main()\n\nif __name__ == "__main__":\n    success, _ = Gtk.init_check()\n    if not success:\n        print("GTK could not be initialized. Check environmental variables")\n        exit(1)\n\n    main() \n\n'})}),(0,s.jsx)(n.p,{children:"\u73b0\u5728\uff0c\u60a8\u53ef\u4ee5\u4f7f\u7528\u4ee5\u4e0b\u547d\u4ee4\u5728 CPU/\u59d4\u6258\u4e0a\u8fd0\u884c\u8be5\u5e94\u7528\u7a0b\u5e8f\uff1a"}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-shell",children:"python3 classification.py\n"})}),(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{src:t(2491).A+"",width:"3397",height:"2736"})}),(0,s.jsx)(n.p,{children:"\u4ece\u4e92\u8054\u7f51\u4e0a\u4e0b\u8f7d\u4efb\u4e00\u56fe\u7247\u3002\u8be5\u793a\u4f8b\u4f7f\u7528\u4e86\u6d88\u9632\u8f66\u56fe\u7247\u3002\u901a\u8fc7 scp \u547d\u4ee4\u5c06\u56fe\u50cf\u590d\u5236\u5230\u8bbe\u5907\u4e0a\u3002"}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-shell",children:"scp xxx.jpg ubuntu@IP_address:/home/ubuntu/\n"})}),(0,s.jsx)(n.p,{children:"\u5728 GUI \u4e0a\u9009\u62e9 CPU \u4f5c\u4e3a\u8fd0\u884c\u65f6\u9009\u9879\uff1a"}),(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{src:t(7144).A+"",width:"3828",height:"2624"})}),(0,s.jsx)(n.p,{children:"\u5728 GUI \u4e0a\u9009\u62e9 Delegate \u4f5c\u4e3a\u8fd0\u884c\u65f6\u9009\u9879\uff1a"}),(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{src:t(2531).A+"",width:"3555",height:"2453"})})]}),(0,s.jsxs)(l.A,{value:"Example3",label:"Object Detection",children:[(0,s.jsx)(n.h3,{id:"object-detection-with-opencv--wayland-display",children:"Object Detection with OpenCV & Wayland Display"}),(0,s.jsxs)(n.p,{children:["\u8fd9\u4e2a Python \u811a\u672c\u4f7f\u7528\u91cf\u5316\u7684 ",(0,s.jsx)(n.strong,{children:"YOLOv8 TensorFlow Lite"})," \u6a21\u578b\u5bf9\u89c6\u9891\u6587\u4ef6\u8fdb\u884c",(0,s.jsx)(n.strong,{children:"\u5b9e\u65f6\u76ee\u6807\u68c0\u6d4b"}),"\uff0c\u5e76\u901a\u8fc7",(0,s.jsx)(n.strong,{children:"GStreamer \u5728 Wayland \u663e\u793a\u5668\u4e0a"}),"\u663e\u793a\u5e26\u6ce8\u91ca\u7684\u5e27\u3002\u5b83\u9488\u5bf9",(0,s.jsx)(n.strong,{children:"\u8fb9\u7f18 AI \u573a\u666f"}),"\u8fdb\u884c\u4e86\u4f18\u5316\uff0c\u901a\u8fc7",(0,s.jsx)(n.strong,{children:"QNN TFLite \u59d4\u6258"}),"\u5b9e\u73b0\u786c\u4ef6\u52a0\u901f\u3002"]}),(0,s.jsx)(n.admonition,{type:"note",children:(0,s.jsxs)(n.p,{children:["YOLOv8 \u6a21\u578b\u9ed8\u8ba4\u4e0d\u53ef\u7528\u3002\u8bf7\u6309\u7167 ",(0,s.jsx)(n.a,{href:"https://docs.qualcomm.com/bundle/publicresource/topics/80-70020-50/download-model-and-label-files.html?vproduct=1601111740013072&version=1.5&facet=Intelligent_Multimedia_SDK.SDK.2.0",children:(0,s.jsx)(n.strong,{children:"Qualcomm Intelligent Multimedia SDK"})})," ",(0,s.jsx)(n.strong,{children:"\u6b65\u9aa4-6"})," \u5bfc\u51fa YOLOv8 \u91cf\u5316\u6a21\u578b\u3002"]})}),(0,s.jsx)(n.p,{children:"\u4e0b\u4e00\u6b65\u662f\u5c06\u6a21\u578b\u63a8\u9001\u5230\u76ee\u6807\u8bbe\u5907\u4e0a\uff0c\u901a\u8fc7 scp \u547d\u4ee4\u5c06\u6a21\u578b\u590d\u5236\u5230\u8bbe\u5907\u4e0a\u3002"}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-shell",children:"scp xxxx.tflite ubuntu@IP_address:/home/ubuntu/\n"})}),(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"\u521d\u59cb\u5316\u548c\u914d\u7f6e"}),(0,s.jsx)(n.br,{}),"\n","\u2022\t\u5b9a\u4e49\u6a21\u578b\u3001\u6807\u7b7e\u3001\u8f93\u5165\u89c6\u9891\u548c\u59d4\u6258\u7684\u8def\u5f84\u3002",(0,s.jsx)(n.br,{}),"\n","\u2022\t\u4e3a\u9884\u5904\u7406\u548c\u540e\u5904\u7406\u8bbe\u7f6e\u5e27\u5c3a\u5bf8\u3001FPS\u3001\u7f6e\u4fe1\u5ea6\u9608\u503c\u548c\u7f29\u653e\u56e0\u5b50\u7b49\u5e38\u91cf\u3002"]}),(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"MODEL_PATH"}),' = "yolov8_det_quantized.tflite"',(0,s.jsx)(n.br,{}),"\n",(0,s.jsx)(n.strong,{children:"LABEL_PATH"}),' = "coco_labels.txt"',(0,s.jsx)(n.br,{}),"\n",(0,s.jsx)(n.strong,{children:"VIDEO_IN"}),' = "video.mp4"',(0,s.jsx)(n.br,{}),"\n",(0,s.jsx)(n.strong,{children:"DELEGATE_PATH"}),' = "libQnnTFLiteDelegate.so"']}),(0,s.jsxs)(n.admonition,{type:"note",children:[(0,s.jsx)(n.mdxAdmonitionTitle,{}),(0,s.jsxs)(n.p,{children:["\u4f7f\u7528\u4e0e\u5bf9\u8c61\u68c0\u6d4b\u6a21\u578b\u914d\u5408\u826f\u597d\u7684\u89c6\u9891\u6587\u4ef6\u3002",(0,s.jsx)(n.br,{}),"\n","\u4e3a\u4e86\u83b7\u5f97\u6700\u4f73\u6548\u679c\uff0c\u8bf7\u9009\u62e9\u4e3b\u9898\u6e05\u6670\u3001\u5149\u7ebf\u5145\u8db3\u4e14\u8fd0\u52a8\u6a21\u7cca\u6700\u5c11\u7684\u89c6\u9891\u3002"]}),(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"\u4f8b\u5982\uff1a"}),(0,s.jsx)(n.br,{}),"\n","\u2022 \u6709\u8f66\u8f86\u548c\u884c\u4eba\u7684\u8857\u9053\u573a\u666f",(0,s.jsx)(n.br,{}),"\n","\u2022 \u6709\u53ef\u89c1\u7269\u4f53\u7684\u4ed3\u5e93\u6216\u5de5\u5382\u8f66\u95f4",(0,s.jsx)(n.br,{}),"\n","\u2022 \u4eba\u7269\u6216\u4ea7\u54c1\u7684\u9759\u6001\u6444\u50cf\u673a\u753b\u9762"]})]}),(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"\u6a21\u578b\u52a0\u8f7d\u548c\u59d4\u6258\u8bbe\u7f6e"}),(0,s.jsx)(n.br,{}),"\n","\u2022\t\u52a0\u8f7d\u786c\u4ef6\u59d4\u6258\u4ee5\u52a0\u901f\u63a8\u7406\u3002",(0,s.jsx)(n.br,{}),"\n","\u2022\t\u4f7f\u7528\u91cf\u5316\u7684 YOLOv8 \u6a21\u578b\u521d\u59cb\u5316 TensorFlow Lite \u89e3\u91ca\u5668\u3002"]}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"delegate_options = { 'backend_type': 'htp' }\ndelegate = tflite.load_delegate(DELEGATE_PATH, delegate_options)\ninterpreter = tflite.Interpreter(model_path=MODEL_PATH, experimental_delegates=[delegate])\ninterpreter.allocate_tensors()\n"})}),(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"\u6807\u7b7e\u52a0\u8f7d"}),(0,s.jsx)(n.br,{}),"\n","\u2022\t\u52a0\u8f7d COCO \u6570\u636e\u96c6\u6807\u7b7e\u4ee5\u8fdb\u884c\u5bf9\u8c61\u6ce8\u91ca\u3002"]}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"labels = [l.strip() for l in open(LABEL_PATH)]\n"})}),(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"GStreamer \u7ba1\u9053\u8bbe\u7f6e"}),(0,s.jsx)(n.br,{}),"\n","\u2022\t\u4f7f\u7528 appsrc \u521b\u5efa GStreamer \u7ba1\u9053\uff0c\u5c06\u5e27\u6d41\u5f0f\u4f20\u8f93\u5230 Wayland \u63a5\u6536\u5668\u3002\u2022\t\u80fd\u591f\u5b9e\u65f6\u663e\u793a\u5df2\u5904\u7406\u7684\u5e27\u3002"]}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-shell",children:"pipeline = Gst.parse_launch(\n    'appsrc name=src is-live=true block=true format=time caps=video/x-raw,format=BGR,width=1600,height=900,framerate=30/1 ! videoconvert ! waylandsink')\n"})}),(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"\u89c6\u9891\u6355\u83b7\u548c\u5e27\u5904\u7406"}),(0,s.jsx)(n.br,{}),"\n","\u2022\t\u4f7f\u7528 OpenCV \u6253\u5f00\u89c6\u9891\u6587\u4ef6\u3002",(0,s.jsx)(n.br,{}),"\n","\u2022\t\u6bcf\u4e2a\u5e27\u90fd\u7ecf\u8fc7\u8c03\u6574\u5927\u5c0f\u548c\u9884\u5904\u7406\uff0c\u4ee5\u5339\u914d\u6a21\u578b\u7684\u8f93\u5165\u5c3a\u5bf8\u3002"]}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"cap = cv2.VideoCapture(VIDEO_IN)\n"})}),(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"\u63a8\u7406\u548c\u540e\u5904\u7406"}),(0,s.jsx)(n.br,{}),"\n","\u2022\t\u5bf9\u6bcf\u4e00\u5e27\u8fdb\u884c\u63a8\u7406\u3002",(0,s.jsx)(n.br,{}),"\n","\u2022\t\u4f7f\u7528\u9884\u5b9a\u4e49\u7684\u7f29\u653e\u56e0\u5b50\u548c\u96f6\u70b9\u5bf9\u6a21\u578b\u8f93\u51fa\u8fdb\u884c\u53bb\u91cf\u5316\u3002"]}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"interpreter.set_tensor(in_det[0]['index'], input_tensor)\ninterpreter.invoke()\nboxes_q = interpreter.get_tensor(out_det[0]['index'])[0]\nscores_q = interpreter.get_tensor(out_det[1]['index'])[0]\nclasses_q = interpreter.get_tensor(out_det[2]['index'])[0]\n"})}),(0,s.jsx)(n.p,{children:"\u2022\t\u5e94\u7528\u7f6e\u4fe1\u5ea6\u9608\u503c\u6765\u8fc7\u6ee4\u4f4e\u6982\u7387\u68c0\u6d4b\u3002\u2022\t\u4f7f\u7528\u975e\u6781\u5927\u6291\u5236\uff08NMS\uff09\u6765\u6d88\u9664\u91cd\u53e0\u7684\u6846\u3002"}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"mask = scores >= CONF_THRES\nboxes_f = boxes[mask]\nscores_f = scores[mask]\nclasses_f = classes[mask]\n"})}),(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"\u6ce8\u91ca\u4e0e\u663e\u793a"})}),(0,s.jsx)(n.p,{children:"\u2022\t\u4f7f\u7528 OpenCV \u5728\u753b\u9762\u4e0a\u7ed8\u5236\u8fb9\u754c\u6846\u548c\u6807\u7b7e\u3002\u2022\t\u6bcf 100 \u5e27\u8bb0\u5f55\u4e00\u6b21\u6700\u9ad8\u68c0\u6d4b\u5206\u6570\u3002"}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'cv2.rectangle(frame_rs, (x1i, y1i), (x2i, y2i), (0,255,0), 2)\ncv2.putText(frame_rs, f"{lab} {sc:.2f}", (x1i, max(10,y1i-5)), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,255,0), 2)\n'})}),(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"\u6d41\u5f0f\u4f20\u8f93\u81f3 Wayland \u663e\u793a\u5668"})}),(0,s.jsx)(n.p,{children:"\u5c06\u5e27\u8f6c\u6362\u4e3a GStreamer \u7f13\u51b2\u533a\uff0c\u5e76\u5c06\u5b83\u4eec\u5e26\u7740\u65f6\u95f4\u6233\u63a8\u9001\u5230\u7ba1\u9053\u4ee5\u5b9e\u73b0\u6d41\u7545\u64ad\u653e\u3002"}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"buf = Gst.Buffer.new_allocate(None, len(data), None)\nbuf.fill(0, data)\nbuf.duration = Gst.util_uint64_scale_int(1, Gst.SECOND, FPS_OUT)\ntimestamp = cap.get(cv2.CAP_PROP_POS_MSEC) * Gst.MSECOND\nbuf.pts = buf.dts = int(timestamp)\nappsrc.emit('push-buffer', buf)\n"})}),(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"\u5b8c\u6210"}),(0,s.jsx)(n.br,{}),"\n","\u5904\u7406\u5b8c\u6240\u6709\u5e27\u540e\u6b63\u5e38\u5173\u95ed\u7ba1\u9053\u3002"]}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"appsrc.emit('end-of-stream')\npipeline.set_state(Gst.State.NULL)\ncap.release()\n"})}),(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"\u7528\u4f8b"}),(0,s.jsx)(n.br,{}),"\n","\u6b64\u811a\u672c\u9002\u7528\u4e8e\uff1a",(0,s.jsx)(n.br,{}),"\n","\u2022\t\u667a\u80fd\u76f8\u673a",(0,s.jsx)(n.br,{}),"\n","\u2022\t\u673a\u5668\u4eba\u6280\u672f",(0,s.jsx)(n.br,{}),"\n","\u2022\t\u57fa\u4e8e Wayland GUI \u7684\u5d4c\u5165\u5f0f\u7cfb\u7edf",(0,s.jsx)(n.br,{}),"\n","\u2022\t\u8fb9\u7f18 AI \u90e8\u7f72\u4e2d\u7684\u5b9e\u65f6\u76d1\u63a7"]}),(0,s.jsx)(n.h3,{id:"\u53c2\u8003\u4ee3\u7801-1",children:"\u53c2\u8003\u4ee3\u7801"}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# -----------------------------------------------------------------------------\n#\n# Copyright (c) Qualcomm Technologies, Inc. and/or its subsidiaries.\n# SPDX-License-Identifier: BSD-3-Clause\n#\n# -----------------------------------------------------------------------------\n\n#!/usr/bin/env python3\n\n# Import necessary libraries\nimport cv2\nimport numpy as np\nimport gi\ngi.require_version('Gst', '1.0')\nfrom gi.repository import Gst\nimport ai_edge_litert.interpreter as tflite\n\n# Initialize GStreamer\nGst.init(None)\n\n# -------------------- Parameters --------------------\nMODEL_PATH = \"/home/ubuntu/yolov8_det_quantized.tflite\"  # Path to TFLite model\nLABEL_PATH = \"/etc/labels/coco_labels.txt\"              # Path to label file            # Path to label file\nVIDEO_IN = \"/etc/media/video.mp4\"                        # Input video file\nDELEGATE_PATH = \"libQnnTFLiteDelegate.so\"                # Delegate for hardware acceleration\n\n# Frame and model parameters\nFRAME_W, FRAME_H = 1600, 900\nFPS_OUT = 30\nCONF_THRES = 0.25\nNMS_IOU_THRES = 0.50\nBOX_SCALE = 3.2108588218688965\nBOX_ZP = 31.0\nSCORE_SCALE = 0.0038042240776121616\n# -------------------- Load Model --------------------\n# Load delegate for hardware acceleration\ndelegate_options = { 'backend_type': 'htp' }\ndelegate = tflite.load_delegate(DELEGATE_PATH, delegate_options)\n\n# Load and allocate TFLite interpreter\ninterpreter = tflite.Interpreter(model_path=MODEL_PATH, experimental_delegates=[delegate])\ninterpreter.allocate_tensors()\n\n# Get input/output tensor details\nin_det = interpreter.get_input_details()\nout_det = interpreter.get_output_details()\nin_h, in_w = in_det[0][\"shape\"][1:3]\n\n# -------------------- Load Labels --------------------\nlabels = [l.strip() for l in open(LABEL_PATH)]\n\n# -------------------- GStreamer Pipeline --------------------\n# Create GStreamer pipeline to display video via Wayland\npipeline = Gst.parse_launch(\n    'appsrc name=src is-live=true block=true format=time caps=video/x-raw,format=BGR,width=1600,height=900,framerate=30/1 ! videoconvert ! waylandsink'\n)\nappsrc = pipeline.get_by_name('src')\npipeline.set_state(Gst.State.PLAYING)\n\n# -------------------- Video Input --------------------\ncap = cv2.VideoCapture(VIDEO_IN)\n\n# Scaling factors for bounding box adjustment\nsx, sy = FRAME_W / in_w, FRAME_H / in_h\n\n# Preallocate frame buffers\nframe_rs = np.empty((FRAME_H, FRAME_W, 3), np.uint8)\ninput_tensor = np.empty((1, in_h, in_w, 3), np.uint8)\n\nframe_cnt = 0\n\n# -------------------- Main Loop --------------------\nwhile True:\n    ok, frame = cap.read()\n    if not ok:\n        break\n    frame_cnt += 1\n\n    # ---------- Preprocessing ----------\n    # Resize frame to display resolution\n    cv2.resize(frame, (FRAME_W, FRAME_H), dst=frame_rs)\n\n    # Resize again to model input resolution\n    cv2.resize(frame_rs, (in_w, in_h), dst=input_tensor[0])\n\n    # ---------- Inference ----------\n    # Set input tensor and run inference\n    interpreter.set_tensor(in_det[0]['index'], input_tensor)\n    interpreter.invoke()\n\n    # ---------- Postprocessing ----------\n    # Get raw output tensors\n    boxes_q = interpreter.get_tensor(out_det[0]['index'])[0]\n    scores_q = interpreter.get_tensor(out_det[1]['index'])[0]\n    classes_q = interpreter.get_tensor(out_det[2]['index'])[0]\n\n    # Dequantize outputs\n    boxes = BOX_SCALE * (boxes_q.astype(np.float32) - BOX_ZP)\n    scores = SCORE_SCALE * scores_q.astype(np.float32)\n    classes = classes_q.astype(np.int32)\n\n    # Filter by confidence threshold\n    mask = scores >= CONF_THRES\n    if np.any(mask):\n        boxes_f = boxes[mask]\n        scores_f = scores[mask]\n        classes_f = classes[mask]\n\n        # Convert boxes to OpenCV format\n        x1, y1, x2, y2 = boxes_f.T\n        boxes_cv2 = np.column_stack((x1, y1, x2 - x1, y2 - y1))\n\n        # Apply Non-Maximum Suppression\n        idx_cv2 = cv2.dnn.NMSBoxes(\n            bboxes=boxes_cv2.tolist(),\n            scores=scores_f.tolist(),\n            score_threshold=CONF_THRES,\n            nms_threshold=NMS_IOU_THRES\n        )\n\n        if len(idx_cv2):\n            idx = idx_cv2.flatten()\n            sel_boxes = boxes_f[idx]\n            sel_scores = scores_f[idx]\n            sel_classes = classes_f[idx]\n\n            # Debug print every 100 frames\n            if frame_cnt % 100 == 0:\n                print(f\"[{frame_cnt:4d}] max score = {sel_scores.max():.3f}\")\n\n            # Rescale boxes to display resolution\n            sel_boxes[:, [0,2]] *= sx\n            sel_boxes[:, [1,3]] *= sy\n            sel_boxes = sel_boxes.astype(np.int32)\n\n            # Clip boxes to frame boundaries\n            sel_boxes[:, [0,2]] = np.clip(sel_boxes[:, [0,2]], 0, FRAME_W-1)\n            sel_boxes[:, [1,3]] = np.clip(sel_boxes[:, [1,3]], 0, FRAME_H-1)\n\n            # Draw boxes and labels\n            for (x1i, y1i, x2i, y2i), sc, cl in zip(sel_boxes, sel_scores, sel_classes):\n                cv2.rectangle(frame_rs, (x1i, y1i), (x2i, y2i), (0,255,0), 2)\n                lab = labels[cl] if cl < len(labels) else str(cl)\n                cv2.putText(frame_rs, f\"{lab} {sc:.2f}\", (x1i, max(10,y1i-5)),\n                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,255,0), 2)\n\n    # ---------- Video Output ----------\n    # Convert frame to bytes and push to GStreamer pipeline\n    data = frame_rs.tobytes()\n    buf = Gst.Buffer.new_allocate(None, len(data), None)\n    buf.fill(0, data)\n    buf.duration = Gst.util_uint64_scale_int(1, Gst.SECOND, FPS_OUT)\n    timestamp = cap.get(cv2.CAP_PROP_POS_MSEC) * Gst.MSECOND\n    buf.pts = buf.dts = int(timestamp)\n    appsrc.emit('push-buffer', buf)\n\n# -------------------- Finish --------------------\nappsrc.emit('end-of-stream')\npipeline.set_state(Gst.State.NULL)\ncap.release()\nprint(\"Done \u2013 video streamed to Wayland sink\")\n"})}),(0,s.jsx)(n.p,{children:"\u73b0\u5728\uff0c\u60a8\u53ef\u4ee5\u4f7f\u7528\u4ee5\u4e0b\u547d\u4ee4\u5728 NPU\uff08\u59d4\u6258\uff09\u4e0a\u8fd0\u884c\u8be5\u5e94\u7528\u7a0b\u5e8f\uff1a"}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"python3 ObjectDetection.py\n"})}),(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{src:t(3244).A+"",width:"975",height:"548"})})]})]})]})}function _(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(u,{...e})}):u(e)}},7144:(e,n,t)=>{t.d(n,{A:()=>i});const i=t.p+"assets/images/IC-LiteRT-CPU-1b96c4321d92f2c1099cd85cb2a6ee8d.jpeg"},8453:(e,n,t)=>{t.d(n,{R:()=>a,x:()=>l});var i=t(6540);const s={},r=i.createContext(s);function a(e){const n=i.useContext(r);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:a(e.components),i.createElement(r.Provider,{value:n},e.children)}},9365:(e,n,t)=>{t.d(n,{A:()=>a});t(6540);var i=t(4164);const s={tabItem:"tabItem_Ymn6"};var r=t(4848);function a({children:e,hidden:n,className:t}){return(0,r.jsx)("div",{role:"tabpanel",className:(0,i.A)(s.tabItem,t),hidden:n,children:e})}}}]);