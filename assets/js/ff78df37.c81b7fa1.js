"use strict";(globalThis.webpackChunkmy_website=globalThis.webpackChunkmy_website||[]).push([[3786],{1470:(e,n,r)=>{r.d(n,{A:()=>T});var t=r(6540),i=r(4164),s=r(7559),a=r(3104),l=r(6347),o=r(205),d=r(7485),p=r(1682),c=r(679);function u(e){return t.Children.toArray(e).filter(e=>"\n"!==e).map(e=>{if(!e||(0,t.isValidElement)(e)&&function(e){const{props:n}=e;return!!n&&"object"==typeof n&&"value"in n}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)})?.filter(Boolean)??[]}function _(e){const{values:n,children:r}=e;return(0,t.useMemo)(()=>{const e=n??function(e){return u(e).map(({props:{value:e,label:n,attributes:r,default:t}})=>({value:e,label:n,attributes:r,default:t}))}(r);return function(e){const n=(0,p.XI)(e,(e,n)=>e.value===n.value);if(n.length>0)throw new Error(`Docusaurus error: Duplicate values "${n.map(e=>e.value).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e},[n,r])}function m({value:e,tabValues:n}){return n.some(n=>n.value===e)}function h({queryString:e=!1,groupId:n}){const r=(0,l.W6)(),i=function({queryString:e=!1,groupId:n}){if("string"==typeof e)return e;if(!1===e)return null;if(!0===e&&!n)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return n??null}({queryString:e,groupId:n});return[(0,d.aZ)(i),(0,t.useCallback)(e=>{if(!i)return;const n=new URLSearchParams(r.location.search);n.set(i,e),r.replace({...r.location,search:n.toString()})},[i,r])]}function g(e){const{defaultValue:n,queryString:r=!1,groupId:i}=e,s=_(e),[a,l]=(0,t.useState)(()=>function({defaultValue:e,tabValues:n}){if(0===n.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(e){if(!m({value:e,tabValues:n}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${e}" but none of its children has the corresponding value. Available values are: ${n.map(e=>e.value).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return e}const r=n.find(e=>e.default)??n[0];if(!r)throw new Error("Unexpected error: 0 tabValues");return r.value}({defaultValue:n,tabValues:s})),[d,p]=h({queryString:r,groupId:i}),[u,g]=function({groupId:e}){const n=function(e){return e?`docusaurus.tab.${e}`:null}(e),[r,i]=(0,c.Dv)(n);return[r,(0,t.useCallback)(e=>{n&&i.set(e)},[n,i])]}({groupId:i}),f=(()=>{const e=d??u;return m({value:e,tabValues:s})?e:null})();(0,o.A)(()=>{f&&l(f)},[f]);return{selectedValue:a,selectValue:(0,t.useCallback)(e=>{if(!m({value:e,tabValues:s}))throw new Error(`Can't select invalid tab value=${e}`);l(e),p(e),g(e)},[p,g,s]),tabValues:s}}var f=r(2303);const x={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};var b=r(4848);function j({className:e,block:n,selectedValue:r,selectValue:t,tabValues:s}){const l=[],{blockElementScrollPositionUntilNextRender:o}=(0,a.a_)(),d=e=>{const n=e.currentTarget,i=l.indexOf(n),a=s[i].value;a!==r&&(o(n),t(a))},p=e=>{let n=null;switch(e.key){case"Enter":d(e);break;case"ArrowRight":{const r=l.indexOf(e.currentTarget)+1;n=l[r]??l[0];break}case"ArrowLeft":{const r=l.indexOf(e.currentTarget)-1;n=l[r]??l[l.length-1];break}}n?.focus()};return(0,b.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,i.A)("tabs",{"tabs--block":n},e),children:s.map(({value:e,label:n,attributes:t})=>(0,b.jsx)("li",{role:"tab",tabIndex:r===e?0:-1,"aria-selected":r===e,ref:e=>{l.push(e)},onKeyDown:p,onClick:d,...t,className:(0,i.A)("tabs__item",x.tabItem,t?.className,{"tabs__item--active":r===e}),children:n??e},e))})}function v({lazy:e,children:n,selectedValue:r}){const s=(Array.isArray(n)?n:[n]).filter(Boolean);if(e){const e=s.find(e=>e.props.value===r);return e?(0,t.cloneElement)(e,{className:(0,i.A)("margin-top--md",e.props.className)}):null}return(0,b.jsx)("div",{className:"margin-top--md",children:s.map((e,n)=>(0,t.cloneElement)(e,{key:n,hidden:e.props.value!==r}))})}function y(e){const n=g(e);return(0,b.jsxs)("div",{className:(0,i.A)(s.G.tabs.container,"tabs-container",x.tabList),children:[(0,b.jsx)(j,{...n,...e}),(0,b.jsx)(v,{...n,...e})]})}function T(e){const n=(0,f.A)();return(0,b.jsx)(y,{...e,children:u(e.children)},String(n))}},2491:(e,n,r)=>{r.d(n,{A:()=>t});const t=r.p+"assets/images/IC-LiteRT-ce50f98bc8865d04a94ebdc42ab6106d.jpeg"},2531:(e,n,r)=>{r.d(n,{A:()=>t});const t=r.p+"assets/images/IC-LiteRT-NPU-338129fd2a6187b7ac81956600ccfd7f.jpeg"},3244:(e,n,r)=>{r.d(n,{A:()=>t});const t=r.p+"assets/images/OpenCV-OD-66beef4f3ce80e7fddf5a740b7706421.png"},5450:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>p,contentTitle:()=>d,default:()=>_,frontMatter:()=>o,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"Application Development and Execution Guide/Framework-Driven AI Sample Execution/litert_tflite","title":"LiteRT / TFLite","description":"LiteRT\uff08\u524d\u8eab\u4e3aTensorFlow Lite\uff09\uff0c\u662f\u8c37\u6b4c\u4e13\u4e3a\u8bbe\u5907\u7aefAI\u6253\u9020\u7684\u9ad8\u6027\u80fd\u63a8\u7406\u8fd0\u884c\u65f6\u3002\u901a\u8fc7\u96c6\u6210\u5728AI Engine Direct\u4e2d\u7684LiteRT\u59d4\u6258\uff0c\u60a8\u53ea\u9700\u4e00\u884c\u4ee3\u7801\u5373\u53ef\u5728Dragonwing\u8bbe\u5907\u7684NPU\u4e0a\u8fd0\u884c\u73b0\u6709\u7684\u91cf\u5316LiteRT\u6a21\u578b\uff08\u652f\u6301Python\u4e0eC++\uff09\u3002","source":"@site/docs/7.Application Development and Execution Guide/2.Framework-Driven AI Sample Execution/3.litert_tflite.md","sourceDirName":"7.Application Development and Execution Guide/2.Framework-Driven AI Sample Execution","slug":"/Application Development and Execution Guide/Framework-Driven AI Sample Execution/litert_tflite","permalink":"/ubuntu100-cn.github.io/docs/Application Development and Execution Guide/Framework-Driven AI Sample Execution/litert_tflite","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/7.Application Development and Execution Guide/2.Framework-Driven AI Sample Execution/3.litert_tflite.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Qualcomm\xae AI Hub","permalink":"/ubuntu100-cn.github.io/docs/Application Development and Execution Guide/Building AI Models/qualcomm_ai_hub"},"next":{"title":"ONNX","permalink":"/ubuntu100-cn.github.io/docs/Application Development and Execution Guide/Framework-Driven AI Sample Execution/onnx"}}');var i=r(4848),s=r(8453),a=r(1470),l=r(9365);const o={},d="LiteRT / TFLite",p={},c=[{value:"\u91cf\u5316\u6a21\u578b",id:"\u91cf\u5316\u6a21\u578b",level:2},{value:"\u5728 NPU \u4e0a\u8fd0\u884c\u6a21\u578b\uff08Python\uff09",id:"\u5728-npu-\u4e0a\u8fd0\u884c\u6a21\u578bpython",level:2},{value:"\u5728 NPU \u4e0a\u8fd0\u884c\u6a21\u578b\uff08C++\uff09",id:"\u5728-npu-\u4e0a\u8fd0\u884c\u6a21\u578bc",level:2},{value:"Python \u793a\u4f8b",id:"python-\u793a\u4f8b",level:3},{value:"Vision Transformers",id:"vision-transformers",level:3},{value:"\u57fa\u4e8eGTK\u7684\u56fe\u50cf\u5206\u7c7b\u5e94\u7528",id:"\u57fa\u4e8egtk\u7684\u56fe\u50cf\u5206\u7c7b\u5e94\u7528",level:3},{value:"\u4e0b\u8f7d TFlite \u6a21\u578b",id:"\u4e0b\u8f7d-tflite-\u6a21\u578b",level:3},{value:"\u8f85\u52a9\u51fd\u6570",id:"\u8f85\u52a9\u51fd\u6570",level:3},{value:"\u53c2\u8003\u4ee3\u7801",id:"\u53c2\u8003\u4ee3\u7801",level:3},{value:"Object Detection with OpenCV &amp; Wayland Display",id:"object-detection-with-opencv--wayland-display",level:3},{value:"\u53c2\u8003\u4ee3\u7801",id:"\u53c2\u8003\u4ee3\u7801-1",level:3}];function u(e){const n={a:"a",admonition:"admonition",br:"br",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",img:"img",li:"li",mdxAdmonitionTitle:"mdxAdmonitionTitle",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"litert--tflite",children:"LiteRT / TFLite"})}),"\n",(0,i.jsx)(n.p,{children:"LiteRT\uff08\u524d\u8eab\u4e3aTensorFlow Lite\uff09\uff0c\u662f\u8c37\u6b4c\u4e13\u4e3a\u8bbe\u5907\u7aefAI\u6253\u9020\u7684\u9ad8\u6027\u80fd\u63a8\u7406\u8fd0\u884c\u65f6\u3002\u901a\u8fc7\u96c6\u6210\u5728AI Engine Direct\u4e2d\u7684LiteRT\u59d4\u6258\uff0c\u60a8\u53ea\u9700\u4e00\u884c\u4ee3\u7801\u5373\u53ef\u5728Dragonwing\u8bbe\u5907\u7684NPU\u4e0a\u8fd0\u884c\u73b0\u6709\u7684\u91cf\u5316LiteRT\u6a21\u578b\uff08\u652f\u6301Python\u4e0eC++\uff09\u3002"}),"\n",(0,i.jsx)(n.h2,{id:"\u91cf\u5316\u6a21\u578b",children:"\u91cf\u5316\u6a21\u578b"}),"\n",(0,i.jsxs)(n.p,{children:["NPU\u4ec5\u652f\u6301uint8/int8\u91cf\u5316\u6a21\u578b\u3002\u4e0d\u652f\u6301\u7684\u6a21\u578b\u6216\u4e0d\u652f\u6301\u7684\u5c42\u5c06\u81ea\u52a8\u79fb\u56de CPU\u3002\u53ef\u4ee5\u4f7f\u7528 ",(0,i.jsx)(n.a,{href:"https://www.tensorflow.org/model_optimization/guide/quantization/training_comprehensive_guide",children:"quantization-aware training"})," \u6216 ",(0,i.jsx)(n.a,{href:"https://ai.google.dev/edge/litert/models/post_training_quantization",children:"post-training quantization"})," \u6765\u91cf\u5316 LiteRT \u6a21\u578b\u3002\u8bf7\u786e\u4fdd\u9075\u5faa\u201c\u5b8c\u5168\u6574\u578b\u91cf\u5316\u201d\u7684\u64cd\u4f5c\u6b65\u9aa4\u3002"]}),"\n",(0,i.jsxs)(n.admonition,{type:"info",children:[(0,i.jsx)(n.mdxAdmonitionTitle,{}),(0,i.jsxs)(n.p,{children:["**\u4e0d\u60f3\u81ea\u5df1\u91cf\u5316\u6a21\u578b\uff1f**\u53ef\u4ee5\u4ece ",(0,i.jsx)(n.a,{href:"https://aihub.qualcomm.com",children:"Qualcomm AI Hub"})," \u4e0b\u8f7d\u4e00\u7cfb\u5217\u7684\u91cf\u5316\u6a21\u578b\uff0c\u6216\u4f7f\u7528 ",(0,i.jsx)(n.a,{href:"https://qc-ai-test.gitbook.io/qc-ai-test-docs/running-building-ai-models/edge-impulse",children:"Edge Impulse"})," \u6765\u91cf\u5316\u5df2\u6709\u7684\u6216\u5168\u65b0\u7684\u6a21\u578b\u3002"]})]}),"\n",(0,i.jsx)(n.h2,{id:"\u5728-npu-\u4e0a\u8fd0\u884c\u6a21\u578bpython",children:"\u5728 NPU \u4e0a\u8fd0\u884c\u6a21\u578b\uff08Python\uff09"}),"\n",(0,i.jsx)(n.p,{children:"\u8981\u5c06\u6a21\u578b\u5378\u8f7d\u5230 NPU\uff0c\u53ea\u9700\u52a0\u8f7d LiteRT \u59d4\u6258\uff0c\u5e76\u5c06\u5176\u4f20\u9012\u5230\u89e3\u91ca\u5668\u4e2d\u3002\u4f8b\u5982\uff1a"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-py",children:'from ai_edge_litert.interpreter import Interpreter, load_delegate\r\n\r\nqnn_delegate = load_delegate("libQnnTFLiteDelegate.so", options={"backend_type": "htp"})\r\ninterpreter = Interpreter(\r\n    model_path=...,\r\n    experimental_delegates=[qnn_delegate]\r\n)\n'})}),"\n",(0,i.jsx)(n.h2,{id:"\u5728-npu-\u4e0a\u8fd0\u884c\u6a21\u578bc",children:"\u5728 NPU \u4e0a\u8fd0\u884c\u6a21\u578b\uff08C++\uff09"}),"\n",(0,i.jsx)(n.p,{children:"\u8981\u5c06\u6a21\u578b\u5378\u8f7d\u5230 NPU\uff0c\u9996\u5148\u9700\u8981\u6dfb\u52a0\u4ee5\u4e0b\u7f16\u8bd1\u6807\u5fd7\uff1a"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-makefile",children:"CFLAGS += -I${QNN_SDK_ROOT}/include\r\nLDFLAGS += -L${QNN_SDK_ROOT}/lib/aarch64-ubuntu-gcc9.4 -lQnnTFLiteDelegate\n"})}),"\n",(0,i.jsx)(n.p,{children:"\u7136\u540e\uff0c\u5b9e\u4f8b\u5316 LiteRT \u59d4\u6258\u5e76\u5c06\u5176\u4f20\u9012\u7ed9 LiteRT \u89e3\u91ca\u5668\uff1a"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-c",children:'// == Includes ==\r\n#include "QNN/TFLiteDelegate/QnnTFLiteDelegate.h"\r\n\r\n// == Application code ==\r\n\r\n// Get your interpreter...\r\ntflite::Interpreter *interpreter = ...;\r\n\r\n// Create QNN Delegate options structure.\r\nTfLiteQnnDelegateOptions options = TfLiteQnnDelegateOptionsDefault();\r\n\r\n// Set the mandatory backend_type option. All other options have default values.\r\noptions.backend_type = kHtpBackend;\r\n\r\n// Instantiate delegate. Must not be freed until interpreter is freed.\r\nTfLiteDelegate* delegate = TfLiteQnnDelegateCreate(&options);\r\n\r\nTfLiteStatus status = interpreter->ModifyGraphWithDelegate(delegate);\r\n// Check that status == kTfLiteOk\n'})}),"\n",(0,i.jsx)(n.h3,{id:"python-\u793a\u4f8b",children:"Python \u793a\u4f8b"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"\u524d\u63d0\u6761\u4ef6"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Ubuntu \u64cd\u4f5c\u7cfb\u7edf"})," \u5df2\u5237\u5165\u3002"]}),"\n",(0,i.jsxs)(n.li,{children:["\u5177\u6709\u9002\u5f53\u6743\u9650\u7684",(0,i.jsx)(n.strong,{children:"\u7ec8\u7aef\u8bbf\u95ee"}),"\u3002"]}),"\n",(0,i.jsxs)(n.li,{children:["\u5982\u679c\u60a8\u4e4b\u524d\u6ca1\u6709\u5b89\u88c5\u8fc7 PPA \u5305\uff0c\u8bf7\u6309\u7167\u4ee5\u4e0b\u6b65\u9aa4\u8fdb\u884c\u5b89\u88c5\u3002","\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-shell",children:"  git clone -b ubuntu_setup --single-branch https://github.com/rubikpi-ai/rubikpi-script.git \r\n  cd rubikpi-script  \r\n  ./install_ppa_pkgs.sh  \n"})}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\u5728\u5f00\u53d1\u677f\u4e0a\u6253\u5f00\u7ec8\u7aef\uff0c\u6216\u5efa\u7acb SSH \u4f1a\u8bdd\uff0c\u7136\u540e\u6267\u884c\u4ee5\u4e0b\u64cd\u4f5c\uff1a",(0,i.jsx)(n.br,{}),"\n","\u521b\u5efa\u4e00\u4e2a\u65b0\u7684\u865a\u62df\u73af\u5883\uff08venv\uff09\uff0c\u5e76\u5b89\u88c5 LiteRT \u8fd0\u884c\u65f6\u548c Pillow\uff1a","\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"python3 -m venv .venv-litert-demo --system-site-packages\r\nsource .venv-litert-demo/bin/activate\r\npip3 install ai-edge-litert==1.3.0 Pillow\r\npip3 install opencv-python\n"})}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\u5b89\u88c5\u5fc5\u8981\u7684 python3 \u548c gtk \u5305\u3002","\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"sudo apt install python3-gi python3-gi-cairo gir1.2-gtk-3.0\r\nsudo apt install python3-venv python3-full\r\nsudo apt install -y pkg-config cmake libcairo2-dev\r\nsudo apt install libgirepository1.0-dev gir1.2-glib-2.0\r\nsudo apt install build-essential python3-dev python3-pip pkg-config meson\n"})}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(a.A,{children:[(0,i.jsxs)(l.A,{value:"Example1",label:"Vision Transformers",children:[(0,i.jsx)(n.h3,{id:"vision-transformers",children:"Vision Transformers"}),(0,i.jsxs)(n.p,{children:["\u4ee5\u4e0b\u8bf4\u660e\u5982\u4f55\u4f7f\u7528 LiteRT \u59d4\u6258\u5728 CPU \u548c NPU \u4e0a\u8fd0\u884c Vision Transformer \u6a21\u578b\uff08\u4ece ",(0,i.jsx)(n.a,{href:"https://aihub.qualcomm.com/models/vit",children:"AI Hub"})," \u4e0b\u8f7d)\u3002"]}),(0,i.jsxs)(n.p,{children:["1\ufe0f\u20e3 \u521b\u5efa ",(0,i.jsx)(n.code,{children:"inference_vit.py"})," \u5e76\u6dfb\u52a0\u4ee5\u4e0b\u53c2\u8003\u4ee3\u7801\uff1a"]}),(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-py",children:'import numpy as np\r\nfrom ai_edge_litert.interpreter import Interpreter, load_delegate\r\nfrom PIL import Image\r\nimport os, time, sys\r\nimport urllib.request\r\n\r\ndef curr_ms():\r\n    return round(time.time() * 1000)\r\n\r\nuse_npu = True if len(sys.argv) >= 2 and sys.argv[1] == \'--use-npu\' else False\r\n\r\n# Path to your quantized TFLite model and test image (will be download automatically)\r\nMODEL_PATH = "vit-vit-w8a8.tflite"\r\nIMAGE_PATH = "boa-constrictor.jpg"\r\nLABELS_PATH = "vit-vit-labels.txt"\r\n\r\nif not os.path.exists(MODEL_PATH):\r\n    print("Downloading model...")\r\n    model_url = \'https://cdn.edgeimpulse.com/qc-ai-docs/models/vit-vit-w8a8.tflite\'\r\n    urllib.request.urlretrieve(model_url, MODEL_PATH)\r\n\r\nif not os.path.exists(LABELS_PATH):\r\n    print("Downloading labels...")\r\n    labels_url = \'https://cdn.edgeimpulse.com/qc-ai-docs/models/vit-vit-labels.txt\'\r\n    urllib.request.urlretrieve(labels_url, LABELS_PATH)\r\n\r\nif not os.path.exists(IMAGE_PATH):\r\n    print("Downloading image...")\r\n    image_url = \'https://cdn.edgeimpulse.com/qc-ai-docs/examples/boa-constrictor.jpg\'\r\n    urllib.request.urlretrieve(image_url, IMAGE_PATH)\r\n\r\nwith open(LABELS_PATH, \'r\') as f:\r\n    labels = [line for line in f.read().splitlines() if line.strip()]\r\n\r\nexperimental_delegates = []\r\nif use_npu:\r\n    experimental_delegates = [load_delegate("libQnnTFLiteDelegate.so", options={"backend_type": "htp"})]\r\n\r\n# Load TFLite model and allocate tensors\r\ninterpreter = Interpreter(\r\n    model_path=MODEL_PATH,\r\n    experimental_delegates=experimental_delegates\r\n)\r\ninterpreter.allocate_tensors()\r\n\r\n# Get input and output tensor details\r\ninput_details = interpreter.get_input_details()\r\noutput_details = interpreter.get_output_details()\r\n\r\n# Load and preprocess image\r\ndef load_image(path, input_shape):\r\n    # Expected input shape: [1, height, width, channels]\r\n    _, height, width, channels = input_shape\r\n\r\n    img = Image.open(path).convert("RGB").resize((width, height))\r\n    img_np = np.array(img, dtype=np.uint8)  # quantized models expect uint8\r\n    img_np = np.expand_dims(img_np, axis=0)\r\n    return img_np\r\n\r\ninput_shape = input_details[0][\'shape\']\r\ninput_data = load_image(IMAGE_PATH, input_shape)\r\n\r\n# Set tensor and run inference\r\ninterpreter.set_tensor(input_details[0][\'index\'], input_data)\r\n\r\n# Run once to warmup\r\ninterpreter.invoke()\r\n\r\n# Then run 10x\r\nstart = curr_ms()\r\nfor i in range(0, 10):\r\n    interpreter.invoke()\r\nend = curr_ms()\r\n\r\n# Get prediction\r\nq_output = interpreter.get_tensor(output_details[0][\'index\'])\r\nscale, zero_point = output_details[0][\'quantization\']\r\nf_output = (q_output.astype(np.float32) - zero_point) * scale\r\n\r\n# Image classification models in AI Hub miss a Softmax() layer at the end of the model, so add it manually\r\ndef softmax(x, axis=-1):\r\n    # subtract max for numerical stability\r\n    x_max = np.max(x, axis=axis, keepdims=True)\r\n    e_x = np.exp(x - x_max)\r\n    return e_x / np.sum(e_x, axis=axis, keepdims=True)\r\n\r\n# show top-5 predictions\r\nscores = softmax(f_output[0])\r\ntop_k = scores.argsort()[-5:][::-1]\r\nprint("\\nTop-5 predictions:")\r\nfor i in top_k:\r\n    print(f"Class {labels[i]}: score={scores[i]}")\r\n\r\nprint(\'\')\r\nprint(f\'Inference took (on average): {(end - start) / 10}ms. per image\')\n'})}),(0,i.jsx)(n.p,{children:"2\ufe0f\u20e3 \u5728 CPU \u4e0a\u8fd0\u884c\u6a21\u578b\uff1a"}),(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"python3 inference_vit.py\r\n\r\n# INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\r\n#\r\n# Top-5 predictions:\r\n# Class boa constrictor: score=0.6264431476593018\r\n# Class rock python: score=0.047579940408468246\r\n# Class night snake: score=0.006721484009176493\r\n# Class mouse: score=0.0022421202156692743\r\n# Class pick: score=0.001942973816767335\r\n#\r\n# Inference took (on average): 391.1ms. per image\n"})}),(0,i.jsx)(n.p,{children:"3\ufe0f\u20e3 \u5728 NPU \u4e0a\u8fd0\u884c\u6a21\u578b\uff1a"}),(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"python3 inference_vit.py --use-npu\r\n\r\n# INFO: TfLiteQnnDelegate delegate: 1382 nodes delegated out of 1633 nodes with 27 partitions.\r\n#\r\n# INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\r\n#\r\n# Top-5 predictions:\r\n# Class boa constrictor: score=0.6113042235374451\r\n# Class rock python: score=0.038359832018613815\r\n# Class night snake: score=0.011630792170763016\r\n# Class mouse: score=0.002294909441843629\r\n# Class lens cap: score=0.0018960189772769809\r\n#\r\n# Inference took (on average): 132.7ms. per image\n"})}),(0,i.jsx)(n.p,{children:"\u6b63\u5982\u6240\u89c1\uff0c\u8be5\u6a21\u578b\u5728 NPU \u4e0a\u8fd0\u884c\u901f\u5ea6\u660e\u663e\u66f4\u5feb\uff0c\u4f46\u6a21\u578b\u7684\u8f93\u51fa\u7565\u6709\u53d8\u5316\u3002\u6b64\u5916\uff0c\u6b64\u6a21\u578b\u5e76\u975e\u6240\u6709\u5c42\u90fd\u53ef\u4ee5\u5728 NPU \u4e0a\u8fd0\u884c\uff08\u201c1633 \u4e2a\u8282\u70b9\u4e2d\u59d4\u6258\u4e86 1382 \u4e2a\u8282\u70b9\uff0c\u5171 27 \u4e2a\u5206\u533a\u201d\uff09\u3002"})]}),(0,i.jsxs)(l.A,{value:"Example2",label:"Image Classification",children:[(0,i.jsx)(n.h3,{id:"\u57fa\u4e8egtk\u7684\u56fe\u50cf\u5206\u7c7b\u5e94\u7528",children:"\u57fa\u4e8eGTK\u7684\u56fe\u50cf\u5206\u7c7b\u5e94\u7528"}),(0,i.jsx)(n.p,{children:"\u4ee5\u4e0b\u4ecb\u7ecd\u5982\u4f55\u901a\u8fc7\u57fa\u4e8eGTK\u7684\u684c\u9762\u5e94\u7528\u7a0b\u5e8f\uff0c\u8fd0\u7528AI Engine Direct\u7684LiteRT\u59d4\u6258\uff0c\u5728CPU\u548cNPU\u4e0a\u8fd0\u884c\u4eceAI Hub\u4e0b\u8f7d\u7684\u56fe\u50cf\u5206\u7c7b\u6a21\u578b\u3002"}),(0,i.jsx)(n.p,{children:"GoogLeNet_w8a8.tflite\u6a21\u578b\u6765\u81eaAI-Hub\uff0c\u5229\u7528\u4e86TensorFlow Lite\u4e0eQNN\u4ee3\u7406\u52a0\u901f\u6280\u672f\uff0c\u5b9e\u73b0\u9ad8\u6548\u7684\u8bbe\u5907\u7aef\u63a8\u7406\u3002"}),(0,i.jsxs)(n.p,{children:["\u8bf7\u6309\u4ee5\u4e0b\u6b65\u9aa4\u521b\u5efa\u56fe\u50cf\u5206\u7c7b\u5e94\u7528\u7a0b\u5e8f\u3002",(0,i.jsx)(n.br,{}),"\n","\u6982\u8ff0",(0,i.jsx)(n.br,{}),"\n","\xb7\u7c7b\u578b\uff1a\u684c\u9762 GUI \u5e94\u7528\u7a0b\u5e8f",(0,i.jsx)(n.br,{}),"\n","\xb7\u529f\u80fd\uff1a\u4f7f\u7528 TFLite \u8fdb\u884c\u56fe\u50cf\u5206\u7c7b",(0,i.jsx)(n.br,{}),"\n","\u2022\u6a21\u5f0f\uff1aCPU \u548c QNN \u59d4\u6258",(0,i.jsx)(n.br,{}),"\n","\u2022\u63a5\u53e3\uff1a\u57fa\u4e8e GTK \u7684 GUI",(0,i.jsx)(n.br,{}),"\n","\u2022\u8f93\u51fa\uff1a\u5e26\u6709\u7f6e\u4fe1\u5ea6\u6761\u7684\u6700\u4f73\u9884\u6d4b"]}),(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"\u73af\u5883\u8bbe\u7f6e\u548c\u5bfc\u5165"}),(0,i.jsx)(n.br,{}),"\n","\u5728\u6b64\u6b65\u9aa4\u4e2d\uff0c\u811a\u672c\u8bbe\u7f6e\u4e0e\u663e\u793a\u76f8\u5173\u7684\u73af\u5883\u53d8\u91cf\uff08\u9488\u5bf9 Linux \u7cfb\u7edf\uff09\u5e76\u5bfc\u5165\u5fc5\u8981\u7684\u5e93\uff0c\u5982 OpenCV\u3001NumPy\u3001GTK \u548c TensorFlow Lite\u3002"]}),(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"import cv2, numpy as np, os, time\r\nfrom gi.repository import Gtk, GLib, GdkPixbuf\r\nimport ai_edge_litert.interpreter as tflite\n"})}),(0,i.jsx)(n.admonition,{type:"note",children:(0,i.jsx)(n.p,{children:"GTK \u7528\u4e8e GUI\uff0cOpenCV \u7528\u4e8e\u56fe\u50cf\u5904\u7406\uff0cTensorFlow Lite \u7528\u4e8e\u63a8\u7406\u3002"})}),(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"\u914d\u7f6e\u5e38\u91cf"}),(0,i.jsx)(n.br,{}),"\n","\u8fd9\u4e9b\u5e38\u91cf\u5b9a\u4e49\u4e86\u6a21\u578b\u3001\u6807\u7b7e\u6587\u4ef6\u548c\u59d4\u6258\u5e93\u7684\u8def\u5f84\u3002"]}),(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'TF_MODEL = "/home/ubuntu/GoogLeNet_w8a8.tflite"\r\nLABELS = "/etc/labels/imagenet_labels.txt"\r\nDELEGATE_PATH = "libQnnTFLiteDelegate.so"\r\nDEVICE_OS = "Ubuntu"\n'})}),(0,i.jsx)(n.h3,{id:"\u4e0b\u8f7d-tflite-\u6a21\u578b",children:"\u4e0b\u8f7d TFlite \u6a21\u578b"}),(0,i.jsx)(n.p,{children:"\u8be5\u811a\u672c\u68c0\u67e5\u672c\u5730\u662f\u5426\u5b58\u5728 TensorFlow Lite \u6a21\u578b\u6587\u4ef6\uff0c\u5982\u679c\u4e0d\u5b58\u5728\uff0c\u5219\u4ece\u6307\u5b9a\u7684 Hugging Face URL \u4e0b\u8f7d\u3002"}),(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"import urllib.request\r\nif not os.path.exists(TF_MODEL):\r\n   print(\"Downloading model...\")\r\n   model_url = 'https://huggingface.co/qualcomm/GoogLeNet/resolve/main/GoogLeNet_w8a8.tflite'\r\n   urllib.request.urlretrieve(model_url, TF_MODEL)\n"})}),(0,i.jsx)(n.h3,{id:"\u8f85\u52a9\u51fd\u6570",children:"\u8f85\u52a9\u51fd\u6570"}),(0,i.jsx)(n.p,{children:"\u6b64\u6b65\u9aa4\u4f7f\u7528 LiteRT \u548c GTK \u8bbe\u7f6e\u56fe\u50cf\u5206\u7c7b\u7684\u6838\u5fc3\u903b\u8f91\u548c\u754c\u9762\u3002"}),(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Softmax \u8ba1\u7b97"}),(0,i.jsx)(n.br,{}),"\n","\u5728\u5c06 logits \u8f6c\u6362\u4e3a\u6982\u7387\u65f6\uff0c\u786e\u4fdd\u6570\u503c\u7684\u7a33\u5b9a\u6027\u3002"]}),(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"Pythondef stable_softmax(logits):    \r\nlogits = logits.astype(np.float32)    \r\nshifted_logits = np.clip(logits - np.max(logits), -500, 500)    \r\nexp_scores = np.exp(shifted_logits)    \r\nreturn exp_scores / np.sum(exp_scores)\n"})}),(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"\u6807\u7b7e\u52a0\u8f7d\u5668"}),(0,i.jsx)(n.br,{}),"\n","\u4ece\u6587\u672c\u6587\u4ef6\u52a0\u8f7d\u7c7b\u6807\u7b7e\u3002"]}),(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"Pythondef load_labels(label_path):    \r\nwith open(label_path, 'r') as f: \r\n       return [line.strip() for line in f.readlines()]\n"})}),(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"\u56fe\u50cf\u9884\u5904\u7406"}),(0,i.jsx)(n.br,{}),"\n","\u4e3a\u6a21\u578b\u8f93\u5165\u51c6\u5907\u56fe\u50cf\uff1a\u8c03\u6574\u5927\u5c0f\u3001\u989c\u8272\u8f6c\u6362\u548c\u91cd\u5851\u3002"]}),(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"Pythondef preprocess_image(image_path, input_shape, input_dtype):   \r\nimg = cv2.imread(image_path)    \r\nimg = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)    \r\nimg = cv2.resize(img, (input_shape[2], input_shape[1]))    \r\nimg = img.astype(input_dtype)    \r\nreturn np.expand_dims(img, axis=0)\n"})}),(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"\u63a8\u7406\u6267\u884c"}),(0,i.jsx)(n.br,{}),"\n","\u8be5\u51fd\u6570\u5c06\uff1a",(0,i.jsx)(n.br,{}),"\n","\u2022\t\u52a0\u8f7d\u6a21\u578b\uff08\u5e26\u6216\u4e0d\u5e26\u59d4\u6258\uff09",(0,i.jsx)(n.br,{}),"\n","\u2022\t\u51c6\u5907\u8f93\u5165",(0,i.jsx)(n.br,{}),"\n","\u2022\t\u8fd0\u884c\u63a8\u7406",(0,i.jsx)(n.br,{}),"\n","\u2022\t\u5e94\u7528 softmax",(0,i.jsx)(n.br,{}),"\n","\u2022\t\u8fd4\u56de\u5177\u6709\u7f6e\u4fe1\u5ea6\u5206\u6570\u7684\u524d 4 \u4e2a\u9884\u6d4b"]}),(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"def runInference(image, use_delegate):\r\n    if use_delegate:\r\n        try:\r\n            delegate = tflite.load_delegate(DELEGATE_PATH, {'backend_type': 'htp'})\r\n            model = tflite.Interpreter(model_path=TF_MODEL, experimental_delegates=[delegate])\r\n        except:\r\n            model = tflite.Interpreter(model_path=TF_MODEL)\r\n    else:\r\n        model = tflite.Interpreter(model_path=TF_MODEL)\r\n    model.allocate_tensors()\r\n    input_details = model.get_input_details()\r\n    input_data = preprocess_image(image, input_details[0]['shape'], input_details[0]['dtype'])\r\n    model.set_tensor(input_details[0]['index'], input_data)\r\n    start_time = time.time()\r\n    model.invoke()\r\n    inference_time = time.time() - start_time\r\n    output_data = model.get_tensor(model.get_output_details()[0]['index'])\r\n    probabilities = stable_softmax(output_data[0])\r\n    labels = load_labels(LABELS)\r\n    top_indices = np.argsort(probabilities)[::-1][:4]\r\n    results = [(labels[i], probabilities[i] * 100) for i in top_indices]\r\n    return results, inference_time\n"})}),(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"GTK GUI \u7ec4\u4ef6"})}),(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"\u6587\u4ef6\u6d4f\u89c8\u5668\u5bf9\u8bdd\u6846"}),(0,i.jsx)(n.br,{}),"\n","\u5141\u8bb8\u7528\u6237\u9009\u62e9\u56fe\u50cf\u6587\u4ef6\u3002"]}),(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'class FileBrowser(Gtk.FileChooserDialog):\r\n    def __init__(self):\r\n        super().__init__(title="Choose an image", action=Gtk.FileChooserAction.OPEN)\r\n        self.add_buttons(Gtk.STOCK_CANCEL, Gtk.ResponseType.CANCEL, Gtk.STOCK_OPEN, Gtk.ResponseType.OK)\r\n    def run_and_get_file(self):\r\n        if self.run() == Gtk.ResponseType.OK:\r\n            return self.get_filename()\r\n        self.destroy()\n'})}),(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"\u4e3b\u7a97\u53e3"}),(0,i.jsx)(n.br,{}),"\n","GUI \u5305\u62ec\uff1a",(0,i.jsx)(n.br,{}),"\n","\u2022\t\u56fe\u50cf\u663e\u793a\u533a\u57df",(0,i.jsx)(n.br,{}),"\n","\u2022\t\u7528\u4e8e\u9009\u62e9 CPU \u6216\u59d4\u6258\u7684\u5355\u9009\u6309\u94ae",(0,i.jsx)(n.br,{}),"\n","\u2022\t\u7528\u4e8e\u9009\u62e9\u548c\u91cd\u65b0\u5904\u7406\u56fe\u50cf\u7684\u6309\u94ae",(0,i.jsx)(n.br,{}),"\n","\u2022\t\u5e26\u6709\u6807\u7b7e\u548c\u8fdb\u5ea6\u6761\u7684\u7ed3\u679c\u663e\u793a"]}),(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'class MainWindow(Gtk.Window):\r\n    def __init__(self):\r\n        super().__init__(title="Image Classification")\r\n        self.set_default_size(800, 600)\r\n        self.imageFilepath = ""\r\n        ...\n'})}),(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"\u56fe\u50cf\u5904\u7406\u4e0e\u663e\u793a"}),(0,i.jsx)(n.br,{}),"\n","\u6b64\u65b9\u6cd5\uff1a",(0,i.jsx)(n.br,{}),"\n","\u2022\t\u8c03\u6574\u5927\u5c0f\u5e76\u663e\u793a\u56fe\u50cf",(0,i.jsx)(n.br,{}),"\n","\u2022\t\u8fd0\u884c\u63a8\u7406",(0,i.jsx)(n.br,{}),"\n","\u2022\t\u663e\u793a\u5e26\u6709\u8fdb\u5ea6\u6761\u548c\u63a8\u7406\u65f6\u95f4\u7684\u7ed3\u679c"]}),(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"def process_file(self, filepath):\r\n    pixbuf = GdkPixbuf.Pixbuf.new_from_file(filepath)\r\n    new_width, new_height = resizeImage(pixbuf)\r\n    scaled_pixbuf = pixbuf.scale_simple(new_width, new_height, GdkPixbuf.InterpType.BILINEAR)\r\n    self.image.set_from_pixbuf(scaled_pixbuf)\r\n\r\n    results, inference_time = runInference(filepath, self.use_delegate())\r\n    ...\n"})}),(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"\u5e94\u7528\u7a0b\u5e8f\u5165\u53e3\u70b9"}),(0,i.jsx)(n.br,{}),"\n","\u521d\u59cb\u5316\u5e76\u542f\u52a8 GTK \u5e94\u7528\u7a0b\u5e8f\u3002"]}),(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'def main():\r\n    app = MainWindow()\r\n    app.connect("destroy", Gtk.main_quit)\r\n    app.show_all()\r\n    Gtk.main()\r\nif __name__ == "__main__":\r\n    success, _ = Gtk.init_check()\r\n    if not success:\r\n        print("GTK could not be initialized.")\r\n        exit(1)\r\n    main()\n'})}),(0,i.jsx)(n.h3,{id:"\u53c2\u8003\u4ee3\u7801",children:"\u53c2\u8003\u4ee3\u7801"}),(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# -----------------------------------------------------------------------------\r\n#\r\n# Copyright (c) Qualcomm Technologies, Inc. and/or its subsidiaries.\r\n# SPDX-License-Identifier: BSD-3-Clause\r\n#\r\n# -----------------------------------------------------------------------------\r\nimport cv2\r\nimport gi\r\nimport numpy as np\r\nimport os\r\nos.environ[\'xDG_RUNTIME_DIR\'] = \'/run/user/1000/\'\r\nos.environ[\'WAYLAND_DISPLAY\'] = \'wayland-1\'\r\nos.environ[\'DISPLAY\'] = \':0\'\r\nimport time\r\nimport urllib.request\r\ngi.require_version("Gtk", "3.0")\r\nfrom gi.repository import Gtk, GLib, GdkPixbuf\r\n\r\n# ========= Constants =========\r\nTF_MODEL = "/home/ubuntu/GoogLeNet_w8a8.tflite"\r\nLABELS = "/etc/labels/imagenet_labels.txt"\r\nDELEGATE_PATH = "libQnnTFLiteDelegate.so"\r\nDEVICE_OS="Ubuntu"\r\nUNAME = os.uname().nodename\r\n\r\nimport ai_edge_litert.interpreter as tflite\r\n\r\nif not os.path.exists(TF_MODEL):\r\n    print("Downloading model...")\r\n    model_url = \'https://huggingface.co/qualcomm/GoogLeNet/resolve/main/GoogLeNet_w8a8.tflite\'\r\n    urllib.request.urlretrieve(model_url, TF_MODEL)\r\n\r\n# ========= Helper Functions =========\r\ndef stable_softmax(logits):\r\n    # Convert logits to float64 for higher precision\r\n    logits = logits.astype(np.float32)\r\n    \r\n    # Subtract the maximum logit to prevent overflow\r\n    shifted_logits = logits - np.max(logits)\r\n    \r\n    # Clip the shifted logits to a safe range to prevent overflow in exp\r\n    shifted_logits = np.clip(shifted_logits, -500, 500)\r\n    \r\n    # Calculate the exponentials and normalize\r\n    exp_scores = np.exp(shifted_logits)\r\n    probabilities = exp_scores / np.sum(exp_scores)\r\n    \r\n    return probabilities\r\n\r\n# Load labels from file\r\ndef load_labels(label_path):\r\n    with open(label_path, \'r\') as f:\r\n        return [line.strip() for line in f.readlines()]\r\n\r\ndef resizeImage(pixbuf):\r\n    original_width = pixbuf.get_width()\r\n    original_height = pixbuf.get_height()\r\n\r\n    # Target display size\r\n    max_width = 800\r\n    max_height = 600\r\n\r\n    # Calculate new size preserving aspect ratio\r\n    scale = min(max_width / original_width, max_height / original_height)\r\n    new_width = int(original_width * scale)\r\n    new_height = int(original_height * scale)\r\n\r\n    return new_width, new_height\r\n\r\n# Load and preprocess input image\r\ndef preprocess_image(image_path, input_shape, input_dtype):\r\n    # Read the image using OpenCV\r\n    img = cv2.imread(image_path)\r\n    if img is None:\r\n        raise ValueError(f"Failed to load image at {image_path}")\r\n    # Convert BGR to RGB\r\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\r\n    # Resize the image to the desired input shape\r\n    img = cv2.resize(img, (input_shape[2], input_shape[1]))\r\n    # Convert to the desired data type\r\n    img = img.astype(input_dtype)\r\n    # Add batch dimension\r\n    img = np.expand_dims(img, axis=0)\r\n    \r\n    return img\r\n\r\n# ====== Inference Function ======\r\ndef runInference(image, use_delegate):\r\n    results = []    \r\n    print(f"Running on {DEVICE_OS} using Delegate:{use_delegate}")\r\n    if use_delegate:\r\n        try:\r\n            # Load the QNN delegate library\r\n            delegate_options = { \'backend_type\': \'htp\' }\r\n            delegate = tflite.load_delegate(DELEGATE_PATH, delegate_options)\r\n            \r\n            # Load the TFLite model\r\n            model = tflite.Interpreter(model_path=TF_MODEL, experimental_delegates=[delegate])\r\n            print("INFO: Loaded QNN delegate with HTP backend")\r\n        except Exception as e:\r\n            print(f"WARNING: Failed to load QNN delegate: {e}")\r\n            print("INFO: Continuing without QNN delegate")\r\n            model = tflite.Interpreter(model_path=TF_MODEL)   \r\n    else:\r\n        model = tflite.Interpreter(model_path=TF_MODEL)  \r\n   \r\n    model.allocate_tensors()\r\n\r\n    # Get and Prepare input \r\n    input_details = model.get_input_details()\r\n    input_shape = input_details[0][\'shape\']\r\n    input_dtype = input_details[0][\'dtype\']\r\n    input_data = preprocess_image(image, input_shape, input_dtype)\r\n    \r\n    # Load input data to input tensor\r\n    model.set_tensor(input_details[0][\'index\'], input_data)\r\n    model.get_signature_list()\r\n    \r\n    # Run inference\r\n    try:\r\n        start_time = time.time()\r\n        model.invoke()\r\n        end_time = time.time()\r\n        print("Interpreter invoked successfully.")\r\n    except Exception as e:\r\n        print(f"Error during model invocation: {e}")\r\n        return []\r\n\r\n    # Calculate and print duration\r\n    inference_time = end_time - start_time\r\n\r\n    # Prepare output tensor details\r\n    output_details = model.get_output_details()\r\n\r\n    # Load output data to output tensor\r\n    output_data = model.get_tensor(output_details[0][\'index\'])\r\n\r\n    # Load labels and get prediction\r\n    labels = load_labels(LABELS)\r\n    predicted_index = np.argmax(output_data)\r\n    predicted_label = labels[predicted_index]\r\n    print("Predicted index:", predicted_index)\r\n    print("Predicted label:", predicted_label)\r\n    \r\n    # Add Softmax function\r\n    logits = output_data[0]\r\n    probabilities = stable_softmax(logits)\r\n\r\n    # Get top 4 predictions\r\n    top_k = 4\r\n    top_indices = np.argsort(probabilities)[::-1][:top_k]\r\n    for i in top_indices:\r\n        result = (labels[i], probabilities[i] * 100)\r\n        results.append(result)\r\n\r\n    return results, inference_time\r\n\r\n# ====== GTK GUI Classes ======\r\nclass FileBrowser(Gtk.FileChooserDialog):\r\n    def __init__(self):\r\n        super().__init__(title="Choose an image", action=Gtk.FileChooserAction.OPEN)\r\n        self.add_buttons(Gtk.STOCK_CANCEL, Gtk.ResponseType.CANCEL, Gtk.STOCK_OPEN, Gtk.ResponseType.OK)\r\n\r\n    def run_and_get_file(self):\r\n        response = super().run()\r\n        if response == Gtk.ResponseType.OK:\r\n            print("Selected file:", self.get_filename())\r\n            self.selected_file = self.get_filename()            \r\n        self.destroy()\r\n        return self.selected_file\r\n\r\nclass MainWindow(Gtk.Window):\r\n    def __init__(self):\r\n        super().__init__(title="Image Classification")\r\n        self.set_default_size(800, 600)\r\n        self.imageFilepath = ""\r\n        # Main layout\r\n        self.mainBox = Gtk.Box(orientation=Gtk.Orientation.HORIZONTAL, spacing=10)\r\n        self.mainBox.set_margin_top(10)\r\n        self.mainBox.set_margin_bottom(10)\r\n        self.mainBox.set_margin_start(10)\r\n        self.mainBox.set_margin_end(10)\r\n        self.add(self.mainBox)\r\n        \r\n        # Main Window Image setup with fallback\r\n        self.image = Gtk.Image()\r\n        try:\r\n            MAIN_IMAGE = "MainWindowPic.jpg"\r\n            self.image.set_from_file(MAIN_IMAGE)         \r\n        except Exception as e:\r\n            print("Error loading main image:", e)\r\n            self.image.set_from_icon_name("image-missing", Gtk.IconSize.DIALOG)\r\n\r\n        self.mainBox.pack_start(self.image, True, True, 0)\r\n\r\n        # Set up a new box to add results and and file button\r\n        self.infoBox = Gtk.Box(orientation=Gtk.Orientation.VERTICAL, spacing=10)\r\n        \r\n        # Radio button to select Delegate\r\n        delegate_label = Gtk.Label(label="Select Inference Mode:")\r\n        self.infoBox.pack_start(delegate_label, False, False, 10)\r\n\r\n        self.cpu_radio = Gtk.RadioButton.new_with_label_from_widget(None, "CPU")\r\n        self.delegate_radio = Gtk.RadioButton.new_with_label_from_widget(self.cpu_radio, "Delegate")\r\n\r\n        self.infoBox.pack_start(self.cpu_radio, False, False, 0)\r\n        self.infoBox.pack_start(self.delegate_radio, False, False, 0)\r\n        \r\n        # Radio button signal\r\n        self.cpu_radio.connect("toggled", self.on_radio_toggled)\r\n        self.delegate_radio.connect("toggled", self.on_radio_toggled)\r\n\r\n        # Open file button\r\n        open_button = Gtk.Button(label="Select Image")\r\n        open_button.connect("clicked", self.on_open_file_clicked)\r\n        self.infoBox.pack_start(open_button, False, True, 10)\r\n\r\n        # Reprocess Image\r\n        reprocess_button = Gtk.Button(label="Reprocess Image")\r\n        reprocess_button.connect("clicked", self.on_reprocess_image_clicked)\r\n        self.infoBox.pack_start(reprocess_button, False, True, 10)\r\n\r\n        # Classification results\r\n        self.results = Gtk.Box(orientation=Gtk.Orientation.VERTICAL, spacing=10)\r\n        self.infoBox.pack_start(self.results, True, True, 0)\r\n        self.mainBox.pack_start(self.infoBox, True, True, 0)\r\n\r\n    def use_delegate(self):\r\n        return self.delegate_radio.get_active()\r\n\r\n    def on_radio_toggled(self, button):\r\n        if button.get_active():\r\n            print(f"Selected option: {button.get_label()}")\r\n\r\n    def process_file(self, filepath): \r\n        try:\r\n            # Resize Image\r\n            pixbuf = GdkPixbuf.Pixbuf.new_from_file(filepath)\r\n            new_width, new_height = resizeImage(pixbuf)\r\n            scaled_pixbuf = pixbuf.scale_simple(new_width, new_height, GdkPixbuf.InterpType.BILINEAR)\r\n            \r\n            # Replace the image with new image\r\n            self.image.set_from_pixbuf(scaled_pixbuf)\r\n           \r\n            # Run Inference\r\n            use_delegate = self.use_delegate()\r\n            print("delegate: " , use_delegate)\r\n            options, inference_time = runInference(filepath, use_delegate)\r\n\r\n            # Clear result box\r\n            for child in self.results.get_children():\r\n                self.results.remove(child)\r\n            \r\n            # Set up predictions\r\n            for label, percent in options:\r\n                textBox = Gtk.Box(orientation=Gtk.Orientation.HORIZONTAL, spacing=10)\r\n                barBox = Gtk.Box(orientation=Gtk.Orientation.HORIZONTAL, spacing=10)\r\n                text = Gtk.Label(label=label, xalign=0)\r\n                text.set_size_request(100, -1) \r\n                \r\n                bar = Gtk.ProgressBar()\r\n                bar.set_fraction(percent / 100.0)\r\n                bar.set_text(f"{percent:.2f}%")\r\n                bar.set_show_text(True)\r\n                \r\n                textBox.pack_start(text, False, False, 0)\r\n                barBox.pack_start(bar, True, True, 0)\r\n            \r\n                self.results.pack_start(textBox, False, False, 0)\r\n                self.results.pack_start(barBox, False, False, 0)\r\n                self.results.show_all()\r\n            \r\n            # Add inference time label\r\n            time_label = Gtk.Label(label=f"Inference Time : {inference_time * 1000:.2f} ms")\r\n            self.results.pack_start(time_label, False, False, 50)\r\n            self.results.show_all()\r\n        except Exception as e:\r\n            print("Error reading file:", e)\r\n\r\n    def on_open_file_clicked(self, widget):\r\n        dialog = FileBrowser()\r\n        selected_file = dialog.run_and_get_file()\r\n        self.imageFilepath = selected_file\r\n        if selected_file:\r\n            self.process_file(selected_file)\r\n \r\n    def on_reprocess_image_clicked(self, widget):\r\n        self.process_file(self.imageFilepath)\r\n\r\n    def on_destroy(self, widget):\r\n        Gtk.main_quit()\r\n\r\n# === Main Entry Point ===\r\ndef main():\r\n    app = MainWindow()\r\n    app.connect("destroy", Gtk.main_quit)\r\n    app.show_all()\r\n    Gtk.main()\r\n\r\nif __name__ == "__main__":\r\n    success, _ = Gtk.init_check()\r\n    if not success:\r\n        print("GTK could not be initialized. Check environmental variables")\r\n        exit(1)\r\n\r\n    main() \r\n\n'})}),(0,i.jsx)(n.p,{children:"\u73b0\u5728\uff0c\u60a8\u53ef\u4ee5\u4f7f\u7528\u4ee5\u4e0b\u547d\u4ee4\u5728 CPU/\u59d4\u6258\u4e0a\u8fd0\u884c\u8be5\u5e94\u7528\u7a0b\u5e8f\uff1a"}),(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-shell",children:"python3 classification.py\n"})}),(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{src:r(2491).A+"",width:"3397",height:"2736"})}),(0,i.jsx)(n.p,{children:"\u4ece\u4e92\u8054\u7f51\u4e0a\u4e0b\u8f7d\u4efb\u4e00\u56fe\u7247\u3002\u8be5\u793a\u4f8b\u4f7f\u7528\u4e86\u6d88\u9632\u8f66\u56fe\u7247\u3002\u901a\u8fc7 scp \u547d\u4ee4\u5c06\u56fe\u50cf\u590d\u5236\u5230\u8bbe\u5907\u4e0a\u3002"}),(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-shell",children:"scp xxx.jpg ubuntu@IP_address:/home/ubuntu/\n"})}),(0,i.jsx)(n.p,{children:"\u5728 GUI \u4e0a\u9009\u62e9 CPU \u4f5c\u4e3a\u8fd0\u884c\u65f6\u9009\u9879\uff1a"}),(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{src:r(7144).A+"",width:"3828",height:"2624"})}),(0,i.jsx)(n.p,{children:"\u5728 GUI \u4e0a\u9009\u62e9 Delegate \u4f5c\u4e3a\u8fd0\u884c\u65f6\u9009\u9879\uff1a"}),(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{src:r(2531).A+"",width:"3555",height:"2453"})})]}),(0,i.jsxs)(l.A,{value:"Example3",label:"Object Detection",children:[(0,i.jsx)(n.h3,{id:"object-detection-with-opencv--wayland-display",children:"Object Detection with OpenCV & Wayland Display"}),(0,i.jsxs)(n.p,{children:["\u8fd9\u4e2a Python \u811a\u672c\u4f7f\u7528\u91cf\u5316\u7684 ",(0,i.jsx)(n.strong,{children:"YOLOv8 TensorFlow Lite"})," \u6a21\u578b\u5bf9\u89c6\u9891\u6587\u4ef6\u8fdb\u884c",(0,i.jsx)(n.strong,{children:"\u5b9e\u65f6\u76ee\u6807\u68c0\u6d4b"}),"\uff0c\u5e76\u901a\u8fc7",(0,i.jsx)(n.strong,{children:"GStreamer \u5728 Wayland \u663e\u793a\u5668\u4e0a"}),"\u663e\u793a\u5e26\u6ce8\u91ca\u7684\u5e27\u3002\u5b83\u9488\u5bf9",(0,i.jsx)(n.strong,{children:"\u8fb9\u7f18 AI \u573a\u666f"}),"\u8fdb\u884c\u4e86\u4f18\u5316\uff0c\u901a\u8fc7",(0,i.jsx)(n.strong,{children:"QNN TFLite \u59d4\u6258"}),"\u5b9e\u73b0\u786c\u4ef6\u52a0\u901f\u3002"]}),(0,i.jsx)(n.admonition,{type:"note",children:(0,i.jsxs)(n.p,{children:["YOLOv8 \u6a21\u578b\u9ed8\u8ba4\u4e0d\u53ef\u7528\u3002\u8bf7\u6309\u7167 ",(0,i.jsx)(n.a,{href:"https://docs.qualcomm.com/bundle/publicresource/topics/80-70020-50/download-model-and-label-files.html?vproduct=1601111740013072&version=1.5&facet=Intelligent_Multimedia_SDK.SDK.2.0",children:(0,i.jsx)(n.strong,{children:"Qualcomm Intelligent Multimedia SDK"})})," ",(0,i.jsx)(n.strong,{children:"\u6b65\u9aa4-6"})," \u5bfc\u51fa YOLOv8 \u91cf\u5316\u6a21\u578b\u3002"]})}),(0,i.jsx)(n.p,{children:"\u4e0b\u4e00\u6b65\u662f\u5c06\u6a21\u578b\u63a8\u9001\u5230\u76ee\u6807\u8bbe\u5907\u4e0a\uff0c\u901a\u8fc7 scp \u547d\u4ee4\u5c06\u6a21\u578b\u590d\u5236\u5230\u8bbe\u5907\u4e0a\u3002"}),(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-shell",children:"scp xxxx.tflite ubuntu@IP_address:/home/ubuntu/\n"})}),(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"\u521d\u59cb\u5316\u548c\u914d\u7f6e"}),(0,i.jsx)(n.br,{}),"\n","\u2022\t\u5b9a\u4e49\u6a21\u578b\u3001\u6807\u7b7e\u3001\u8f93\u5165\u89c6\u9891\u548c\u59d4\u6258\u7684\u8def\u5f84\u3002",(0,i.jsx)(n.br,{}),"\n","\u2022\t\u4e3a\u9884\u5904\u7406\u548c\u540e\u5904\u7406\u8bbe\u7f6e\u5e27\u5c3a\u5bf8\u3001FPS\u3001\u7f6e\u4fe1\u5ea6\u9608\u503c\u548c\u7f29\u653e\u56e0\u5b50\u7b49\u5e38\u91cf\u3002"]}),(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"MODEL_PATH"}),' = "yolov8_det_quantized.tflite"',(0,i.jsx)(n.br,{}),"\n",(0,i.jsx)(n.strong,{children:"LABEL_PATH"}),' = "coco_labels.txt"',(0,i.jsx)(n.br,{}),"\n",(0,i.jsx)(n.strong,{children:"VIDEO_IN"}),' = "video.mp4"',(0,i.jsx)(n.br,{}),"\n",(0,i.jsx)(n.strong,{children:"DELEGATE_PATH"}),' = "libQnnTFLiteDelegate.so"']}),(0,i.jsxs)(n.admonition,{type:"note",children:[(0,i.jsx)(n.mdxAdmonitionTitle,{}),(0,i.jsxs)(n.p,{children:["\u4f7f\u7528\u4e0e\u5bf9\u8c61\u68c0\u6d4b\u6a21\u578b\u914d\u5408\u826f\u597d\u7684\u89c6\u9891\u6587\u4ef6\u3002",(0,i.jsx)(n.br,{}),"\n","\u4e3a\u4e86\u83b7\u5f97\u6700\u4f73\u6548\u679c\uff0c\u8bf7\u9009\u62e9\u4e3b\u9898\u6e05\u6670\u3001\u5149\u7ebf\u5145\u8db3\u4e14\u8fd0\u52a8\u6a21\u7cca\u6700\u5c11\u7684\u89c6\u9891\u3002"]}),(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"\u4f8b\u5982\uff1a"}),(0,i.jsx)(n.br,{}),"\n","\u2022 \u6709\u8f66\u8f86\u548c\u884c\u4eba\u7684\u8857\u9053\u573a\u666f",(0,i.jsx)(n.br,{}),"\n","\u2022 \u6709\u53ef\u89c1\u7269\u4f53\u7684\u4ed3\u5e93\u6216\u5de5\u5382\u8f66\u95f4",(0,i.jsx)(n.br,{}),"\n","\u2022 \u4eba\u7269\u6216\u4ea7\u54c1\u7684\u9759\u6001\u6444\u50cf\u673a\u753b\u9762"]})]}),(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"\u6a21\u578b\u52a0\u8f7d\u548c\u59d4\u6258\u8bbe\u7f6e"}),(0,i.jsx)(n.br,{}),"\n","\u2022\t\u52a0\u8f7d\u786c\u4ef6\u59d4\u6258\u4ee5\u52a0\u901f\u63a8\u7406\u3002",(0,i.jsx)(n.br,{}),"\n","\u2022\t\u4f7f\u7528\u91cf\u5316\u7684 YOLOv8 \u6a21\u578b\u521d\u59cb\u5316 TensorFlow Lite \u89e3\u91ca\u5668\u3002"]}),(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"delegate_options = { 'backend_type': 'htp' }\r\ndelegate = tflite.load_delegate(DELEGATE_PATH, delegate_options)\r\ninterpreter = tflite.Interpreter(model_path=MODEL_PATH, experimental_delegates=[delegate])\r\ninterpreter.allocate_tensors()\n"})}),(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"\u6807\u7b7e\u52a0\u8f7d"}),(0,i.jsx)(n.br,{}),"\n","\u2022\t\u52a0\u8f7d COCO \u6570\u636e\u96c6\u6807\u7b7e\u4ee5\u8fdb\u884c\u5bf9\u8c61\u6ce8\u91ca\u3002"]}),(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"labels = [l.strip() for l in open(LABEL_PATH)]\n"})}),(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"GStreamer \u7ba1\u9053\u8bbe\u7f6e"}),(0,i.jsx)(n.br,{}),"\n","\u2022\t\u4f7f\u7528 appsrc \u521b\u5efa GStreamer \u7ba1\u9053\uff0c\u5c06\u5e27\u6d41\u5f0f\u4f20\u8f93\u5230 Wayland \u63a5\u6536\u5668\u3002\u2022\t\u80fd\u591f\u5b9e\u65f6\u663e\u793a\u5df2\u5904\u7406\u7684\u5e27\u3002"]}),(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-shell",children:"pipeline = Gst.parse_launch(\r\n    'appsrc name=src is-live=true block=true format=time caps=video/x-raw,format=BGR,width=1600,height=900,framerate=30/1 ! videoconvert ! waylandsink')\n"})}),(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"\u89c6\u9891\u6355\u83b7\u548c\u5e27\u5904\u7406"}),(0,i.jsx)(n.br,{}),"\n","\u2022\t\u4f7f\u7528 OpenCV \u6253\u5f00\u89c6\u9891\u6587\u4ef6\u3002",(0,i.jsx)(n.br,{}),"\n","\u2022\t\u6bcf\u4e2a\u5e27\u90fd\u7ecf\u8fc7\u8c03\u6574\u5927\u5c0f\u548c\u9884\u5904\u7406\uff0c\u4ee5\u5339\u914d\u6a21\u578b\u7684\u8f93\u5165\u5c3a\u5bf8\u3002"]}),(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"cap = cv2.VideoCapture(VIDEO_IN)\n"})}),(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"\u63a8\u7406\u548c\u540e\u5904\u7406"}),(0,i.jsx)(n.br,{}),"\n","\u2022\t\u5bf9\u6bcf\u4e00\u5e27\u8fdb\u884c\u63a8\u7406\u3002",(0,i.jsx)(n.br,{}),"\n","\u2022\t\u4f7f\u7528\u9884\u5b9a\u4e49\u7684\u7f29\u653e\u56e0\u5b50\u548c\u96f6\u70b9\u5bf9\u6a21\u578b\u8f93\u51fa\u8fdb\u884c\u53bb\u91cf\u5316\u3002"]}),(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"interpreter.set_tensor(in_det[0]['index'], input_tensor)\r\ninterpreter.invoke()\r\nboxes_q = interpreter.get_tensor(out_det[0]['index'])[0]\r\nscores_q = interpreter.get_tensor(out_det[1]['index'])[0]\r\nclasses_q = interpreter.get_tensor(out_det[2]['index'])[0]\n"})}),(0,i.jsx)(n.p,{children:"\u2022\t\u5e94\u7528\u7f6e\u4fe1\u5ea6\u9608\u503c\u6765\u8fc7\u6ee4\u4f4e\u6982\u7387\u68c0\u6d4b\u3002\u2022\t\u4f7f\u7528\u975e\u6781\u5927\u6291\u5236\uff08NMS\uff09\u6765\u6d88\u9664\u91cd\u53e0\u7684\u6846\u3002"}),(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"mask = scores >= CONF_THRES\r\nboxes_f = boxes[mask]\r\nscores_f = scores[mask]\r\nclasses_f = classes[mask]\n"})}),(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"\u6ce8\u91ca\u4e0e\u663e\u793a"})}),(0,i.jsx)(n.p,{children:"\u2022\t\u4f7f\u7528 OpenCV \u5728\u753b\u9762\u4e0a\u7ed8\u5236\u8fb9\u754c\u6846\u548c\u6807\u7b7e\u3002\u2022\t\u6bcf 100 \u5e27\u8bb0\u5f55\u4e00\u6b21\u6700\u9ad8\u68c0\u6d4b\u5206\u6570\u3002"}),(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'cv2.rectangle(frame_rs, (x1i, y1i), (x2i, y2i), (0,255,0), 2)\r\ncv2.putText(frame_rs, f"{lab} {sc:.2f}", (x1i, max(10,y1i-5)), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,255,0), 2)\n'})}),(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"\u6d41\u5f0f\u4f20\u8f93\u81f3 Wayland \u663e\u793a\u5668"})}),(0,i.jsx)(n.p,{children:"\u5c06\u5e27\u8f6c\u6362\u4e3a GStreamer \u7f13\u51b2\u533a\uff0c\u5e76\u5c06\u5b83\u4eec\u5e26\u7740\u65f6\u95f4\u6233\u63a8\u9001\u5230\u7ba1\u9053\u4ee5\u5b9e\u73b0\u6d41\u7545\u64ad\u653e\u3002"}),(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"buf = Gst.Buffer.new_allocate(None, len(data), None)\r\nbuf.fill(0, data)\r\nbuf.duration = Gst.util_uint64_scale_int(1, Gst.SECOND, FPS_OUT)\r\ntimestamp = cap.get(cv2.CAP_PROP_POS_MSEC) * Gst.MSECOND\r\nbuf.pts = buf.dts = int(timestamp)\r\nappsrc.emit('push-buffer', buf)\n"})}),(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"\u5b8c\u6210"}),(0,i.jsx)(n.br,{}),"\n","\u5904\u7406\u5b8c\u6240\u6709\u5e27\u540e\u6b63\u5e38\u5173\u95ed\u7ba1\u9053\u3002"]}),(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"appsrc.emit('end-of-stream')\r\npipeline.set_state(Gst.State.NULL)\r\ncap.release()\n"})}),(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"\u7528\u4f8b"}),(0,i.jsx)(n.br,{}),"\n","\u6b64\u811a\u672c\u9002\u7528\u4e8e\uff1a",(0,i.jsx)(n.br,{}),"\n","\u2022\t\u667a\u80fd\u76f8\u673a",(0,i.jsx)(n.br,{}),"\n","\u2022\t\u673a\u5668\u4eba\u6280\u672f",(0,i.jsx)(n.br,{}),"\n","\u2022\t\u57fa\u4e8e Wayland GUI \u7684\u5d4c\u5165\u5f0f\u7cfb\u7edf",(0,i.jsx)(n.br,{}),"\n","\u2022\t\u8fb9\u7f18 AI \u90e8\u7f72\u4e2d\u7684\u5b9e\u65f6\u76d1\u63a7"]}),(0,i.jsx)(n.h3,{id:"\u53c2\u8003\u4ee3\u7801-1",children:"\u53c2\u8003\u4ee3\u7801"}),(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# -----------------------------------------------------------------------------\r\n#\r\n# Copyright (c) Qualcomm Technologies, Inc. and/or its subsidiaries.\r\n# SPDX-License-Identifier: BSD-3-Clause\r\n#\r\n# -----------------------------------------------------------------------------\r\n\r\n#!/usr/bin/env python3\r\n\r\n# Import necessary libraries\r\nimport cv2\r\nimport numpy as np\r\nimport gi\r\ngi.require_version('Gst', '1.0')\r\nfrom gi.repository import Gst\r\nimport ai_edge_litert.interpreter as tflite\r\n\r\n# Initialize GStreamer\r\nGst.init(None)\r\n\r\n# -------------------- Parameters --------------------\r\nMODEL_PATH = \"/home/ubuntu/yolov8_det_quantized.tflite\"  # Path to TFLite model\r\nLABEL_PATH = \"/etc/labels/coco_labels.txt\"              # Path to label file            # Path to label file\r\nVIDEO_IN = \"/etc/media/video.mp4\"                        # Input video file\r\nDELEGATE_PATH = \"libQnnTFLiteDelegate.so\"                # Delegate for hardware acceleration\r\n\r\n# Frame and model parameters\r\nFRAME_W, FRAME_H = 1600, 900\r\nFPS_OUT = 30\r\nCONF_THRES = 0.25\r\nNMS_IOU_THRES = 0.50\r\nBOX_SCALE = 3.2108588218688965\r\nBOX_ZP = 31.0\r\nSCORE_SCALE = 0.0038042240776121616\r\n# -------------------- Load Model --------------------\r\n# Load delegate for hardware acceleration\r\ndelegate_options = { 'backend_type': 'htp' }\r\ndelegate = tflite.load_delegate(DELEGATE_PATH, delegate_options)\r\n\r\n# Load and allocate TFLite interpreter\r\ninterpreter = tflite.Interpreter(model_path=MODEL_PATH, experimental_delegates=[delegate])\r\ninterpreter.allocate_tensors()\r\n\r\n# Get input/output tensor details\r\nin_det = interpreter.get_input_details()\r\nout_det = interpreter.get_output_details()\r\nin_h, in_w = in_det[0][\"shape\"][1:3]\r\n\r\n# -------------------- Load Labels --------------------\r\nlabels = [l.strip() for l in open(LABEL_PATH)]\r\n\r\n# -------------------- GStreamer Pipeline --------------------\r\n# Create GStreamer pipeline to display video via Wayland\r\npipeline = Gst.parse_launch(\r\n    'appsrc name=src is-live=true block=true format=time caps=video/x-raw,format=BGR,width=1600,height=900,framerate=30/1 ! videoconvert ! waylandsink'\r\n)\r\nappsrc = pipeline.get_by_name('src')\r\npipeline.set_state(Gst.State.PLAYING)\r\n\r\n# -------------------- Video Input --------------------\r\ncap = cv2.VideoCapture(VIDEO_IN)\r\n\r\n# Scaling factors for bounding box adjustment\r\nsx, sy = FRAME_W / in_w, FRAME_H / in_h\r\n\r\n# Preallocate frame buffers\r\nframe_rs = np.empty((FRAME_H, FRAME_W, 3), np.uint8)\r\ninput_tensor = np.empty((1, in_h, in_w, 3), np.uint8)\r\n\r\nframe_cnt = 0\r\n\r\n# -------------------- Main Loop --------------------\r\nwhile True:\r\n    ok, frame = cap.read()\r\n    if not ok:\r\n        break\r\n    frame_cnt += 1\r\n\r\n    # ---------- Preprocessing ----------\r\n    # Resize frame to display resolution\r\n    cv2.resize(frame, (FRAME_W, FRAME_H), dst=frame_rs)\r\n\r\n    # Resize again to model input resolution\r\n    cv2.resize(frame_rs, (in_w, in_h), dst=input_tensor[0])\r\n\r\n    # ---------- Inference ----------\r\n    # Set input tensor and run inference\r\n    interpreter.set_tensor(in_det[0]['index'], input_tensor)\r\n    interpreter.invoke()\r\n\r\n    # ---------- Postprocessing ----------\r\n    # Get raw output tensors\r\n    boxes_q = interpreter.get_tensor(out_det[0]['index'])[0]\r\n    scores_q = interpreter.get_tensor(out_det[1]['index'])[0]\r\n    classes_q = interpreter.get_tensor(out_det[2]['index'])[0]\r\n\r\n    # Dequantize outputs\r\n    boxes = BOX_SCALE * (boxes_q.astype(np.float32) - BOX_ZP)\r\n    scores = SCORE_SCALE * scores_q.astype(np.float32)\r\n    classes = classes_q.astype(np.int32)\r\n\r\n    # Filter by confidence threshold\r\n    mask = scores >= CONF_THRES\r\n    if np.any(mask):\r\n        boxes_f = boxes[mask]\r\n        scores_f = scores[mask]\r\n        classes_f = classes[mask]\r\n\r\n        # Convert boxes to OpenCV format\r\n        x1, y1, x2, y2 = boxes_f.T\r\n        boxes_cv2 = np.column_stack((x1, y1, x2 - x1, y2 - y1))\r\n\r\n        # Apply Non-Maximum Suppression\r\n        idx_cv2 = cv2.dnn.NMSBoxes(\r\n            bboxes=boxes_cv2.tolist(),\r\n            scores=scores_f.tolist(),\r\n            score_threshold=CONF_THRES,\r\n            nms_threshold=NMS_IOU_THRES\r\n        )\r\n\r\n        if len(idx_cv2):\r\n            idx = idx_cv2.flatten()\r\n            sel_boxes = boxes_f[idx]\r\n            sel_scores = scores_f[idx]\r\n            sel_classes = classes_f[idx]\r\n\r\n            # Debug print every 100 frames\r\n            if frame_cnt % 100 == 0:\r\n                print(f\"[{frame_cnt:4d}] max score = {sel_scores.max():.3f}\")\r\n\r\n            # Rescale boxes to display resolution\r\n            sel_boxes[:, [0,2]] *= sx\r\n            sel_boxes[:, [1,3]] *= sy\r\n            sel_boxes = sel_boxes.astype(np.int32)\r\n\r\n            # Clip boxes to frame boundaries\r\n            sel_boxes[:, [0,2]] = np.clip(sel_boxes[:, [0,2]], 0, FRAME_W-1)\r\n            sel_boxes[:, [1,3]] = np.clip(sel_boxes[:, [1,3]], 0, FRAME_H-1)\r\n\r\n            # Draw boxes and labels\r\n            for (x1i, y1i, x2i, y2i), sc, cl in zip(sel_boxes, sel_scores, sel_classes):\r\n                cv2.rectangle(frame_rs, (x1i, y1i), (x2i, y2i), (0,255,0), 2)\r\n                lab = labels[cl] if cl < len(labels) else str(cl)\r\n                cv2.putText(frame_rs, f\"{lab} {sc:.2f}\", (x1i, max(10,y1i-5)),\r\n                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,255,0), 2)\r\n\r\n    # ---------- Video Output ----------\r\n    # Convert frame to bytes and push to GStreamer pipeline\r\n    data = frame_rs.tobytes()\r\n    buf = Gst.Buffer.new_allocate(None, len(data), None)\r\n    buf.fill(0, data)\r\n    buf.duration = Gst.util_uint64_scale_int(1, Gst.SECOND, FPS_OUT)\r\n    timestamp = cap.get(cv2.CAP_PROP_POS_MSEC) * Gst.MSECOND\r\n    buf.pts = buf.dts = int(timestamp)\r\n    appsrc.emit('push-buffer', buf)\r\n\r\n# -------------------- Finish --------------------\r\nappsrc.emit('end-of-stream')\r\npipeline.set_state(Gst.State.NULL)\r\ncap.release()\r\nprint(\"Done \u2013 video streamed to Wayland sink\")\n"})}),(0,i.jsx)(n.p,{children:"\u73b0\u5728\uff0c\u60a8\u53ef\u4ee5\u4f7f\u7528\u4ee5\u4e0b\u547d\u4ee4\u5728 NPU\uff08\u59d4\u6258\uff09\u4e0a\u8fd0\u884c\u8be5\u5e94\u7528\u7a0b\u5e8f\uff1a"}),(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"python3 ObjectDetection.py\n"})}),(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{src:r(3244).A+"",width:"975",height:"548"})})]})]})]})}function _(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(u,{...e})}):u(e)}},7144:(e,n,r)=>{r.d(n,{A:()=>t});const t=r.p+"assets/images/IC-LiteRT-CPU-1b96c4321d92f2c1099cd85cb2a6ee8d.jpeg"},8453:(e,n,r)=>{r.d(n,{R:()=>a,x:()=>l});var t=r(6540);const i={},s=t.createContext(i);function a(e){const n=t.useContext(s);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:a(e.components),t.createElement(s.Provider,{value:n},e.children)}},9365:(e,n,r)=>{r.d(n,{A:()=>a});r(6540);var t=r(4164);const i={tabItem:"tabItem_Ymn6"};var s=r(4848);function a({children:e,hidden:n,className:r}){return(0,s.jsx)("div",{role:"tabpanel",className:(0,t.A)(i.tabItem,r),hidden:n,children:e})}}}]);