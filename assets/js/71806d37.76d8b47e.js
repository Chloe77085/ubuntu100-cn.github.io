"use strict";(globalThis.webpackChunkmy_website=globalThis.webpackChunkmy_website||[]).push([[7523],{2994:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>o,contentTitle:()=>c,default:()=>d,frontMatter:()=>t,metadata:()=>l,toc:()=>i});const l=JSON.parse('{"id":"Application Development and Execution Guide/Framework-Driven AI Sample Execution/llama_cpp","title":"Llama.cpp","description":"\u60a8\u53ef\u4ee5\u4f7f\u7528 llama.cpp \u5728 Dragonwing \u5f00\u53d1\u677f\u4e0a\u8fd0\u884c\u5404\u79cd\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u3002\u5728 llama.cpp \u4e0b\u8fd0\u884c\u7684\u6a21\u578b\u5728GPU\u800c\u975eNPU\u4e0a\u8fd0\u884c\u3002\u60a8\u53ef\u4ee5\u901a\u8fc7 GENIE \u5728NPU\u4e0a\u8fd0\u884c\u90e8\u5206\u6a21\u578b\u5b50\u96c6\u3002","source":"@site/docs/7.Application Development and Execution Guide/2.Framework-Driven AI Sample Execution/5.llama_cpp.md","sourceDirName":"7.Application Development and Execution Guide/2.Framework-Driven AI Sample Execution","slug":"/Application Development and Execution Guide/Framework-Driven AI Sample Execution/llama_cpp","permalink":"/ubuntu100-cn.github.io/docs/Application Development and Execution Guide/Framework-Driven AI Sample Execution/llama_cpp","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/7.Application Development and Execution Guide/2.Framework-Driven AI Sample Execution/5.llama_cpp.md","tags":[],"version":"current","sidebarPosition":5,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"ONNX","permalink":"/ubuntu100-cn.github.io/docs/Application Development and Execution Guide/Framework-Driven AI Sample Execution/onnx"},"next":{"title":"Genie","permalink":"/ubuntu100-cn.github.io/docs/Application Development and Execution Guide/Framework-Driven AI Sample Execution/genie"}}');var a=r(4848),s=r(8453);const t={},c="Llama.cpp",o={},i=[{value:"\u6784\u5efa llama.cpp",id:"\u6784\u5efa-llamacpp",level:2},{value:"\u4e0b\u8f7d\u5e76\u91cf\u5316\u6a21\u578b",id:"\u4e0b\u8f7d\u5e76\u91cf\u5316\u6a21\u578b",level:3},{value:"\u4f7f\u7528 llama-cli \u8fd0\u884c\u4f60\u7684\u7b2c\u4e00\u4e2a LLM",id:"\u4f7f\u7528-llama-cli-\u8fd0\u884c\u4f60\u7684\u7b2c\u4e00\u4e2a-llm",level:3},{value:"\u4f7f\u7528 llama-server \u90e8\u7f72 LLM",id:"\u4f7f\u7528-llama-server-\u90e8\u7f72-llm",level:3},{value:"\u90e8\u7f72\u591a\u6a21\u6001LLM",id:"\u90e8\u7f72\u591a\u6a21\u6001llm",level:3},{value:"Tips &amp; tricks",id:"tips--tricks",level:2},{value:"\u6bd4\u8f83 CPU \u6027\u80fd",id:"\u6bd4\u8f83-cpu-\u6027\u80fd",level:3}];function p(e){const n={a:"a",admonition:"admonition",br:"br",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",img:"img",mdxAdmonitionTitle:"mdxAdmonitionTitle",p:"p",pre:"pre",strong:"strong",...(0,s.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"llamacpp",children:"Llama.cpp"})}),"\n",(0,a.jsxs)(n.p,{children:["\u60a8\u53ef\u4ee5\u4f7f\u7528 ",(0,a.jsx)(n.a,{href:"https://github.com/ggml-org/llama.cpp",children:"llama.cpp"})," \u5728 Dragonwing \u5f00\u53d1\u677f\u4e0a\u8fd0\u884c\u5404\u79cd\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u3002\u5728 llama.cpp \u4e0b\u8fd0\u884c\u7684\u6a21\u578b\u5728",(0,a.jsx)(n.em,{children:"GPU"}),"\u800c\u975e",(0,a.jsx)(n.em,{children:"NPU\u4e0a\u8fd0\u884c"}),"\u3002\u60a8\u53ef\u4ee5\u901a\u8fc7 ",(0,a.jsx)(n.a,{href:"https://qc-ai-test.gitbook.io/qc-ai-test-docs/running-building-ai-models/genie",children:"GENIE"})," \u5728NPU\u4e0a\u8fd0\u884c\u90e8\u5206\u6a21\u578b\u5b50\u96c6\u3002"]}),"\n",(0,a.jsx)(n.h2,{id:"\u6784\u5efa-llamacpp",children:"\u6784\u5efa llama.cpp"}),"\n",(0,a.jsx)(n.p,{children:"\u60a8\u9700\u8981\u4e3a llama.cpp \u6784\u5efa\u4e00\u4e9b\u4f9d\u8d56\u9879\u3002\u5728\u5f00\u53d1\u677f\u4e0a\u6253\u5f00\u7ec8\u7aef\uff0c\u6216\u5efa\u7acb SSH \u4f1a\u8bdd\uff0c\u7136\u540e\u6267\u884c\u4ee5\u4e0b\u64cd\u4f5c\uff1a"}),"\n",(0,a.jsx)(n.p,{children:"1\ufe0f\u20e3 \u5b89\u88c5\u6784\u5efa\u4f9d\u8d56\u9879\uff1a"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"sudo apt update\r\nsudo apt install -y cmake ninja-build curl libcurl4-openssl-dev\n"})}),"\n",(0,a.jsx)(n.p,{children:"2\ufe0f\u20e3 \u5b89\u88c5 OpenCL \u5934\u6587\u4ef6\u548c ICD \u52a0\u8f7d\u5668\u5e93\uff1a"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:'mkdir -p ~/dev/llm\r\n\r\n# Symlink the OpenCL shared library\r\nsudo rm -f /usr/lib/libOpenCL.so\r\nsudo ln -s /lib/aarch64-linux-gnu/libOpenCL.so.1.0.0 /usr/lib/libOpenCL.so\r\n\r\n# OpenCL headers\r\ncd ~/dev/llm\r\ngit clone https://github.com/KhronosGroup/OpenCL-Headers\r\ncd OpenCL-Headers\r\ngit checkout 5d52989617e7ca7b8bb83d7306525dc9f58cdd46\r\nmkdir -p build && cd build\r\ncmake .. -G Ninja \\\r\n    -DBUILD_TESTING=OFF \\\r\n    -DOPENCL_HEADERS_BUILD_TESTING=OFF \\\r\n    -DOPENCL_HEADERS_BUILD_CXX_TESTS=OFF \\\r\n    -DCMAKE_INSTALL_PREFIX="$HOME/dev/llm/opencl"\r\ncmake --build . --target install\r\n\r\n# ICD Loader\r\ncd ~/dev/llm\r\ngit clone https://github.com/KhronosGroup/OpenCL-ICD-Loader\r\ncd OpenCL-ICD-Loader\r\ngit checkout 02134b05bdff750217bf0c4c11a9b13b63957b04\r\nmkdir -p build && cd build\r\ncmake .. -G Ninja \\\r\n    -DCMAKE_BUILD_TYPE=Release \\\r\n    -DCMAKE_PREFIX_PATH="$HOME/dev/llm/opencl" \\\r\n    -DCMAKE_INSTALL_PREFIX="$HOME/dev/llm/opencl"\r\ncmake --build . --target install\r\n\r\n# Symlink OpenCL headers\r\nsudo rm -f /usr/include/CL\r\nsudo ln -s ~/dev/llm/opencl/include/CL/ /usr/include/CL\n'})}),"\n",(0,a.jsx)(n.p,{children:"3\ufe0f\u20e3 \u4f7f\u7528 OpenCL \u540e\u7aef\u7f16\u8bd1 llama.cpp\uff1a"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"cd ~/dev/llm\r\n\r\n# Clone repository\r\ngit clone https://github.com/ggml-org/llama.cpp\r\ncd llama.cpp\r\n\r\n# We've tested this commit explicitly, you can try master if you want bleeding edge\r\ngit checkout f6da8cb86a28f0319b40d9d2a957a26a7d875f8c\r\n\r\n# Build\r\nmkdir -p build\r\ncd build\r\ncmake .. -G Ninja \\\r\n    -DCMAKE_BUILD_TYPE=Release \\\r\n    -DBUILD_SHARED_LIBS=OFF \\\r\n    -DGGML_OPENCL=ON\r\nninja -j`nproc`\n"})}),"\n",(0,a.jsx)(n.p,{children:"4\ufe0f\u20e3 \u5c06 llama.cpp \u8def\u5f84\u6dfb\u52a0\u5230 PATH\uff1a"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:'cd ~/dev/llm/llama.cpp/build/bin\r\n\r\necho "" >> ~/.bash_profile\r\necho "# Begin llama.cpp" >> ~/.bash_profile\r\necho "export PATH=\\$PATH:$PWD" >> ~/.bash_profile\r\necho "# End llama.cpp" >> ~/.bash_profile\r\necho "" >> ~/.bash_profile\r\n\r\n# To use the llama.cpp files in your current session\r\nsource ~/.bash_profile\n'})}),"\n",(0,a.jsx)(n.p,{children:"5\ufe0f\u20e3 \u73b0\u5728\u5df2\u5b8c\u6210 llama.cpp \u7684\u90e8\u7f72:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"llama-cli --version\r\n# ggml_opencl: selected platform: 'QUALCOMM Snapdragon(TM)'\r\n# ggml_opencl: device: 'QUALCOMM Adreno(TM) 635 (OpenCL 3.0 Adreno(TM) 635)'\r\n# ggml_opencl: OpenCL driver: OpenCL 3.0 QUALCOMM build: 0808.0.7 Compiler E031.49.02.00\r\n# ggml_opencl: vector subgroup broadcast support: true\n"})}),"\n",(0,a.jsx)(n.h3,{id:"\u4e0b\u8f7d\u5e76\u91cf\u5316\u6a21\u578b",children:"\u4e0b\u8f7d\u5e76\u91cf\u5316\u6a21\u578b"}),"\n",(0,a.jsxs)(n.p,{children:["\u4e3a\u4e86\u8fd0\u884c GPU \u52a0\u901f\u7684\u6a21\u578b\uff0c\u4f60\u9700\u8981\u4f7f\u7528\u7eaf 4-bit \u91cf\u5316(",(0,a.jsx)(n.code,{children:"Q4_0"}),")\u6a21\u578b\uff0c\u5e76\u4e14\u683c\u5f0f\u4e3a GGUF\uff08llama.cpp \u683c\u5f0f\uff0c",(0,a.jsx)(n.a,{href:"https://github.com/ggml-org/llama.cpp/discussions/2948",children:"\u8f6c\u6362\u6307\u5357"}),")\u3002\u53ef\u4ee5\u9009\u62e9\u5df2\u91cf\u5316\u597d\u7684\u6a21\u578b\uff0c\u6216\u4f7f\u7528 ",(0,a.jsx)(n.code,{children:"llama-quantize"})," \u81ea\u884c\u91cf\u5316\u6a21\u578b\u3002\u4f8b\u5982\uff0c\u5bf9\u4e8e Qwen2-1.5B-Instruct\uff1a"]}),"\n",(0,a.jsxs)(n.p,{children:["1\ufe0f\u20e3\u4ece HuggingFace \u83b7\u53d6 fp16 \u683c\u5f0f\u7684 ",(0,a.jsx)(n.a,{href:"https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF",children:"Qwen2-1.5B-Instruct"})," \u5e76\u4f7f\u7528 ",(0,a.jsx)(n.code,{children:"llama-quantize"})," \u8fdb\u884c\u91cf\u5316\uff1a"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"# Download fp16 model\r\nwget https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF/resolve/main/qwen2-1_5b-instruct-fp16.gguf\r\n\r\n# Quantize (pure Q4_0)\r\nllama-quantize --pure qwen2-1_5b-instruct-fp16.gguf qwen2-1_5b-instruct-q4_0-pure.gguf Q4_0\n"})}),"\n",(0,a.jsx)(n.p,{children:"2\ufe0f\u20e3\u6309\u7167 llama.cpp \u7f16\u8bd1\u8bf4\u660e\u8fd0\u884c\u6b64\u6a21\u578b\u3002"}),"\n",(0,a.jsx)(n.h3,{id:"\u4f7f\u7528-llama-cli-\u8fd0\u884c\u4f60\u7684\u7b2c\u4e00\u4e2a-llm",children:"\u4f7f\u7528 llama-cli \u8fd0\u884c\u4f60\u7684\u7b2c\u4e00\u4e2a LLM"}),"\n",(0,a.jsxs)(n.p,{children:["\u60a8\u73b0\u5728\u53ef\u4ee5\u901a\u8fc7",(0,a.jsx)(n.code,{children:"llama-cli"})," \u8fd0\u884cLLM\u3002\u5b83\u4f1a\u81ea\u52a8\u5c06\u5404\u5c42\u8f6c\u79fb\u5230 GPU\uff1a"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:'llama-cli -m ./qwen2-1_5b-instruct-q4_0-pure.gguf -no-cnv --no-warmup -b 128 -c 2048 -s 11 -n 128 -p "Knock knock, " -fa off\r\n\r\n# ... You\'ll see:\r\n# load_tensors: offloaded 29/29 layers to GPU\r\n# ...\r\n# Knock knock, 11:59 pm ... rest of the story\n'})}),"\n",(0,a.jsx)(n.p,{children:"\u73b0\u5728\uff0c\u60a8\u8bbe\u5907\u7684 GPU \u4e0a\u5df2\u7ecf\u8fd0\u884c\u6709 LLM\u3002"}),"\n",(0,a.jsx)(n.h3,{id:"\u4f7f\u7528-llama-server-\u90e8\u7f72-llm",children:"\u4f7f\u7528 llama-server \u90e8\u7f72 LLM"}),"\n",(0,a.jsxs)(n.p,{children:["\u63a5\u4e0b\u6765\u53ef\u4ee5\u4f7f\u7528",(0,a.jsx)(n.code,{children:"llama-server"}),"\u542f\u52a8\u5e26\u6709\u804a\u5929\u754c\u9762\u7684 Web \u670d\u52a1\u5668\uff0c\u8be5\u670d\u52a1\u5668\u540c\u65f6\u63d0\u4f9bOpenAI\u517c\u5bb9\u7684\u4f1a\u8bdd\u8865\u5168API\u3002"]}),"\n",(0,a.jsx)(n.p,{children:"1\ufe0f\u20e3 \u9996\u5148\uff0c\u627e\u5230\u5f00\u53d1\u677f\u7684IP\u5730\u5740\uff1a"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"ifconfig | grep -Eo 'inet (addr:)?([0-9]*\\.){3}[0-9]*' | grep -Eo '([0-9]*\\.){3}[0-9]*' | grep -v '127.0.0.1'\r\n\r\n# ... Example:\r\n# 192.168.1.253\n"})}),"\n",(0,a.jsx)(n.p,{children:"2\ufe0f\u20e3 \u901a\u8fc7\u4ee5\u4e0b\u65b9\u5f0f\u542f\u52a8\u670d\u52a1\u5668\uff1a"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"llama-server -m ./qwen2-1_5b-instruct-q4_0-pure.gguf --no-warmup -b 128 -c 2048 -s 11 -n 128 --host 0.0.0.0 --port 9876\n"})}),"\n",(0,a.jsxs)(n.p,{children:["3\ufe0f\u20e3 \u5728\u60a8\u7684\u8ba1\u7b97\u673a\u4e0a\uff0c\u6253\u5f00 Web \u6d4f\u89c8\u5668\u5e76\u524d\u5f80 ",(0,a.jsx)(n.code,{children:"http://192.168.1.253:9876"})," \uff08\u5c06 IP \u5730\u5740\u66ff\u6362\u4e3a\u60a8\u5728 1 \u4e2d\u627e\u5230\u7684 IP \u5730\u5740\uff09\uff1a"]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.img,{src:"https://3580193864-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxM5xrbdbelLSl7uN8oac%2Fuploads%2Fgit-blob-1c1d0f018f51abf311522cf4f398bcc4b69fb102%2Fllamacpp1.png?alt=media",alt:"",title:"\u4f7f\u7528 llama-server \u90e8\u7f72 LLM"}),"4\ufe0f\u20e3 \u60a8\u8fd8\u53ef\u4ee5\u901a\u8fc7 OpenAI \u4f1a\u8bdd\u8865\u5168 API \u4ee5\u7f16\u7a0b\u65b9\u5f0f\u8bbf\u95ee\u6b64\u670d\u52a1\u5668\u3002\u4f8b\u5982\uff0c\u4ece Python\uff1a",(0,a.jsx)(n.br,{}),"\n","a.\u521b\u5efa\u4e00\u4e2a\u65b0\u7684\u865a\u62df\u73af\u5883 \uff08venv\uff09 \u5e76\u5b89\u88c5",(0,a.jsx)(n.code,{children:"requests"}),"\uff1a"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"python3 -m venv .venv-chat\r\nsource .venv/bin/activate\r\npip3 install requests\n"})}),"\n",(0,a.jsxs)(n.p,{children:["b. \u521b\u5efa\u4e00\u4e2a\u65b0\u6587\u4ef6",(0,a.jsx)(n.code,{children:"chat.py"}),"\uff1a"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import requests\r\n\r\n# if running from your own computer, replace localhost with the IP address of your development board\r\nurl = "http://localhost:9876/v1/chat/completions"\r\n\r\npayload = {\r\n    "messages": [\r\n        {"role": "system", "content": "You are a helpful assistant."},\r\n        {"role": "user", "content": "Explain Qualcomm in one sentence."}\r\n    ],\r\n    "temperature": 0.7,\r\n    "max_tokens": 200\r\n}\r\n\r\nresponse = requests.post(url, headers={ "Content-Type": "application/json" }, json=payload)\r\nprint(response.json())\n'})}),"\n",(0,a.jsxs)(n.p,{children:["c. \u8fd0\u884c",(0,a.jsx)(n.code,{children:"chat.py"}),"\uff1a"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"python3 chat.py\r\n\r\n# ...\r\n# {'choices': [{'finish_reason': 'stop', 'index': 0, 'message': {'role': 'assistant', 'content': 'Qualcomm is a leading global technology company that designs, develops, licenses, and markets semiconductor-based products and mobile platform technologies to major telecommunications and consumer electronics manufacturers worldwide.'}}], 'created': 1757073340, 'model': 'gpt-3.5-turbo', 'system_fingerprint': 'b6362-f6da8cb8', 'object': 'chat.completion', 'usage': {'completion_tokens': 34, 'prompt_tokens': 26, 'total_tokens': 60}, 'id': 'chatcmpl-3O7l005WG1DzN191FTNomJNweHMoH8Is', 'timings': {'prompt_n': 12, 'prompt_ms': 303.581, 'prompt_per_token_ms': 25.298416666666668, 'prompt_per_second': 39.52816546490064, 'predicted_n': 34, 'predicted_ms': 4052.23, 'predicted_per_token_ms': 119.18323529411765, 'predicted_per_second': 8.390441806116632}}\n"})}),"\n",(0,a.jsx)(n.h3,{id:"\u90e8\u7f72\u591a\u6a21\u6001llm",children:"\u90e8\u7f72\u591a\u6a21\u6001LLM"}),"\n",(0,a.jsxs)(n.p,{children:["\u60a8\u540c\u6837\u53ef\u4ee5\u90e8\u7f72\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u3002\u4f8b\u5982 ",(0,a.jsx)(n.a,{href:"https://huggingface.co/ggml-org/SmolVLM-500M-Instruct-GGUF",children:"SmolVLM-500M-Instruct-GGUF"}),"\u3002\u4e0b\u8f7d Q4_0 \u91cf\u5316\u6743\u91cd\u6587\u4ef6\uff08\u6216\u81ea\u884c\u91cf\u5316\uff09\uff0c\u5e76\u4e0b\u8f7d CLIP \u7f16\u7801\u5668",(0,a.jsx)(n.code,{children:"mmproj-*.gguf"}),"\u6587\u4ef6\u3002\u4f8b\u5982\uff1a"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"# Download weights\r\nwget https://huggingface.co/ggml-org/SmolVLM-500M-Instruct-GGUF/resolve/main/SmolVLM-500M-Instruct-f16.gguf\r\nwget https://huggingface.co/ggml-org/SmolVLM-500M-Instruct-GGUF/resolve/main/mmproj-SmolVLM-500M-Instruct-f16.gguf\r\n\r\n# Quantize model (mmproj- models are not quantizable via llama-quantize, see below)\r\nllama-quantize --pure SmolVLM-500M-Instruct-f16.gguf SmolVLM-500M-Instruct-q4_0-pure.gguf Q4_0\r\n\r\n# Server the model\r\nllama-server -m ./SmolVLM-500M-Instruct-q4_0-pure.gguf --mmproj ./mmproj-SmolVLM-500M-Instruct-f16.gguf --no-warmup -b 128 -c 2048 -s 11 -n 128 --host 0.0.0.0 --port 9876\n"})}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.img,{src:"https://3580193864-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxM5xrbdbelLSl7uN8oac%2Fuploads%2Fgit-blob-272f54a74290f52156033bda8b2d480621ae78ab%2Fllamacpp2.png?alt=media",alt:"",title:"\u4f7f\u7528 llama-server \u90e8\u7f72\u591a\u6a21\u6001 LLM"})}),"\n",(0,a.jsxs)(n.admonition,{type:"info",children:[(0,a.jsx)(n.mdxAdmonitionTitle,{}),(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"CLIP \u6a21\u578b\u4ecd\u4e3a fp16:"})," ",(0,a.jsx)(n.code,{children:"mmproj"})," \u6a21\u578b\u4ecd\u7136\u662f fp16\uff1b\u56e0\u6b64\u5904\u7406\u56fe\u50cf\u4f1a\u5f88\u6162\u3002",(0,a.jsx)(n.a,{href:"https://github.com/ggml-org/llama.cpp/pull/11644",children:"\u65e7\u7248\u672c llama.cpp"})," \u4e2d\uff0c\u6709\u91cf\u5316 CLIP \u7f16\u7801\u5668\u7684\u4ee3\u7801\u3002"]})]}),"\n",(0,a.jsx)(n.h2,{id:"tips--tricks",children:"Tips & tricks"}),"\n",(0,a.jsx)(n.h3,{id:"\u6bd4\u8f83-cpu-\u6027\u80fd",children:"\u6bd4\u8f83 CPU \u6027\u80fd"}),"\n",(0,a.jsxs)(n.p,{children:["\u628a",(0,a.jsx)(n.code,{children:"-ngl 0"}),"\u6dfb\u52a0\u5230",(0,a.jsx)(n.code,{children:"llama-*"}),"\u547d\u4ee4\u4ee5\u8df3\u8fc7\u5c06\u5c42\u8f6c\u79fb\u5230 GPU \u7684\u64cd\u4f5c\u3002\u6a21\u578b\u5c06\u5728 CPU \u4e0a\u8fd0\u884c\uff0c\u60a8\u53ef\u4ee5\u5c06\u5176\u4e0e GPU \u6027\u80fd\u8fdb\u884c\u6bd4\u8f83\u3002"]}),"\n",(0,a.jsx)(n.p,{children:"\u4f8b\u5982\uff0c RB3 Gen 2 Vision Kit \u4e0a\u7684 Qwen2-1.5B-Instruct Q4_0\uff1a"}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"GPU:"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:'llama-cli -m ./qwen2-1_5b-instruct-q4_0-pure.gguf -no-cnv --no-warmup -b 128 -c 2048 -s 11 -n 128 -p "Knock knock, " -fa off\r\n\r\n# llama_perf_sampler_print:    sampling time =     225.78 ms /   133 runs   (    1.70 ms per token,   589.06 tokens per second)\r\n# llama_perf_context_print:        load time =    5338.13 ms\r\n# llama_perf_context_print: prompt eval time =     201.32 ms /     5 tokens (   40.26 ms per token,    24.84 tokens per second)\r\n# llama_perf_context_print:        eval time =   13214.35 ms /   127 runs   (  104.05 ms per token,     9.61 tokens per second)\r\n# llama_perf_context_print:       total time =   18958.06 ms /   132 tokens\r\n# llama_perf_context_print:    graphs reused =        122\n'})}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"CPU:"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:'llama-cli -m ./qwen2-1_5b-instruct-q4_0-pure.gguf -no-cnv --no-warmup -b 128 -ngl 99 -c 2048 -s 11 -n 128 -p "Knock knock, " -fa off -ngl 0\r\n\r\n# llama_perf_sampler_print:    sampling time =      23.47 ms /   133 runs   (    0.18 ms per token,  5666.08 tokens per second)\r\n# llama_perf_context_print:        load time =     677.25 ms\r\n# llama_perf_context_print: prompt eval time =     253.39 ms /     5 tokens (   50.68 ms per token,    19.73 tokens per second)\r\n# llama_perf_context_print:        eval time =   17751.29 ms /   127 runs   (  139.77 ms per token,     7.15 tokens per second)\r\n# llama_perf_context_print:       total time =   18487.26 ms /   132 tokens\r\n# llama_perf_context_print:    graphs reused =        122\n'})}),"\n",(0,a.jsx)(n.p,{children:"\u8fd9\u91cc GPU \u7684\u4ee4\u724c\u5904\u7406\u901f\u5ea6\u6bd4 CPU \u5feb\u7ea6 33%\u3002"})]})}function d(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(p,{...e})}):p(e)}},8453:(e,n,r)=>{r.d(n,{R:()=>t,x:()=>c});var l=r(6540);const a={},s=l.createContext(a);function t(e){const n=l.useContext(s);return l.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function c(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:t(e.components),l.createElement(s.Provider,{value:n},e.children)}}}]);