"use strict";(globalThis.webpackChunkmy_website=globalThis.webpackChunkmy_website||[]).push([[8612],{1045:(e,n,t)=>{t.d(n,{A:()=>i});const i=t.p+"assets/images/IMSDK_3-5f3156437a0bd430520f6618a2fe6c70.png"},2263:(e,n,t)=>{t.d(n,{A:()=>i});const i=t.p+"assets/images/IMSDK_1-4d8561f82f0209429c819a43f2714a8f.png"},5660:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>d,contentTitle:()=>o,default:()=>c,frontMatter:()=>s,metadata:()=>i,toc:()=>l});const i=JSON.parse('{"id":"Application Development and Execution Guide/IMSDK/Develop-app","title":"\u5f00\u53d1\u4f60\u7684\u7b2c\u4e00\u4e2a\u5e94\u7528\u7a0b\u5e8f","description":"Qualcomm\xae \u667a\u80fd\u591a\u5a92\u4f53 SDK (IM SDK) \u662f\u4e00\u7ec4 GStreamer \u63d2\u4ef6\uff0c\u53ef\u8ba9\u60a8\u5728 Dragonwing \u5f00\u53d1\u677f\u7684 GPU \u4e0a\u8fd0\u884c\u8ba1\u7b97\u673a\u89c6\u89c9\u64cd\u4f5c\uff0c\u5e76\u4e14\u53ef\u4ee5\u521b\u5efa\u5b8c\u5168\u5728 GPU \u548c NPU \u4e0a\u8fd0\u884c\u7684 AI \u7ba1\u9053\uff0c\u800c\u65e0\u9700\u56de\u5230 CPU\uff08\u96f6\u62f7\u8d1d\uff09\u3002\u6bd4\u8d77\u5728 OpenCV + TFLite \u4e2d\u5b9e\u73b0 AI CV \u7ba1\u9053\uff0c\u8fd9\u6837\u53ef\u4ee5\u5b9e\u73b0\u66f4\u9ad8\u7684\u541e\u5410\u7387\u3002","source":"@site/docs/7.Application Development and Execution Guide/3.IMSDK/3.Develop-app.md","sourceDirName":"7.Application Development and Execution Guide/3.IMSDK","slug":"/Application Development and Execution Guide/IMSDK/Develop-app","permalink":"/ubuntu100-cn.github.io/docs/Application Development and Execution Guide/IMSDK/Develop-app","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/7.Application Development and Execution Guide/3.IMSDK/3.Develop-app.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"\u5b9a\u5236\u73b0\u6709\u7684\u793a\u4f8b\u5e94\u7528\u7a0b\u5e8f","permalink":"/ubuntu100-cn.github.io/docs/Application Development and Execution Guide/IMSDK/Customize-Sample"},"next":{"title":"Robotics Sample Applications","permalink":"/ubuntu100-cn.github.io/docs/Application Development and Execution Guide/Robotics-Sample-Applications/Robotics Sample Applications"}}');var r=t(4848),a=t(8453);const s={},o="\u5f00\u53d1\u4f60\u7684\u7b2c\u4e00\u4e2a\u5e94\u7528\u7a0b\u5e8f",d={},l=[{value:"\u96c6\u6210 IM SDK \u7684 GStreamer \u7ba1\u9053",id:"\u96c6\u6210-im-sdk-\u7684-gstreamer-\u7ba1\u9053",level:2},{value:"\u8bbe\u7f6e GStreamer \u548c IM SDK",id:"\u8bbe\u7f6e-gstreamer-\u548c-im-sdk",level:2},{value:"\u793a\u4f8b 1\uff1a\u5728GPU \u548c CPU \u4e0a\u8fdb\u884c\u5c3a\u5bf8\u8c03\u6574\u548c\u753b\u9762\u88c1\u526a",id:"\u793a\u4f8b-1\u5728gpu-\u548c-cpu-\u4e0a\u8fdb\u884c\u5c3a\u5bf8\u8c03\u6574\u548c\u753b\u9762\u88c1\u526a",level:3},{value:"\u793a\u4f8b 2\uff1a\u6d41\u5206\u53d1\u4e0e\u591a\u8def\u8f93\u51fa",id:"\u793a\u4f8b-2\u6d41\u5206\u53d1\u4e0e\u591a\u8def\u8f93\u51fa",level:3},{value:"\u793a\u4f8b 3\uff1a\u8fd0\u884c\u795e\u7ecf\u7f51\u7edc",id:"\u793a\u4f8b-3\u8fd0\u884c\u795e\u7ecf\u7f51\u7edc",level:3},{value:"3.1\uff1aPython \u4e2d\u7684\u795e\u7ecf\u7f51\u7edc\u4e0e\u5408\u6210",id:"31python-\u4e2d\u7684\u795e\u7ecf\u7f51\u7edc\u4e0e\u5408\u6210",level:4},{value:"3.2\uff1a\u4f7f\u7528 IM SDK \u8fd0\u884c\u795e\u7ecf\u7f51\u7edc",id:"32\u4f7f\u7528-im-sdk-\u8fd0\u884c\u795e\u7ecf\u7f51\u7edc",level:4},{value:"3.3: \u53e0\u52a0\u5c42",id:"33-\u53e0\u52a0\u5c42",level:4},{value:"3.4\uff1a\u5c06\u795e\u7ecf\u7f51\u7edc\u4e0e\u53e0\u52a0\u5c42\u76f8\u7ed3\u5408",id:"34\u5c06\u795e\u7ecf\u7f51\u7edc\u4e0e\u53e0\u52a0\u5c42\u76f8\u7ed3\u5408",level:4},{value:"\u6545\u969c\u6392\u9664",id:"\u6545\u969c\u6392\u9664",level:2},{value:"\u7ba1\u9053\u65e0\u8f93\u51fa",id:"\u7ba1\u9053\u65e0\u8f93\u51fa",level:4},{value:"QMMF Recorder StartCamera Failed / \u65e0\u6cd5\u6253\u5f00\u6444\u50cf\u5934",id:"qmmf-recorder-startcamera-failed--\u65e0\u6cd5\u6253\u5f00\u6444\u50cf\u5934",level:4}];function m(e){const n={a:"a",admonition:"admonition",br:"br",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",img:"img",li:"li",p:"p",pre:"pre",ul:"ul",...(0,a.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"\u5f00\u53d1\u4f60\u7684\u7b2c\u4e00\u4e2a\u5e94\u7528\u7a0b\u5e8f",children:"\u5f00\u53d1\u4f60\u7684\u7b2c\u4e00\u4e2a\u5e94\u7528\u7a0b\u5e8f"})}),"\n",(0,r.jsx)(n.p,{children:"Qualcomm\xae \u667a\u80fd\u591a\u5a92\u4f53 SDK (IM SDK) \u662f\u4e00\u7ec4 GStreamer \u63d2\u4ef6\uff0c\u53ef\u8ba9\u60a8\u5728 Dragonwing \u5f00\u53d1\u677f\u7684 GPU \u4e0a\u8fd0\u884c\u8ba1\u7b97\u673a\u89c6\u89c9\u64cd\u4f5c\uff0c\u5e76\u4e14\u53ef\u4ee5\u521b\u5efa\u5b8c\u5168\u5728 GPU \u548c NPU \u4e0a\u8fd0\u884c\u7684 AI \u7ba1\u9053\uff0c\u800c\u65e0\u9700\u56de\u5230 CPU\uff08\u96f6\u62f7\u8d1d\uff09\u3002\u6bd4\u8d77\u5728 OpenCV + TFLite \u4e2d\u5b9e\u73b0 AI CV \u7ba1\u9053\uff0c\u8fd9\u6837\u53ef\u4ee5\u5b9e\u73b0\u66f4\u9ad8\u7684\u541e\u5410\u7387\u3002"}),"\n",(0,r.jsx)(n.h2,{id:"\u96c6\u6210-im-sdk-\u7684-gstreamer-\u7ba1\u9053",children:"\u96c6\u6210 IM SDK \u7684 GStreamer \u7ba1\u9053"}),"\n",(0,r.jsxs)(n.p,{children:["\u667a\u80fd\u591a\u5a92\u4f53 SDK (IM SDK) \u5efa\u7acb\u5728\u5f3a\u5927\u7684\u591a\u5a92\u4f53\u6846\u67b6 GStreamer \u4e4b\u4e0a\u3002\u5f00\u53d1\u4eba\u5458\u80fd\u591f\u5c06\u89c6\u9891\u548c\u97f3\u9891\u5904\u7406\u5de5\u4f5c\u6d41\u5b9a\u4e49\u4e3a\u7ba1\u9053\u3002\u4f7f\u7528 GStreamer\uff0c\u60a8\u53ef\u4ee5\u5728\u4e00\u4e2a\u7ba1\u9053\u5b57\u7b26\u4e32\u4e2d\u5b9a\u4e49\u6574\u4e2a\u5904\u7406\u6d41\u7a0b\uff0c\u800c\u4e0d\u9700\u8981\u624b\u52a8\u7f16\u5199\u6355\u83b7\u5e27\u3001\u5c3a\u5bf8\u8c03\u6574\u3001\u753b\u9762\u88c1\u526a\u3001\u8fd0\u884c\u63a8\u7406\u548c\u6e32\u67d3\u8f93\u51fa\u7b49\u6b65\u9aa4\u3002\u6846\u67b6\u4f1a\u81ea\u52a8\u5904\u7406\u6267\u884c\u3001\u540c\u6b65\u548c\u6570\u636e\u6d41\u3002",(0,r.jsx)(n.br,{}),"\n","\u5728 Qualcomm\xae \u786c\u4ef6\u4e0a\uff0cIM SDK \u901a\u8fc7\u5168\u7ba1\u9053\u7684\u65e0\u7f1d\u52a0\u901f\u6765\u8fdb\u4e00\u6b65\u4f18\u5316\u8fd9\u79cd\u4f53\u9a8c\u3002\u56fe\u50cf\u5c3a\u5bf8\u8c03\u6574\u3001\u753b\u9762\u88c1\u526a\u548c\u53e0\u52a0\u6e32\u67d3\u7b49\u4efb\u52a1\u7531 GPU \u627f\u62c5\uff0c\u800c\u63a8\u7406\u64cd\u4f5c\u5219\u5728 NPU \u4e0a\u6267\u884c\u3002\u8fd9\u79cd\u96f6\u62f7\u8d1d\u67b6\u6784\u786e\u4fdd\u6570\u636e\u6d41\u7ecf\u5728\u6574\u4e2a\u5904\u7406\u8fc7\u7a0b\u4e2d\u65e0\u9700CPU\u4ecb\u5165\uff0c\u4ece\u800c\u5b9e\u73b0\u5b9e\u65f6\u6027\u80fd\u5e76\u964d\u4f4e\u7cfb\u7edf\u8d1f\u8f7d\u3002",(0,r.jsx)(n.br,{}),"\n","\u4e3a\u5b9e\u73b0\u8fd9\u4e00\u76ee\u6807\uff0cIM SDK \u63d0\u4f9b\u4e86\u4e13\u95e8\u7684 GStreamer \u63d2\u4ef6\uff1a"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.code,{children:"qtivtransform"}),"\uff1a\u4f7f\u7528 GPU \u52a0\u901f\u8272\u5f69\u8f6c\u6362\u3001\u753b\u9762\u88c1\u526a\u548c\u5c3a\u5bf8\u8c03\u6574\u3002",(0,r.jsx)(n.br,{}),"\n",(0,r.jsx)(n.code,{children:"qtimltflite"}),"\uff1a\u5728 NPU \u4e0a\u6267\u884c TensorFlow Lite \u6a21\u578b\u3002"]}),"\n",(0,r.jsx)(n.p,{children:"\u901a\u8fc7\u96c6\u6210\u8fd9\u4e9b\u63d2\u4ef6\uff0c\u5f00\u53d1\u4eba\u5458\u53ef\u4ee5\u4f7f\u7528\u719f\u6089\u7684 GStreamer \u8bed\u6cd5\u6784\u5efa\u9ad8\u6027\u80fd\u591a\u5a92\u4f53\u5e94\u7528\u7a0b\u5e8f\uff0c\u540c\u65f6\u4eab\u53d7\u5e95\u5c42\u7684\u786c\u4ef6\u52a0\u901f\u7684\u6027\u80fd\u3002"}),"\n",(0,r.jsx)(n.h2,{id:"\u8bbe\u7f6e-gstreamer-\u548c-im-sdk",children:"\u8bbe\u7f6e GStreamer \u548c IM SDK"}),"\n",(0,r.jsx)(n.p,{children:"\u73b0\u5728\u6211\u4eec\u4e00\u8d77\u6765\u4f7f\u7528 IM SDK \u6784\u5efa\u4e00\u4e9b\u5e94\u7528\u7a0b\u5e8f\u3002"}),"\n",(0,r.jsx)(n.p,{children:"1\ufe0f\u20e3\u5b89\u88c5 GStreamer\u3001IM SDK \u4ee5\u53ca\u672c\u4f8b\u4e2d\u9700\u8981\u7684\u4e00\u4e9b\u989d\u5916\u4f9d\u8d56\u9879\u3002\u5728\u5f00\u53d1\u677f\u4e0a\u6253\u5f00\u7ec8\u7aef\uff0c\u6216\u5efa\u7acb SSH \u4f1a\u8bdd\uff0c\u7136\u540e\u6267\u884c\u4ee5\u4e0b\u64cd\u4f5c\uff1a"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# Add the Qualcomm IoT PPA\nsudo apt-add-repository -y ppa:ubuntu-qcom-iot/qcom-ppa\n\n# Install GStreamer / IM SDK\nsudo apt update\nsudo apt install -y gstreamer1.0-tools gstreamer1.0-tools gstreamer1.0-plugins-good gstreamer1.0-plugins-base gstreamer1.0-plugins-base-apps gstreamer1.0-plugins-qcom-good gstreamer1.0-qcom-sample-apps\n\n# Install Python bindings for GStreamer, and some build dependencies\nsudo apt install -y v4l-utils libcairo2-dev pkg-config python3-dev libgirepository1.0-dev gir1.2-gstreamer-1.0\n"})}),"\n",(0,r.jsx)(n.p,{children:"2\ufe0f\u20e3\u514b\u9686\u793a\u4f8b repo\uff0c\u521b\u5efa\u865a\u62df\u73af\u5883\uff08venv\uff09\uff0c\u5e76\u5b89\u88c5\u5176\u4f9d\u8d56\u9879\uff1a"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# Clone repo\ngit clone https://github.com/edgeimpulse/qc-ai-docs-examples-imsdk.git\ncd qc-ai-docs-examples-imsdk/tutorial\n\n# Create a new venv\npython3 -m venv .venv\nsource .venv/bin/activate\n\n# Install Python dependencies\npip3 install -r requirements.txt\n"})}),"\n",(0,r.jsx)(n.p,{children:"3\ufe0f\u20e3\u9700\u8981\u51c6\u5907\u4e00\u4e2a\u6444\u50cf\u5934\uff08RB3 Gen 2 Vision Kit\u5185\u7f6e\uff09\u6216\u4e00\u4e2a USB \u7f51\u7edc\u6444\u50cf\u5934\u3002"}),"\n",(0,r.jsx)(n.p,{children:"\u5982\u679c\u60a8\u60f3\u4f7f\u7528 USB \u7f51\u7edc\u6444\u50cf\u5934\uff1a"}),"\n",(0,r.jsx)(n.p,{children:"\u67e5\u770b\u8bbe\u5907 ID\uff1a"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"v4l2-ctl --list-devices\n# msm_vidc_media (platform:aa00000.video-codec):\n#         /dev/media0\n#\n# msm_vidc_decoder (platform:msm_vidc_bus):\n#         /dev/video32\n#         /dev/video33\n#\n# C922 Pro Stream Webcam (usb-0000:01:00.0-2):\n#         /dev/video2     <-- So /dev/video2\n#         /dev/video3\n#         /dev/media3\n"})}),"\n",(0,r.jsx)(n.p,{children:"4\ufe0f\u20e3\u8bbe\u7f6e\u73af\u5883\u53d8\u91cf\uff08\u6211\u4eec\u5c06\u5728\u793a\u4f8b\u4e2d\u4f7f\u7528\uff09\uff1a"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'export IMSDK_VIDEO_SOURCE="v4l2src device=/dev/video2"\n'})}),"\n",(0,r.jsx)(n.p,{children:"\u5982\u679c\u60a8\u4f7f\u7528\u7684\u662f RB3 Gen 2 Vision Kit\uff0c\u5e76\u4e14\u60f3\u8981\u4f7f\u7528\u5185\u7f6e\u6444\u50cf\u5934\uff1a"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'export IMSDK_VIDEO_SOURCE="qtiqmmfsrc name=camsrc camera=0"\n'})}),"\n",(0,r.jsx)(n.h3,{id:"\u793a\u4f8b-1\u5728gpu-\u548c-cpu-\u4e0a\u8fdb\u884c\u5c3a\u5bf8\u8c03\u6574\u548c\u753b\u9762\u88c1\u526a",children:"\u793a\u4f8b 1\uff1a\u5728GPU \u548c CPU \u4e0a\u8fdb\u884c\u5c3a\u5bf8\u8c03\u6574\u548c\u753b\u9762\u88c1\u526a"}),"\n",(0,r.jsx)(n.p,{children:"\u672c\u8282\u4f1a\u5c55\u793aGPU\u5728\u5904\u7406\u901f\u5ea6\u4e0a\u76f8\u8f83\u4e8eCPU\u7684\u4f18\u52bf\u3002\u5982\u679c\u795e\u7ecf\u7f51\u7edc\u8981\u6c42\u8f93\u5165 224x224 RGB \u7684\u56fe\u50cf\uff0c\u5219\u9700\u8981\u9884\u5904\u7406\u6570\u636e\uff1a\u9996\u5148\uff0c\u4ece\u7f51\u7edc\u6444\u50cf\u5934\u6293\u53d6\u5e27\uff08\u4f8b\u5982\u539f\u59cb\u5206\u8fa8\u7387\u4e3a 1980x1080\uff09\uff1b\u5c06\u5176\u88c1\u526a\u4e3a 1/1 \u7684\u5bbd\u9ad8\u6bd4\uff08\u4f8b\u5982\uff0c\u88c1\u526a\u4e3a 1080x1080\uff09\uff1b\u8c03\u6574\u5927\u5c0f\u4e3a\u6240\u9700\u7684\u5206\u8fa8\u7387\uff08224x224\uff09\uff1b\u4ece\u50cf\u7d20\u6570\u636e\u521b\u5efa\u4e00\u4e2a Numpy \u6570\u7ec4\u3002"}),"\n",(0,r.jsxs)(n.p,{children:["1\ufe0f\u20e3 \u521b\u5efa\u4e00\u4e2a\u65b0\u7684 ",(0,r.jsx)(n.code,{children:"ex1.py"})," \u6587\u4ef6\u5e76\u5199\u5165\u4ee5\u4e0b\u4ee3\u7801\uff1a"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"from gst_helper import gst_grouped_frames, atomic_save_image, timing_marks_to_str\nimport time, argparse\n\nparser = argparse.ArgumentParser(description='GStreamer -> Python RGB frames')\nparser.add_argument('--video-source', type=str, required=True, help='GStreamer video source (e.g. \"v4l2src device=/dev/video2\" or \"qtiqmmfsrc name=camsrc camera=0\")')\nargs, unknown = parser.parse_known_args()\n\nPIPELINE = (\n    # Video source\n    f\"{args.video_source} ! \"\n    # Properties for the video source\n    \"video/x-raw,width=1920,height=1080 ! \"\n    # An identity element so we can track when a new frame is ready (so we can calc. processing time)\n    \"identity name=frame_ready_webcam silent=false ! \"\n    # Crop to square\n    \"videoconvert ! aspectratiocrop aspect-ratio=1/1 ! \"\n    # Scale to 224x224 and RGB\n    \"videoscale ! video/x-raw,format=RGB,width=224,height=224 ! \"\n    # Event when the crop/scale are done\n    \"identity name=transform_done silent=false ! \"\n    # Send out the resulting frame to an appsink (where we can pick it up from Python)\n    \"queue max-size-buffers=2 leaky=downstream ! \"\n    \"appsink name=frame drop=true sync=false max-buffers=1 emit-signals=true\"\n)\n\nfor frames_by_sink, marks in gst_grouped_frames(PIPELINE):\n    print(f\"Frame ready\")\n    print('    Data:', end='')\n    for key in list(frames_by_sink):\n        print(f' name={key} {frames_by_sink[key].shape}', end='')\n    print('')\n    print('    Timings:', timing_marks_to_str(marks))\n\n    # Save image to disk, frames_by_sink has all the\n    frame = frames_by_sink['frame']\n    atomic_save_image(frame=frame, path='out/gstreamer.png')\n"})}),"\n",(0,r.jsx)(n.p,{children:"2\ufe0f\u20e3\u8fd0\u884c\u8fd9\u6bb5\u4ee3\u7801\u3002\u8be5\u7ba1\u9053\u4f7f\u7528\u6807\u51c6 GStreamer \u7ec4\u4ef6\u5728 CPU \u4e0a\u8fd0\u884c\uff1a"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:'python\npython3 ex1.py --video-source "$IMSDK_VIDEO_SOURCE"\n\n# Frame ready\n#     Data: name=frame (224, 224, 3)\n#     Timings: frame_ready_webcam\u2192transform_done: 17.89ms, transform_done\u2192pipeline_finished: 1.89ms (total 19.78ms)\n# Frame ready\n#     Data: name=frame (224, 224, 3)\n#     Timings: frame_ready_webcam\u2192transform_done: 18.01ms, transform_done\u2192pipeline_finished: 1.42ms (total 19.44ms)\n'})}),"\n",(0,r.jsx)(n.p,{children:"\u5982\u4e0a\uff0c\u5c3a\u5bf8\u8c03\u6574/\u753b\u9762\u88c1\u526a\u9700\u8981 18 \u6beb\u79d2\uff0c\u603b\u5171\u6bcf\u5e27\u5904\u7406\u65f6\u95f4\u7ea6\u4e3a 20 \u6beb\u79d2\uff08\u4f7f\u7528 RB3 \u5185\u7f6e\u6444\u50cf\u5934\u6d4b\u91cf\uff09\u3002"}),"\n",(0,r.jsx)(n.p,{children:"\u518d\u5728 GPU \u4e0a\u8fd0\u884c\u3002\u5c06\uff1a"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'    # Crop to square\n    "videoconvert ! aspectratiocrop aspect-ratio=1/1 ! "\n    # Scale to 224x224 and RGB\n    "videoscale ! video/x-raw,format=RGB,width=224,height=224 ! "\n'})}),"\n",(0,r.jsx)(n.p,{children:"\u66ff\u6362\u4e3a\uff1a"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'    # Crop (square), the crop syntax is (`<X, Y, WIDTH, HEIGHT >`).\n    # So here we use 1920x1080 input, then center crop to 1080x1080 ((1920-1080)/2 = 420 = x crop)\n    f\'qtivtransform crop="<420, 0, 1080, 1080>" ! \'\n    # then resize to 224x224\n    "video/x-raw,format=RGB,width=224,height=224 ! "\n'})}),"\n",(0,r.jsx)(n.p,{children:"\u518d\u6b21\u8fd0\u884c\uff1a"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'python3 ex1.py --video-source "$IMSDK_VIDEO_SOURCE"\n\n# Frame ready\n#     Data: name=frame (224, 224, 3)\n#     Timings: frame_ready_webcam\u2192transform_done: 2.48ms, transform_done\u2192pipeline_finished: 1.64ms (total 4.13ms)\n# Frame ready\n#     Data: name=frame (224, 224, 3)\n#     Timings: frame_ready_webcam\u2192transform_done: 1.93ms, transform_done\u2192pipeline_finished: 1.26ms (total 3.19ms)\n'})}),"\n",(0,r.jsx)(n.p,{children:"\u73b0\u5728\uff0c\u53ea\u9700\u4e24\u884c\u4ee3\u7801\u5c31\u5c06\u753b\u9762\u88c1\u526a/\u5c3a\u5bf8\u8c03\u6574\u64cd\u4f5c\u7684\u901f\u5ea6\u63d0\u9ad8\u4e86 9 \u500d\u3002"}),"\n",(0,r.jsx)(n.h3,{id:"\u793a\u4f8b-2\u6d41\u5206\u53d1\u4e0e\u591a\u8def\u8f93\u51fa",children:"\u793a\u4f8b 2\uff1a\u6d41\u5206\u53d1\u4e0e\u591a\u8def\u8f93\u51fa"}),"\n",(0,r.jsx)(n.p,{children:"\u4e0a\u9762\u7684\u7ba1\u9053\u4e2d\u5df2\u7ecf\u4f7f\u7528\u4e86\u51e0\u4e2a\u4e0e\u81ea\u5b9a\u4e49\u4ee3\u7801\u4ea4\u4e92\u65f6\u4f1a\u4f7f\u7528\u7684\u5143\u7d20\uff1a"}),"\n",(0,r.jsxs)(n.p,{children:["\u6807\u8bc6\u5143\u7d20\uff08\u4f8b\u5982 ",(0,r.jsx)(n.code,{children:"identity name=frame_ready_webcam silent=false"}),")\u3002\u53ef\u7528\u4e8e\u8c03\u8bd5\u7ba1\u9053\u4e2d\u7684\u65f6\u5e8f\u3002\u5b83\u4eec\u89e6\u53d1\u65f6\u7684\u65f6\u95f4\u6233\u88ab\u4fdd\u5b58\uff0c\u7136\u540e\u5728\u7ba1\u9053\u672b\u5c3e\u7684\u201cmarks\u201d\u5143\u7d20\u4e2d\u4ee5\u952e\u503c\u5bf9\u5f62\u5f0f\u8fd4\u56de\uff08\u952e\u8868\u793a\u5143\u7d20\u540d\u79f0\uff0c\u503c\u662f\u65f6\u95f4\u6233\uff09\u3002"]}),"\n",(0,r.jsxs)(n.p,{children:["Appsink \u5143\u7d20\uff08\u4f8b\u5982 ",(0,r.jsx)(n.code,{children:"appsink name=frame"}),"\uff09\u3002\u7528\u4e8e\u5c06\u6570\u636e\u4ece GStreamer \u7ba1\u9053\u53d1\u9001\u5230\u4f60\u7684\u5e94\u7528\u7a0b\u5e8f\u3002\u8fd9\u91cc appsink \u4e4b\u524d\u7684\u5143\u7d20\u662f ",(0,r.jsx)(n.code,{children:"video/x-raw,format=RGB,width=224,height=224"}),"\uff0c\u56e0\u6b64\u5c06\u5411 Python \u53d1\u9001\u4e00\u4e2a 224x224 RGB \u6570\u7ec4\u3002\u8fd9\u4e9b\u6570\u636e\u901a\u8fc7 ",(0,r.jsx)(n.code,{children:"frames_by_sink"})," \u5143\u7d20\u4ee5\u952e\u503c\u5bf9\u5f62\u5f0f\u63a5\u6536\uff08\u952e\u4e3aappsink\u540d\u79f0\uff0c\u503c\u4e3a\u5b9e\u9645\u6570\u636e\uff09\u3002"]}),"\n",(0,r.jsxs)(n.p,{children:["\u6bcf\u4e2a\u7ba1\u9053\u53ef\u4ee5\u6709\u591a\u4e2a appsink\u3002\u6bd4\u5982\u4f60\u8fd8\u60f3\u83b7\u53d6\u539f\u59cb\u7684 1920x1080 \u56fe\u50cf\uff0c\u90a3\u5c31\u53ef\u4ee5\u5728 ",(0,r.jsx)(n.code,{children:"identity name=frame_ready_webcam"})," \u4e4b\u540e\u5c06\u7ba1\u9053\u5206\u6210\u4e24\u90e8\u5206\uff0c\u5e76\u5c06\u4e00\u90e8\u5206\u53d1\u9001\u5230\u65b0\u7684 appsink\uff0c\u53e6\u4e00\u90e8\u5206\u901a\u8fc7\u5c3a\u5bf8\u8c03\u6574/\u753b\u9762\u88c1\u526a\u7ba1\u9053\u53d1\u9001\u3002"]}),"\n",(0,r.jsxs)(n.p,{children:["1\ufe0f\u20e3 \u521b\u5efa\u4e00\u4e2a\u65b0\u7684 ",(0,r.jsx)(n.code,{children:"ex2.py"})," \u6587\u4ef6\u5e76\u5199\u5165\u5982\u4e0b\u4ee3\u7801\uff1a"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from gst_helper import gst_grouped_frames, atomic_save_image, timing_marks_to_str\nimport time, argparse\n\nparser = argparse.ArgumentParser(description=`GStreamer -> Python RGB frames`)\nparser.add_argument(`--video-source`, type=str, required=True, help=`GStreamer video source (e.g. "v4l2src device=/dev/video2" or "qtiqmmfsrc name=camsrc camera=0")`)\nargs, unknown = parser.parse_known_args()\n\nPIPELINE = (\n    # Video source\n    f"{args.video_source} ! "\n    # Properties for the video source\n    "video/x-raw,width=1920,height=1080 ! "\n    # An identity element so we can track when a new frame is ready (so we can calc. processing time)\n    "identity name=frame_ready_webcam silent=false ! "\n\n    # Split the stream\n    "tee name=t "\n\n    # Branch A) convert to RGB and send to original appsink\n        "t. ! queue max-size-buffers=1 leaky=downstream ! "\n        "qtivtransform ! video/x-raw,format=RGB ! "\n        "appsink name=original drop=true sync=false max-buffers=1 emit-signals=true "\n\n    # Branch B) resize/crop to 224x224 -> send to another appsink\n        "t. ! queue max-size-buffers=1 leaky=downstream ! "\n        # Crop (square), the crop syntax is (\'<X, Y, WIDTH, HEIGHT >\').\n        # So here we use 1920x1080 input, then center crop to 1080x1080 ((1920-1080)/2 = 420 = x crop)\n        f\'qtivtransform crop="<420, 0, 1080, 1080>" ! \'\n        # then resize to 224x224\n        "video/x-raw,format=RGB,width=224,height=224 ! "\n        # Event when the crop/scale are done\n        "identity name=transform_done silent=false ! "\n        # Send out the resulting frame to an appsink (where we can pick it up from Python)\n        "queue max-size-buffers=2 leaky=downstream ! "\n        "appsink name=frame drop=true sync=false max-buffers=1 emit-signals=true "\n)\n\nfor frames_by_sink, marks in gst_grouped_frames(PIPELINE):\n    print(f"Frame ready")\n    print(\'    Data:\', end=\'\')\n    for key in list(frames_by_sink):\n        print(f\' name={key} {frames_by_sink[key].shape}\', end=\'\')\n    print(\'\')\n    print(\'    Timings:\', timing_marks_to_str(marks))\n\n    # Save image to disk\n    frame = frames_by_sink[\'frame\']\n    atomic_save_image(frame=frame, path=\'out/imsdk.png\')\n    original = frames_by_sink[\'original\']\n    atomic_save_image(frame=original, path=\'out/imsdk_original.png\')\n'})}),"\n",(0,r.jsx)(n.p,{children:"2\ufe0f\u20e3 \u8fd0\u884c"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-Python",children:'python3 ex2.py --video-source "$IMSDK_VIDEO_SOURCE"\n\n# Frame ready\n#     Data: name=frame (224, 224, 3) name=original (1080, 1920, 3)\n#     Timings: frame_ready_webcam\u2192transform_done: 1.79ms, transform_done\u2192pipeline_finished: 4.75ms (total 6.54ms)\n# Frame ready\n#     Data: name=frame (224, 224, 3) name=original (1080, 1920, 3)\n#     Timings: frame_ready_webcam\u2192transform_done: 3.63ms, transform_done\u2192pipeline_finished: 3.59ms (total 7.22ms)\n'})}),"\n",(0,r.jsxs)(n.p,{children:["\uff08",(0,r.jsx)(n.code,{children:"out/"})," \u76ee\u5f55\u5305\u542b\u539f\u59cb\u5206\u8fa8\u7387\u548c\u8c03\u6574\u5927\u5c0f\u540e\u7684\u6700\u540e\u5904\u7406\u7684\u5e27\uff09"]}),"\n",(0,r.jsx)(n.p,{children:"\u597d\u7684\uff0c\u73b0\u5728\u5df2\u6210\u529f\u901a\u8fc7\u5355\u6761\u6d41\u6c34\u7ebf\u5b9e\u73b0\u53cc\u8def\u8f93\u51fa\uff0c\u53ef\u4ee5\u5728\u5355\u4e00\u7ba1\u9053\u4e2d\u6784\u5efa\u66f4\u590d\u6742\u7684\u5e94\u7528\u7a0b\u5e8f\u3002"}),"\n",(0,r.jsx)(n.h3,{id:"\u793a\u4f8b-3\u8fd0\u884c\u795e\u7ecf\u7f51\u7edc",children:"\u793a\u4f8b 3\uff1a\u8fd0\u884c\u795e\u7ecf\u7f51\u7edc"}),"\n",(0,r.jsx)(n.p,{children:"\u73b0\u5728\u5df2\u7ecf\u80fd\u591f\u4ece\u7f51\u7edc\u6444\u50cf\u5934\u83b7\u53d6\u6b63\u786e\u5206\u8fa8\u7387\u7684\u56fe\u50cf\u6d41\uff0c\u63a5\u4e0b\u6765\u6dfb\u52a0\u795e\u7ecf\u7f51\u7edc\u3002"}),"\n",(0,r.jsx)(n.h4,{id:"31python-\u4e2d\u7684\u795e\u7ecf\u7f51\u7edc\u4e0e\u5408\u6210",children:"3.1\uff1aPython \u4e2d\u7684\u795e\u7ecf\u7f51\u7edc\u4e0e\u5408\u6210"}),"\n",(0,r.jsxs)(n.p,{children:["1\ufe0f\u20e3 \u9996\u5148\u6211\u4eec\u8981\u505a\u4e00\u4e2a ",(0,r.jsx)(n.code,{children:"normal"})," \u5b9e\u73b0\uff0c\u4ece IM SDK \u7ba1\u9053\u4e2d\u83b7\u53d6\u5df2\u8c03\u6574\u5927\u5c0f\u7684\u5e27\uff0c\u7136\u540e\u4f7f\u7528 ",(0,r.jsx)(n.a,{href:"https://qc-ai-test.gitbook.io/qc-ai-test-docs/running-building-ai-models/lite-rt",children:"LiteRT"})," \u5728 NPU \u4e0a\u8fd0\u884c\u6a21\u578b\u3002\u4e4b\u540e\uff0c\u6211\u4eec\u5c06\u5bf9\u56fe\u50cf\u5f97\u51fa\u6700\u7ec8\u7ed3\u8bba\u5e76\u5c06\u5176\u5199\u5165\u78c1\u76d8\u3002\u521b\u5efa\u4e00\u4e2a\u65b0\u6587\u4ef6 ",(0,r.jsx)(n.code,{children:"ex3_from_python.py"})," \u5e76\u5199\u5165\u4ee5\u4e0b\u4ee3\u7801\uff1a"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"from gst_helper import gst_grouped_frames, atomic_save_pillow_image, timing_marks_to_str, download_file_if_needed, softmax\nimport time, argparse, numpy as np\nfrom ai_edge_litert.interpreter import Interpreter, load_delegate\nfrom PIL import ImageDraw, Image\n\nparser = argparse.ArgumentParser(description='GStreamer -> SqueezeNet')\nparser.add_argument('--video-source', type=str, required=True, help='GStreamer video source (e.g. \"v4l2src device=/dev/video2\" or \"qtiqmmfsrc name=camsrc camera=0\")')\nargs, unknown = parser.parse_known_args()\n\nMODEL_PATH = download_file_if_needed('models/squeezenet1_1-squeezenet-1.1-w8a8.tflite', 'https://cdn.edgeimpulse.com/qc-ai-docs/models/squeezenet1_1-squeezenet-1.1-w8a8.tflite')\nLABELS_PATH = download_file_if_needed('models/SqueezeNet-1.1_labels.txt', 'https://cdn.edgeimpulse.com/qc-ai-docs/models/SqueezeNet-1.1_labels.txt')\n\n# Parse labels\nwith open(LABELS_PATH, 'r') as f:\n    labels = [line for line in f.read().splitlines() if line.strip()]\n\n# Load TFLite model and allocate tensors, note: this is a 224x224 model with uint8 input!\n# If your models are different, then you'll need to update the pipeline below.\ninterpreter = Interpreter(\n    model_path=MODEL_PATH,\n    experimental_delegates=[load_delegate(\"libQnnTFLiteDelegate.so\", options={\"backend_type\": \"htp\"})]     # Use NPU\n)\ninterpreter.allocate_tensors()\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\nPIPELINE = (\n    # Video source\n    f\"{args.video_source} ! \"\n    # Properties for the video source\n    \"video/x-raw,width=1920,height=1080 ! \"\n    # An identity element so we can track when a new frame is ready (so we can calc. processing time)\n    \"identity name=frame_ready_webcam silent=false ! \"\n    # Crop (square), the crop syntax is ('<X, Y, WIDTH, HEIGHT >').\n    # So here we use 1920x1080 input, then center crop to 1080x1080 ((1920-1080)/2 = 420 = x crop)\n    f'qtivtransform crop=\"<420, 0, 1080, 1080>\" ! '\n    # then resize to 224x224\n    \"video/x-raw,format=RGB,width=224,height=224 ! \"\n    # Event when the crop/scale are done\n    \"identity name=transform_done silent=false ! \"\n    # Send out the resulting frame to an appsink (where we can pick it up from Python)\n    \"queue max-size-buffers=2 leaky=downstream ! \"\n    \"appsink name=frame drop=true sync=false max-buffers=1 emit-signals=true \"\n)\n\nfor frames_by_sink, marks in gst_grouped_frames(PIPELINE):\n    print(f\"Frame ready\")\n    print('    Data:', end='')\n    for key in list(frames_by_sink):\n        print(f' name={key} {frames_by_sink[key].shape}', end='')\n    print('')\n\n    # Begin inference timer\n    inference_start = time.perf_counter()\n\n    # Set tensor with the image received in \"frames_by_sink['frame']\", add batch dim, and run inference\n    interpreter.set_tensor(input_details[0]['index'], frames_by_sink['frame'].reshape((1, 224, 224, 3)))\n    interpreter.invoke()\n\n    # Get prediction (dequantized)\n    q_output = interpreter.get_tensor(output_details[0]['index'])\n    scale, zero_point = output_details[0]['quantization']\n    f_output = (q_output.astype(np.float32) - zero_point) * scale\n\n    # Image classification models in AI Hub miss a Softmax() layer at the end of the model, so add it manually\n    scores = softmax(f_output[0])\n\n    # End inference timer\n    inference_end = time.perf_counter()\n\n    # Add an extra mark, so we have timing info for the complete pipeline\n    marks['inference_done'] = list(marks.items())[-1][1] + (inference_end - inference_start)\n\n    # Print top-5 predictions\n    top_k = scores.argsort()[-5:][::-1]\n    print(f\"    Top-5 predictions:\")\n    for i in top_k:\n        print(f\"        Class {labels[i]}: score={scores[i]}\")\n\n    # Image composition timer\n    image_composition_start = time.perf_counter()\n\n    # Add the top 5 scores to the image, and save image to disk (for debug purposes)\n    frame = frames_by_sink['frame']\n    img = Image.fromarray(frame)\n    img_draw = ImageDraw.Draw(img)\n    img_draw.text((10, 10), f\"{labels[top_k[0]]} ({scores[top_k[0]]:.2f})\", fill=\"black\")\n    atomic_save_pillow_image(img=img, path='out/imsdk_with_prediction.png')\n\n    image_composition_end = time.perf_counter()\n\n    # Add an extra mark, so we have timing info for the complete pipeline\n    marks['image_composition_end'] = list(marks.items())[-1][1] + (image_composition_end - image_composition_start)\n\n    print('    Timings:', timing_marks_to_str(marks))\n\n"})}),"\n",(0,r.jsx)(n.p,{children:"2\ufe0f\u20e3\u73b0\u5728\u8fd0\u884c\u6b64\u5e94\u7528\u7a0b\u5e8f\uff1a"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# We use \'| grep -v "<W>"\' to filter out some warnings - you can omit it if you want.\npython3 ex3_from_python.py --video-source "$IMSDK_VIDEO_SOURCE" | grep -v "<W>"\n\n# Frame ready\n# Data: name=frame (224, 224, 3)\n# Top-5 predictions:\n#     Class grand piano: score=0.236373171210289\n#     Class studio couch: score=0.06304315477609634\n#     Class dining table: score=0.04321642965078354\n#     Class umbrella: score=0.04321642965078354\n#     Class quilt: score=0.035781171172857285\n# Timings: frame_ready_webcam\u2192transform_done: 2.59ms, transform_done\u2192pipeline_finished: 1.52ms, pipeline_finished\u2192inference_done: 1.14ms, inference_done\u2192image_composition_end: 24.84ms (total 30.09ms)\n'})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.img,{alt:"\u5e26\u53e0\u52a0\u5c42\u7684\u56fe\u50cf\u5206\u7c7b\u6a21\u578b",src:t(2263).A+"",width:"408",height:"414"})}),"\n",(0,r.jsx)(n.p,{children:"\u6548\u679c\u4e0d\u9519\u3002\u518d\u770b\u770b\u662f\u5426\u53ef\u4ee5\u505a\u5f97\u66f4\u597d\u3002"}),"\n",(0,r.jsx)(n.h4,{id:"32\u4f7f\u7528-im-sdk-\u8fd0\u884c\u795e\u7ecf\u7f51\u7edc",children:"3.2\uff1a\u4f7f\u7528 IM SDK \u8fd0\u884c\u795e\u7ecf\u7f51\u7edc"}),"\n",(0,r.jsxs)(n.p,{children:["\u5c06\u795e\u7ecf\u7f51\u7edc\u63a8\u7406\u8f6c\u79fb\u5230 IM SDK\u3002\u53ef\u4ee5\u901a\u8fc7\u4e09\u4e2a\u63d2\u4ef6\u6765\u5b9e\u73b0\uff1a",(0,r.jsx)(n.br,{}),"\n",(0,r.jsx)(n.code,{children:"qtimlvconverter"})," - \u5c06\u5e27\u8f6c\u6362\u4e3a\u8f93\u5165\u5f20\u91cf\u3002"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.code,{children:"qtimltflite"})," - \u8fd0\u884c\u795e\u7ecf\u7f51\u7edc\uff08LiteRT \u683c\u5f0f\uff09\u3002\u5982\u679c\u901a\u8fc7 appsink \u53d1\u9001\u8fd9\u4e9b\u7ed3\u679c\uff0c\u5c06\u83b7\u5f97\u4e0e\u4e4b\u524d\u5b8c\u5168\u76f8\u540c\u7684\u5f20\u91cf\uff08\u53ea\u662f\u4e0d\u9700\u8981\u901a\u8fc7 CPU \u6765\u8c03\u7528\u63a8\u7406\u5f15\u64ce\uff09\u3002"]}),"\n",(0,r.jsxs)(n.p,{children:["\u50cf ",(0,r.jsx)(n.code,{children:"qtimlvclassification"})," \u8fd9\u6837\u7684\u5143\u7d20\u6765\u89e3\u91ca\u8f93\u51fa\u3002\u8fd9\u4e2a\u63d2\u4ef6\u4e13\u4e3a\u56fe\u50cf\u5206\u7c7b\u7528\u4f8b\uff08\u4f8b\u5982\u6211\u4eec\u4f7f\u7528\u7684 SqueezeNet \u6a21\u578b\uff09\u8bbe\u8ba1\uff0c\u8f93\u51fa\u5f62\u72b6\u4e3a (1, n)\u3002\u8fd9\u4e2a\u63d2\u4ef6\u53ef\u4ee5\u8f93\u51fa\u6587\u672c\uff08\u5305\u542b\u9884\u6d4b\u7ed3\u679c\uff09\uff0c\u6216\u8005\u8f93\u51fa\u8986\u76d6\u5c42\uff08\u7528\u4e8e\u7ed8\u5236\u5230\u539f\u59cb\u56fe\u50cf\u4e0a\uff09\u3002"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\u8be5\u5143\u7d20\u5177\u6709\u7279\u5b9a\u7684\u6807\u7b7e\u683c\u5f0f\uff08\u89c1\u4e0b\u6587\uff09\u3002",(0,r.jsx)(n.br,{}),"\n","1\ufe0f\u20e3 \u521b\u5efa\u4e00\u4e2a\u65b0\u7684 ",(0,r.jsx)(n.code,{children:"ex3_nn_imsdk.py"})," \u6587\u4ef6\u5e76\u5199\u5165\u5982\u4e0b\u4ee3\u7801\uff1a"]}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"from gst_helper import gst_grouped_frames, atomic_save_pillow_image, timing_marks_to_str, download_file_if_needed, softmax\nimport time, argparse, numpy as np\nfrom ai_edge_litert.interpreter import Interpreter, load_delegate\nfrom PIL import ImageDraw, Image\n\nparser = argparse.ArgumentParser(description='GStreamer -> SqueezeNet')\nparser.add_argument('--video-source', type=str, required=True, help='GStreamer video source (e.g. \"v4l2src device=/dev/video2\" or \"qtiqmmfsrc name=camsrc camera=0\")')\nargs, unknown = parser.parse_known_args()\n\nMODEL_PATH = download_file_if_needed('models/squeezenet1_1-squeezenet-1.1-w8a8.tflite', 'https://cdn.edgeimpulse.com/qc-ai-docs/models/squeezenet1_1-squeezenet-1.1-w8a8.tflite')\nLABELS_PATH = download_file_if_needed('models/SqueezeNet-1.1_labels.txt', 'https://cdn.edgeimpulse.com/qc-ai-docs/models/SqueezeNet-1.1_labels.txt')\n\n# Parse labels\nwith open(LABELS_PATH, 'r') as f:\n    labels = [line for line in f.read().splitlines() if line.strip()]\n\n# IM SDK expects labels in this format\n# (structure)\"white-shark,id=(guint)0x3,color=(guint)0x00FF00FF;\" (so no spaces in the name)\nIMSDK_LABELS_PATH = 'models/SqueezeNet-1.1_imsdk_labels.txt'\nwith open(IMSDK_LABELS_PATH, 'w') as f:\n    imsdk_labels_content = []\n    for i in range(0, len(labels)):\n        label = labels[i]\n        label = label.replace(' ', '-') # no space allowed\n        label = label.replace(\"'\", '') # no ' allowed\n        imsdk_labels_content.append(f'(structure)\"{label},id=(guint){hex(i)},color=(guint)0x00FF00FF;\"')\n    f.write('\\n'.join(imsdk_labels_content))\n\n# Load TFLite model and allocate tensors, note: this is a 224x224 model with uint8 input!\n# If your models are different, then you'll need to update the pipeline below.\ninterpreter = Interpreter(\n    model_path=MODEL_PATH,\n    experimental_delegates=[load_delegate(\"libQnnTFLiteDelegate.so\", options={\"backend_type\": \"htp\"})]     # Use NPU\n)\ninterpreter.allocate_tensors()\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\nscale, zero_point = output_details[0]['quantization']\n\nPIPELINE = (\n    # Video source\n    f\"{args.video_source} ! \"\n    # Properties for the video source\n    \"video/x-raw,width=1920,height=1080 ! \"\n    # An identity element so we can track when a new frame is ready (so we can calc. processing time)\n    \"identity name=frame_ready_webcam silent=false ! \"\n    # Crop (square), the crop syntax is ('<X, Y, WIDTH, HEIGHT >').\n    # So here we use 1920x1080 input, then center crop to 1080x1080 ((1920-1080)/2 = 420 = x crop)\n    f'qtivtransform crop=\"<420, 0, 1080, 1080>\" ! '\n    # then resize to 224x224, (!! NOTE: here you need to use format=NV12 to get a tightly packed buffer - if you use RGB this won't work !!)\n    \"video/x-raw,width=224,height=224,format=NV12 ! \"\n    # Event when the crop/scale are done\n    \"identity name=transform_done silent=false ! \"\n\n    # turn into right format (UINT8 data type) and add batch dimension\n    'qtimlvconverter ! neural-network/tensors,type=UINT8,dimensions=<<1,224,224,3>> ! '\n    # Event when conversion is done\n    \"identity name=conversion_done silent=false ! \"\n    # run inference (using the QNN delegates to run on NPU)\n    f'qtimltflite delegate=external external-delegate-path=libQnnTFLiteDelegate.so external-delegate-options=\"QNNExternalDelegate,backend_type=htp;\" model=\"{MODEL_PATH}\" ! '\n    # Event when inference is done\n    \"identity name=inference_done silent=false ! \"\n\n    # Run the classifier (add softmax, as AI Hub models miss it), this will return the top n labels (above threshold, min. threshold is 10)\n    # note that you also need to pass the quantization params (see below under the \"gst_grouped_frames\" call).\n    f'qtimlvclassification name=cls module=mobilenet extra-operation=softmax threshold=10 results=1 labels=\"{IMSDK_LABELS_PATH}\" ! '\n    \"identity name=classification_done silent=false ! \"\n\n    # The qtimlvclassification can either output a video/x-raw,format=BGRA,width=224,height=224 element (overlay),\n    # or a text/x-raw element (raw text) - here we want the text\n    \"text/x-raw,format=utf8 ! \"\n\n    # Send to application\n    \"queue max-size-buffers=2 leaky=downstream ! \"\n    'appsink name=qtimlvclassification_text drop=true sync=false max-buffers=1 emit-signals=true '\n)\n\nfor frames_by_sink, marks in gst_grouped_frames(PIPELINE, element_properties={\n    # the qtimlvclassification element does not like these variables passed in as a string in the pipeline, so set them like this\n    'cls': { 'constants': f'Mobilenet,q-offsets=<{zero_point}>,q-scales=<{scale}>' }\n}):\n    print(f\"Frame ready\")\n    print('    Data:', end='')\n    for key in list(frames_by_sink):\n        print(f' name={key} {frames_by_sink[key].shape} ({frames_by_sink[key].dtype})', end='')\n    print('')\n\n    # Grab the qtimlvclassification_text (utf8 text) with predictions from IM SDK\n    qtimlvclassification_text = frames_by_sink['qtimlvclassification_text'].tobytes().decode(\"utf-8\")\n    print('    qtimlvclassification_text:', qtimlvclassification_text)\n\n    print('    Timings:', timing_marks_to_str(marks))\n"})}),"\n",(0,r.jsx)(n.admonition,{type:"warning",children:(0,r.jsxs)(n.p,{children:["NV12: \u6211\u4eec\u5c06\u683c\u5f0f\u4ece ",(0,r.jsx)(n.code,{children:"RGB"})," \u5207\u6362\u4e3a ",(0,r.jsx)(n.code,{children:"NV12"})," (\u5728 ",(0,r.jsx)(n.code,{children:"qtivtransform"}),"\u4e4b\u540e)\uff0c\u56e0\u4e3a ",(0,r.jsx)(n.code,{children:"qtimltflite"})," \u8981\u6c42\u7f13\u51b2\u533a\u4e3a\u7d27\u51d1\u6392\u5217\uff0c\u800c RGB \u8f93\u51fa\u4f7f\u7528\u4e86\u884c\u6b65\u8fdb\u586b\u5145\u3002 \u8fd9\u7c7b\u95ee\u9898\u901a\u5e38\u6781\u96be\u8c03\u8bd5\u3002 \u5728\u547d\u4ee4\u524d\u6dfb\u52a0 ",(0,r.jsx)(n.code,{children:"GST_DEBUG=3"}),"\uff08 \u4f8b\u5982 ",(0,r.jsx)(n.code,{children:"GST_DEBUG=3 python3 ex3_nn_imsdk.py"}),") \u7136\u540e\u5c06\u7ba1\u9053\u7684\u8be6\u7ec6\u65e5\u5fd7\u4e0e\u62a5\u9519\u4fe1\u606f\u63d0\u4ea4\u7ed9\u50cf ChatGPT \u8fd9\u6837\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u6709\u65f6\u80fd\u83b7\u5f97\u5e2e\u52a9\u3002"]})}),"\n",(0,r.jsx)(n.p,{children:"2\ufe0f\u20e3 \u73b0\u5728\u8bf7\u8fd0\u884c\u6b64\u5e94\u7528\u7a0b\u5e8f\uff1a"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# We use \'| grep -v "<W>"\' to filter out some warnings - you can omit it if you want.\npython3 ex3_nn_imsdk.py --video-source "$IMSDK_VIDEO_SOURCE" | grep -v "<W>"\n\n# Frame ready\n#     Data: name=qtimlvclassification_text (337,) (uint8)\n#     qtimlvclassification_text: { (structure)"ImageClassification\\,\\ batch-index\\=\\(uint\\)0\\,\\ labels\\=\\(structure\\)\\<\\ \\"grand.piano\\\\\\,\\\\\\ id\\\\\\=\\\\\\(uint\\\\\\)256\\\\\\,\\\\\\ confidence\\\\\\=\\\\\\(double\\\\\\)52.870616912841797\\\\\\,\\\\\\ color\\\\\\=\\\\\\(uint\\\\\\)16711935\\\\\\;\\"\\ \\>\\,\\ timestamp\\=\\(guint64\\)1471167589\\,\\ sequence-index\\=\\(uint\\)1\\,\\ sequence-num-entries\\=\\(uint\\)1\\;" }\n#     Timings: frame_ready_webcam\u2192transform_done: 3.86ms, transform_done\u2192inference_done: 4.04ms, inference_done\u2192pipeline_finished: 0.74ms (total 8.65ms)\n'})}),"\n",(0,r.jsx)(n.p,{children:"\u597d\u7684\uff0c\u8be5\u6a21\u578b\u73b0\u5728\u5728 IM SDK \u7ba1\u9053\u5185\u7684 NPU \u4e0a\u8fd0\u884c\u3002\u5982\u679c\u4f60\u66f4\u60f3\u83b7\u5f97\u524d 5 \u4e2a\u8f93\u51fa\uff08\u5982 3.1\uff09\uff0c\u53ef\u4ee5\u5728qtimltflite\u5143\u7d20\u4e4b\u540e\u5206\u6d41\uff0c\u5e76\u5c06\u539f\u59cb\u8f93\u51fa\u5f20\u91cf\u53d1\u9001\u56de\u5e94\u7528\u7a0b\u5e8f\u3002"}),"\n",(0,r.jsx)(n.admonition,{type:"note",children:(0,r.jsxs)(n.p,{children:["\u53e0\u52a0\u56fe\u50cf\uff1a\u5982\u679c\u60f3\u67e5\u770b\u8986\u76d6\u56fe\u50cf\u800c\u975e\u6587\u672c\uff0c\u8bf7\u53c2\u8003 ",(0,r.jsx)(n.code,{children:"tutorial/_ex3_nn_imsdk_show_overlay.py"}),"\u3002"]})}),"\n",(0,r.jsx)(n.h4,{id:"33-\u53e0\u52a0\u5c42",children:"3.3: \u53e0\u52a0\u5c42"}),"\n",(0,r.jsxs)(n.p,{children:["\u4e3a\u4e86\u6548\u4eff 3.1 \u4e2d\u7684\u8f93\u51fa\uff0c\u6211\u4eec\u8fd8\u8981\u7ed8\u5236\u4e00\u4e2a\u53e0\u52a0\u5c42\u3002\u8ba9\u6211\u4eec\u9996\u5148\u7528\u9759\u6001\u53e0\u52a0\u56fe\u50cf\u6765\u6f14\u793a\u3002",(0,r.jsx)(n.br,{}),"\n","1\ufe0f\u20e3 \u4e0b\u8f7d\u534a\u900f\u660e\u56fe\u50cf ",(0,r.jsx)(n.a,{href:"https://commons.wikimedia.org/wiki/File:PNG_transparency_demonstration_2.png",children:"\u6e90"}),"\uff1a"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"mkdir -p images\nwget -O images/imsdk-transparent-static.png https://cdn.edgeimpulse.com/qc-ai-docs/example-images/imsdk-transparent-static.png\n"})}),"\n",(0,r.jsxs)(n.p,{children:["2\ufe0f\u20e3 \u521b\u5efa\u4e00\u4e2a\u65b0\u7684 ",(0,r.jsx)(n.code,{children:"ex3_overlay.py"})," \u6587\u4ef6\u5e76\u5199\u5165\u5982\u4e0b\u4ee3\u7801\uff1a"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from gst_helper import gst_grouped_frames, atomic_save_image, timing_marks_to_str, download_file_if_needed, softmax\nimport time, argparse, numpy as np\nfrom ai_edge_litert.interpreter import Interpreter, load_delegate\nfrom PIL import ImageDraw, Image\n\nparser = argparse.ArgumentParser(description=\'GStreamer -> SqueezeNet\')\nparser.add_argument(\'--video-source\', type=str, required=True, help=\'GStreamer video source (e.g. "v4l2src device=/dev/video2" or "qtiqmmfsrc name=camsrc camera=0")\')\nargs, unknown = parser.parse_known_args()\n\nif args.video_source.strip() == \'\':\n    raise Exception(\'--video-source is empty, did you not set the IMSDK_VIDEO_SOURCE env variable? E.g.:\\n\' +\n    \'    export IMSDK_VIDEO_SOURCE="v4l2src device=/dev/video2"\')\n\n# Source: https://commons.wikimedia.org/wiki/File:Arrow_png_image.png\nOVERLAY_IMAGE = download_file_if_needed(\'images/imsdk-transparent-static.png\', \'https://cdn.edgeimpulse.com/qc-ai-docs/example-images/imsdk-transparent-static.png\')\nOVERLAY_WIDTH = 128\nOVERLAY_HEIGHT = 96\n\nPIPELINE = (\n    # Part 1: Create a qtivcomposer with two sinks (we\'ll write webcam to sink 0, overlay to sink 1)\n    "qtivcomposer name=comp sink_0::zorder=0 "\n        # Sink 1 (the overlay) will be at x=10, y=10; and sized 128x96\n        f"sink_1::zorder=1 sink_1::alpha=1.0 sink_1::position=<10,10> sink_1::dimensions=<{OVERLAY_WIDTH},{OVERLAY_HEIGHT}> ! "\n    "videoconvert ! "\n    "video/x-raw,format=RGBA,width=224,height=224 ! "\n    # Write frames to appsink\n    "appsink name=overlay_raw drop=true sync=false max-buffers=1 emit-signals=true "\n\n    # Part 2: Grab image from webcam and write the composer\n        # Video source\n        f"{args.video_source} ! "\n        # Properties for the video source\n        "video/x-raw,width=1920,height=1080 ! "\n        # An identity element so we can track when a new frame is ready (so we can calc. processing time)\n        "identity name=frame_ready_webcam silent=false ! "\n        # Crop (square), the crop syntax is (\'<X, Y, WIDTH, HEIGHT >\').\n        # So here we use 1920x1080 input, then center crop to 1080x1080 ((1920-1080)/2 = 420 = x crop)\n        f\'qtivtransform crop="<420, 0, 1080, 1080>" ! \'\n        # then resize to 224x224\n        "video/x-raw,width=224,height=224,format=NV12 ! "\n        # Event when the crop/scale are done\n        "identity name=transform_done silent=false ! "\n        # Write to sink 0 on the composer\n        "comp.sink_0 "\n\n    # Part 3: Load overlay from disk and write to composer (sink 1)\n        # Image (statically from disk)\n        f\'filesrc location="{OVERLAY_IMAGE}" ! \'\n        # Decode PNG\n        "pngdec ! "\n        # Turn into a video (scaled to 128x96, RGBA format so we keep transparency, requires a framerate)\n        "imagefreeze ! "\n        "videoscale ! "\n        "videoconvert ! "\n        f"video/x-raw,format=RGBA,width={OVERLAY_WIDTH},height={OVERLAY_HEIGHT},framerate=30/1 ! "\n        # Write to sink 1 on the composer\n        "comp.sink_1 "\n)\n\nfor frames_by_sink, marks in gst_grouped_frames(PIPELINE):\n    print(f"Frame ready")\n    print(\'    Data:\', end=\'\')\n    for key in list(frames_by_sink):\n        print(f\' name={key} {frames_by_sink[key].shape} ({frames_by_sink[key].dtype})\', end=\'\')\n    print(\'\')\n\n    # Save image to disk\n    save_image_start = time.perf_counter()\n    frame = frames_by_sink[\'overlay_raw\']\n    atomic_save_image(frame=frame, path=\'out/webcam_with_overlay.png\')\n    save_image_end = time.perf_counter()\n\n    # Add an extra mark, so we have timing info for the complete pipeline\n    marks[\'save_image_end\'] = list(marks.items())[-1][1] + (save_image_end - save_image_start)\n\n    print(\'    Timings:\', timing_marks_to_str(marks))\n'})}),"\n",(0,r.jsx)(n.p,{children:"3\ufe0f\u20e3 \u8fd0\u884c\u6b64\u5e94\u7528\u7a0b\u5e8f\uff1a"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# We use \'| grep -v "<W>"\' to filter out some warnings - you can omit it if you want.\npython3 ex3_overlay.py --video-source "$IMSDK_VIDEO_SOURCE" | grep -v "<W>"\n\n# Frame ready\n#     Data: name=overlay_raw (224, 224, 4) (uint8)\n#     Timings: frame_ready_webcam\u2192transform_done: 2.22ms, transform_done\u2192pipeline_finished: 5.17ms, pipeline_finished\u2192save_image_end: 21.51ms (total 28.89ms)\n'})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.img,{alt:"\u9759\u6001\u53e0\u52a0\u5230\u7f51\u7edc\u6444\u50cf\u5934\u56fe\u50cf\u4e0a",src:t(7180).A+"",width:"408",height:"409"})}),"\n",(0,r.jsx)(n.h4,{id:"34\u5c06\u795e\u7ecf\u7f51\u7edc\u4e0e\u53e0\u52a0\u5c42\u76f8\u7ed3\u5408",children:"3.4\uff1a\u5c06\u795e\u7ecf\u7f51\u7edc\u4e0e\u53e0\u52a0\u5c42\u76f8\u7ed3\u5408"}),"\n",(0,r.jsx)(n.p,{children:"\u73b0\u5728\u4f60\u5df2\u7ecf\u4e86\u89e3\u4e86\u5982\u4f55\u5c06\u795e\u7ecf\u7f51\u7edc\u4f5c\u4e3a IM SDK \u7ba1\u9053\u7684\u4e00\u90e8\u5206\u8fd0\u884c\uff0c\u5e76\u4e14\u5df2\u7ecf\u4e86\u89e3\u5982\u4f55\u7ed8\u5236\u53e0\u52a0\u56fe\u3002\u4e0b\u9762\u5c06\u5b83\u4eec\u7ec4\u5408\u6210\u4e00\u4e2a\u7ba1\u9053\uff0c\u5c06\u9884\u6d4b\u53e0\u52a0\u5230\u56fe\u50cf\u4e0a\u3002\u6240\u6709\u64cd\u4f5c\u4e0d\u6d89\u53ca CPU\u3002"}),"\n",(0,r.jsxs)(n.p,{children:["1\ufe0f\u20e3 \u521b\u5efa\u4e00\u4e2a\u65b0\u7684 ",(0,r.jsx)(n.code,{children:"ex3_from_imsdk.py"})," \u6587\u4ef6\u5e76\u5199\u5165\u5982\u4e0b\u4ee3\u7801\uff1a"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'\nfrom gst_helper import gst_grouped_frames, atomic_save_numpy_buffer, timing_marks_to_str, download_file_if_needed, softmax\nimport time, argparse, numpy as np\nfrom ai_edge_litert.interpreter import Interpreter, load_delegate\nfrom PIL import ImageDraw, Image\n\nparser = argparse.ArgumentParser(description=\'GStreamer -> SqueezeNet\')\nparser.add_argument(\'--video-source\', type=str, required=True, help=\'GStreamer video source (e.g. "v4l2src device=/dev/video2" or "qtiqmmfsrc name=camsrc camera=0")\')\nargs, unknown = parser.parse_known_args()\n\nif args.video_source.strip() == \'\':\n    raise Exception(\'--video-source is empty, did you not set the IMSDK_VIDEO_SOURCE env variable? E.g.:\\n\' +\n    \'    export IMSDK_VIDEO_SOURCE="v4l2src device=/dev/video2"\')\n\nMODEL_PATH = download_file_if_needed(\'models/squeezenet1_1-squeezenet-1.1-w8a8.tflite\', \'https://cdn.edgeimpulse.com/qc-ai-docs/models/squeezenet1_1-squeezenet-1.1-w8a8.tflite\')\nLABELS_PATH = download_file_if_needed(\'models/SqueezeNet-1.1_labels.txt\', \'https://cdn.edgeimpulse.com/qc-ai-docs/models/SqueezeNet-1.1_labels.txt\')\n\n# Parse labels\nwith open(LABELS_PATH, \'r\') as f:\n    labels = [line for line in f.read().splitlines() if line.strip()]\n\n# IM SDK expects labels in this format\n# (structure)"white-shark,id=(guint)0x3,color=(guint)0x00FF00FF;" (so no spaces in the name)\nIMSDK_LABELS_PATH = \'models/SqueezeNet-1.1_imsdk_labels.txt\'\nwith open(IMSDK_LABELS_PATH, \'w\') as f:\n    imsdk_labels_content = []\n    for i in range(0, len(labels)):\n        label = labels[i]\n        label = label.replace(\' \', \'-\') # no space allowed\n        label = label.replace("\'", \'\') # no \' allowed\n        imsdk_labels_content.append(f\'(structure)"{label},id=(guint){hex(i)},color=(guint)0x00FF00FF;"\')\n    f.write(\'\\n\'.join(imsdk_labels_content))\n\n# Load TFLite model and allocate tensors, note: this is a 224x224 model with uint8 input!\n# If your models are different, then you\'ll need to update the pipeline below.\ninterpreter = Interpreter(\n    model_path=MODEL_PATH,\n    experimental_delegates=[load_delegate("libQnnTFLiteDelegate.so", options={"backend_type": "htp"})]     # Use NPU\n)\ninterpreter.allocate_tensors()\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\nscale, zero_point = output_details[0][\'quantization\']\n\nPIPELINE = (\n    # Part 1: Create a qtivcomposer with two sinks (we\'ll write webcam to sink 0, overlay to sink 1)\n    "qtivcomposer name=comp sink_0::zorder=0 "\n        # Sink 1 (the overlay). We don\'t need to pass in a position/size as the overlay will already be the right size.\n        f"sink_1::zorder=1 sink_1::alpha=1.0 ! "\n    "videoconvert ! "\n    "video/x-raw,format=RGBA,width=224,height=224 ! "\n    # Convert to PNG\n    "identity name=pngenc_begin silent=false ! "\n    "pngenc ! "\n    "identity name=pngenc_done silent=false ! "\n    # Write frames to appsink\n    "appsink name=image_with_overlay drop=true sync=false max-buffers=1 emit-signals=true "\n\n    # Video source\n    f"{args.video_source} ! "\n    # Properties for the video source\n    "video/x-raw,width=1920,height=1080 ! "\n    # An identity element so we can track when a new frame is ready (so we can calc. processing time)\n    "identity name=frame_ready_webcam silent=false ! "\n    # Crop (square), the crop syntax is (\'<X, Y, WIDTH, HEIGHT >\').\n    # So here we use 1920x1080 input, then center crop to 1080x1080 ((1920-1080)/2 = 420 = x crop)\n    f\'qtivtransform crop="<420, 0, 1080, 1080>" ! \'\n    # then resize to 224x224, (!! NOTE: here you need to use format=NV12 to get a tightly packed buffer - if you use RGB this won\'t work !!)\n    "video/x-raw,width=224,height=224,format=NV12 ! "\n    # Event when the crop/scale are done\n    "identity name=transform_done silent=false ! "\n\n    # Tee the stream\n    "tee name=v "\n\n    # Branch A) send the image to the composer (sink 0)\n        "v. ! queue max-size-buffers=1 leaky=downstream ! "\n        "comp.sink_0 "\n\n    # Branch B) run inference over the image\n        "v. ! queue max-size-buffers=1 leaky=downstream ! "\n        # turn into right format (UINT8 data type) and add batch dimension\n        \'qtimlvconverter ! neural-network/tensors,type=UINT8,dimensions=<<1,224,224,3>> ! \'\n        # run inference (using the QNN delegates to run on NPU)\n        f\'qtimltflite delegate=external external-delegate-path=libQnnTFLiteDelegate.so external-delegate-options="QNNExternalDelegate,backend_type=htp;" model="{MODEL_PATH}" ! \'\n\n        # Split the stream\n        "tee name=t "\n\n        # Branch B1) send raw results to the appsink (note that these are still quantized!)\n            "t. ! queue max-size-buffers=1 leaky=downstream ! "\n            "queue max-size-buffers=2 leaky=downstream ! "\n            "appsink name=qtimltflite_output drop=true sync=false max-buffers=1 emit-signals=true "\n\n        # Branch B2) parse the output tensor in IM SDK\n            "t. ! queue max-size-buffers=1 leaky=downstream ! "\n            # Run the classifier (add softmax, as AI Hub models miss it), this will return the top n labels (above threshold, min. threshold is 10)\n            # note that you also need to pass the quantization params (see below under the "gst_grouped_frames" call).\n            f\'qtimlvclassification name=cls module=mobilenet extra-operation=softmax threshold=10 results=1 labels="{IMSDK_LABELS_PATH}" ! \'\n            # Event when inference is done\n            "identity name=inference_done silent=false ! "\n\n            # create an RGBA overlay\n            "video/x-raw,format=BGRA,width=224,height=224 ! "\n\n            # And send to the composer\n            "comp.sink_1 "\n)\n\nfor frames_by_sink, marks in gst_grouped_frames(PIPELINE, element_properties={\n    # the qtimlvclassification element does not like these variables passed in as a string in the pipeline, so set them like this\n    \'cls\': { \'constants\': f\'Mobilenet,q-offsets=<{zero_point}>,q-scales=<{scale}>\' }\n}):\n    print(f"Frame ready")\n    print(\'    Data:\', end=\'\')\n    for key in list(frames_by_sink):\n        print(f\' name={key} {frames_by_sink[key].shape} ({frames_by_sink[key].dtype})\', end=\'\')\n    print(\'\')\n\n    # Get prediction (these come in quantized, so dequantize first)\n    q_output = frames_by_sink[\'qtimltflite_output\']\n    f_output = (q_output.astype(np.float32) - zero_point) * scale\n\n    # Image classification models in AI Hub miss a Softmax() layer at the end of the model, so add it manually\n    scores = softmax(f_output)\n    top_k = scores.argsort()[-5:][::-1]\n    print(f"    Top-5 predictions:")\n    for i in top_k:\n        print(f"        Class {labels[i]}: score={scores[i]}")\n\n    # Save image to disk\n    save_image_start = time.perf_counter()\n    png_file = frames_by_sink[\'image_with_overlay\']\n    atomic_save_numpy_buffer(png_file, path=\'out/webcam_with_overlay_imsdk.png\')\n    save_image_end = time.perf_counter()\n\n    # Add an extra mark, so we have timing info for the complete pipeline\n    marks[\'save_image_end\'] = list(marks.items())[-1][1] + (save_image_end - save_image_start)\n\n    print(\'    Timings:\', timing_marks_to_str(marks))\n'})}),"\n",(0,r.jsx)(n.p,{children:"2\ufe0f\u20e3 \u8fd0\u884c\u6b64\u5e94\u7528\u7a0b\u5e8f\uff1a"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# We use \'| grep -v "<W>"\' to filter out some warnings - you can omit it if you want.\npython3 ex3_from_imsdk.py --video-source "$IMSDK_VIDEO_SOURCE" | grep -v "<W>"\n\n# Frame ready\n#     Data: name=image_with_overlay (49550,) (uint8) name=qtimltflite_output (1000,) (uint8)\n#     Top-5 predictions:\n#         Class grand piano: score=0.2539741098880768\n#         Class spotlight: score=0.056083470582962036\n#         Class punching bag: score=0.03183111920952797\n#         Class accordion: score=0.03183111920952797\n#         Class projector: score=0.0218204278498888\n#   Timings: frame_ready_webcam\u2192transform_done: 1.69ms, transform_done\u2192inference_done: 6.93ms, inference_done\u2192pngenc_begin: 1.50ms, pngenc_begin\u2192pngenc_done: 15.96ms, pngenc_done\u2192pipeline_finished: 0.76ms, pipeline_finished\u2192save_image_end: 1.10ms (total 27.95ms)\n'})}),"\n",(0,r.jsxs)(n.p,{children:["\u5f88\u68d2\uff0c\u6574\u4e2a\u7ba1\u9053\u73b0\u5728\u5728 IM SDK \u4e2d\u8fd0\u884c\u3002\u8f93\u51fa\u56fe\u50cf\u5728 ",(0,r.jsx)(n.em,{children:"out/webcam_with_overlay_imsdk.png"}),"\uff1a"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.img,{alt:"IM SDK \u6e32\u67d3\u7684\u5e26\u6709\u53e0\u52a0\u5c42\u7684\u56fe\u50cf\u5206\u7c7b\u6a21\u578b",src:t(1045).A+"",width:"417",height:"409"})}),"\n",(0,r.jsx)(n.h2,{id:"\u6545\u969c\u6392\u9664",children:"\u6545\u969c\u6392\u9664"}),"\n",(0,r.jsx)(n.h4,{id:"\u7ba1\u9053\u65e0\u8f93\u51fa",children:"\u7ba1\u9053\u65e0\u8f93\u51fa"}),"\n",(0,r.jsxs)(n.p,{children:["\u5982\u679c\u7ba1\u9053\u6ca1\u6709\u4efb\u4f55\u8f93\u51fa\uff0c\u8bf7\u6dfb\u52a0 ",(0,r.jsx)(n.code,{children:"GST_DEBUG=3"})," \u67e5\u770b\u8c03\u8bd5\u8be6\u60c5\u3002"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"GST_DEBUG=3 python3 ex1.py\n"})}),"\n",(0,r.jsx)(n.h4,{id:"qmmf-recorder-startcamera-failed--\u65e0\u6cd5\u6253\u5f00\u6444\u50cf\u5934",children:"QMMF Recorder StartCamera Failed / \u65e0\u6cd5\u6253\u5f00\u6444\u50cf\u5934"}),"\n",(0,r.jsx)(n.p,{children:"\u82e5\u5728\u4f7f\u7528 RB3 Gen 2 Vision Kit \u5185\u7f6e\u6444\u50cf\u5934\u65f6\u770b\u5230\u6b64\u7c7b\u62a5\u9519\uff1a"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"0:00:00.058915726  7329     0x1faf28a0 ERROR             qtiqmmfsrc qmmf_source_context.cc:1426:gst_qmmf_context_open: QMMF Recorder StartCamera Failed!\n0:00:00.058955986  7329     0x1faf28a0 WARN              qtiqmmfsrc qmmf_source.c:1206:qmmfsrc_change_state:<camsrc> error: Failed to Open Camera!\n"})}),"\n",(0,r.jsx)(n.p,{children:"\u8fd0\u884c\uff1a"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"sudo killall cam-server\n"})})]})}function c(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(m,{...e})}):m(e)}},7180:(e,n,t)=>{t.d(n,{A:()=>i});const i=t.p+"assets/images/IMSDK_2-43c29ca5f49e28acd6c61bd8669a4809.png"},8453:(e,n,t)=>{t.d(n,{R:()=>s,x:()=>o});var i=t(6540);const r={},a=i.createContext(r);function s(e){const n=i.useContext(a);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:s(e.components),i.createElement(a.Provider,{value:n},e.children)}}}]);