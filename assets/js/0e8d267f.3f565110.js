"use strict";(globalThis.webpackChunkmy_website=globalThis.webpackChunkmy_website||[]).push([[3729],{6354:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>p,frontMatter:()=>s,metadata:()=>a,toc:()=>d});const a=JSON.parse('{"id":"Application Development and Execution Guide/Building AI Models/qualcomm_ai_hub","title":"Qualcomm\xae AI Hub","description":"Qualcomm AI Hub \u5305\u542b\u5927\u91cf\u9884\u8bad\u7ec3\u7684 AI \u6a21\u578b\uff0c\u8fd9\u4e9b\u6a21\u578b\u7ecf\u8fc7\u4f18\u5316\uff0c\u53ef\u5728 Dragonwing \u786c\u4ef6NPU\u4e0a\u8fd0\u884c\u3002","source":"@site/docs/7.Application Development and Execution Guide/1.Building AI Models/2.qualcomm_ai_hub.md","sourceDirName":"7.Application Development and Execution Guide/1.Building AI Models","slug":"/Application Development and Execution Guide/Building AI Models/qualcomm_ai_hub","permalink":"/ubuntu100-cn.github.io/docs/Application Development and Execution Guide/Building AI Models/qualcomm_ai_hub","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/7.Application Development and Execution Guide/1.Building AI Models/2.qualcomm_ai_hub.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Edge Impulse","permalink":"/ubuntu100-cn.github.io/docs/Application Development and Execution Guide/Building AI Models/edge_impulse"},"next":{"title":"LiteRT / TFLite","permalink":"/ubuntu100-cn.github.io/docs/Application Development and Execution Guide/Framework-Driven AI Sample Execution/litert_tflite"}}');var i=t(4848),o=t(8453);const s={},r="Qualcomm\xae AI Hub",l={},d=[{value:"\u67e5\u627e\u652f\u6301\u7684\u6a21\u578b",id:"\u67e5\u627e\u652f\u6301\u7684\u6a21\u578b",level:2},{value:"\u5c06\u6a21\u578b\u90e8\u7f72\u5230 NPU\uff08Python\uff09",id:"\u5c06\u6a21\u578b\u90e8\u7f72\u5230-npupython",level:2},{value:"\u8fd0\u884c\u793a\u4f8b\u4ee3\u7801\u5e93",id:"\u8fd0\u884c\u793a\u4f8b\u4ee3\u7801\u5e93",level:3},{value:"\u5c06\u6a21\u578b\u79fb\u690d\u5230NPU",id:"\u5c06\u6a21\u578b\u79fb\u690d\u5230npu",level:3},{value:"\u9884\u5904\u7406\u8f93\u5165",id:"\u9884\u5904\u7406\u8f93\u5165",level:4},{value:"\u540e\u5904\u7406\u8f93\u51fa",id:"\u540e\u5904\u7406\u8f93\u51fa",level:4},{value:"\u7aef\u5230\u7aef\u793a\u4f8b\uff08Python\uff09",id:"\u7aef\u5230\u7aef\u793a\u4f8bpython",level:3},{value:"\u5c06\u6a21\u578b\u90e8\u7f72\u5230 NPU\uff08Edge Impulse\uff09",id:"\u5c06\u6a21\u578b\u90e8\u7f72\u5230-npuedge-impulse",level:2}];function c(e){const n={a:"a",admonition:"admonition",br:"br",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",img:"img",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"qualcomm-ai-hub",children:"Qualcomm\xae AI Hub"})}),"\n",(0,i.jsxs)(n.p,{children:["Qualcomm ",(0,i.jsx)(n.a,{href:"https://aihub.qualcomm.com",children:"AI Hub"})," \u5305\u542b\u5927\u91cf\u9884\u8bad\u7ec3\u7684 AI \u6a21\u578b\uff0c\u8fd9\u4e9b\u6a21\u578b\u7ecf\u8fc7\u4f18\u5316\uff0c\u53ef\u5728 Dragonwing \u786c\u4ef6NPU\u4e0a\u8fd0\u884c\u3002"]}),"\n",(0,i.jsx)(n.h2,{id:"\u67e5\u627e\u652f\u6301\u7684\u6a21\u578b",children:"\u67e5\u627e\u652f\u6301\u7684\u6a21\u578b"}),"\n",(0,i.jsx)(n.p,{children:"AI Hub \u4e2d\u7684\u6a21\u578b\u6309\u652f\u6301\u7684 Qualcomm \u82af\u7247\u7ec4\u8fdb\u884c\u5206\u7c7b\u3002\u67e5\u770b\u5728\u60a8\u7684\u5f00\u53d1\u5957\u4ef6\u4e0a\u8fd0\u884c\u7684\u6a21\u578b\uff1a"}),"\n",(0,i.jsxs)(n.p,{children:["1\ufe0f\u20e3 \u524d\u5f80 ",(0,i.jsx)(n.a,{href:"https://aihub.qualcomm.com/iot/models",children:"model list"}),"\u3002",(0,i.jsx)(n.br,{}),"\n","2\ufe0f\u20e3 \u5728\u201c\u82af\u7247\u7ec4\u201d\u4e0b\uff0c\u9009\u62e9\uff1a"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"RB3 Gen 2 Vision Kit: 'Qualcomm QCS6490 (Proxy)'"}),"\n",(0,i.jsx)(n.li,{children:"RUBIK Pi 3: 'Qualcomm QCS6490 (Proxy)'"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"\u5c06\u6a21\u578b\u90e8\u7f72\u5230-npupython",children:"\u5c06\u6a21\u578b\u90e8\u7f72\u5230 NPU\uff08Python\uff09"}),"\n",(0,i.jsxs)(n.p,{children:["\u4ee5\u90e8\u7f72 ",(0,i.jsx)(n.a,{href:"https://aihub.qualcomm.com/iot/models/face_det_lite",children:"Lightweight-Face-Detection"})," \u6a21\u578b\u4e3a\u793a\u4f8b\u3002"]}),"\n",(0,i.jsx)(n.h3,{id:"\u8fd0\u884c\u793a\u4f8b\u4ee3\u7801\u5e93",children:"\u8fd0\u884c\u793a\u4f8b\u4ee3\u7801\u5e93"}),"\n",(0,i.jsxs)(n.p,{children:["\u6240\u6709 AI Hub \u6a21\u578b\u90fd\u9644\u5e26\u4e00\u4e2a\u793a\u4f8b\u4ee3\u7801\u5e93\u3002\u8fd9\u662f\u7406\u60f3\u7684\u5165\u95e8\u8d77\u70b9\u3002\u4ee3\u7801\u5e93\u51c6\u786e\u5730\u5c55\u793a\u4e86\u6a21\u578b",(0,i.jsx)(n.em,{children:"\u8fd0\u884c"}),"\u6d41\u7a0b\u3002\u5b83\u5c55\u793a\u4e86\u7f51\u7edc\u8f93\u5165\u7684\u8981\u6c42\uff0c\u4ee5\u53ca\u5982\u4f55\u89e3\u6790\u8f93\u51fa\uff08\u5982\uff0c\u5c06\u8f93\u51fa\u5f20\u91cf\u6620\u5c04\u5230\u8fb9\u754c\u6846\uff09\u3002\u793a\u4f8b\u4ee3\u7801\u5e93",(0,i.jsx)(n.strong,{children:"\u5c1a\u672a"}),"\u652f\u6301 NPU \u6216 GPU \u52a0\u901f\u3002\u5728\u5c06\u6b64\u6a21\u578b\u79fb\u81f3 NPU \u4e4b\u524d\uff0c\u6211\u4eec\u9996\u5148\u4e86\u89e3\u4e00\u4e0b\u6b63\u786e\u7684\u8f93\u5165/\u8f93\u51fa\u683c\u5f0f\u3002"]}),"\n",(0,i.jsxs)(n.p,{children:["\u5728 ",(0,i.jsx)(n.a,{href:"https://aihub.qualcomm.com/iot/models/face_det_lite",children:"Lightweight-Face-Detection"})," \u7684AI Hub\u9875\u9762\u4e0a\uff0c\u70b9\u51fb\u201cModel repository\u201d\uff0c\u8bbf\u95eeREADME\u6587\u4ef6\u3002\u8be5\u6587\u4ef6\u5305\u542b\u8fd0\u884c\u793a\u4f8b\u4ee3\u7801\u5e93\u7684\u5177\u4f53\u8bf4\u660e\u3002"]}),"\n",(0,i.jsx)(n.p,{children:"\u5982\u9700\u90e8\u7f72\u6b64\u6a21\u578b\uff0c\u8bf7\u5728\u5f00\u53d1\u677f\u4e0a\u6253\u5f00\u7ec8\u7aef\u6216\u5efa\u7acbSSH\u4f1a\u8bdd\uff0c\u5e76\u6267\u884c\u4ee5\u4e0b\u64cd\u4f5c\uff1a"}),"\n",(0,i.jsx)(n.p,{children:"1\ufe0f\u20e3 \u521b\u5efa\u4e00\u4e2a\u65b0\u7684\u865a\u62df\u73af\u5883\uff08venv\uff09\u5e76\u5b89\u88c5\u4e00\u4e9b\u57fa\u7840\u5305\uff1a"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"mkdir -p ~/aihub-demo\ncd ~/aihub-demo\n\npython3 -m venv .venv\nsource .venv/bin/activate\n\npip3 install numpy setuptools Cython shapely\n"})}),"\n",(0,i.jsx)(n.p,{children:"2\ufe0f\u20e3 \u5c06\u4e00\u5f20\u5e26\u6709\u4eba\u8138\u7684\u56fe\u50cf\uff08640x480 \u5206\u8fa8\u7387\uff0cJPG \u683c\u5f0f\uff09\u4e0b\u8f7d\u5230\u60a8\u7684\u5f00\u53d1\u677f\u4e0a\uff0c\u4f8b\u5982\uff1a"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"wget https://cdn.edgeimpulse.com/qc-ai-docs/example-images/three-people-640-480.jpg\n"})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{src:"https://3580193864-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxM5xrbdbelLSl7uN8oac%2Fuploads%2Fgit-blob-0cc3ca50bf1d29e11512b6bedb879ed101e8f7bb%2Faihub-three-people-in.jpg?",alt:"",title:"\u8f93\u5165\u6709\u4e09\u4e2a\u4eba\u7684\u56fe\u50cf"})}),"\n",(0,i.jsx)(n.admonition,{type:"warning",children:(0,i.jsxs)(n.p,{children:["**\u8f93\u5165\u5206\u8fa8\u7387\uff1a**AI Hub \u6a21\u578b\u5bf9\u8f93\u5165\u6570\u636e\u6709\u5c3a\u5bf8\u8981\u6c42\u3002\u53ef\u4ee5\u5728\u201c\u6280\u672f\u7ec6\u8282 > \u8f93\u5165\u5206\u8fa8\u7387\u201d\u4e0b\u627e\u5230\u6240\u9700\u7684\u5206\u8fa8\u7387\uff08",(0,i.jsx)(n.em,{children:"\u9ad8\u5ea6 x \u5bbd\u5ea6"}),"\uff0c\u6b64\u5904 480x640 => 640x480 \u4e3a\u5bbd x \u9ad8\uff09\uff09\u3002\u4e5f\u53ef\u901a\u8fc7\u67e5\u770bTFLite\u6216ONNX\u6587\u4ef6\u4e2d\u8f93\u5165\u5f20\u91cf\u7684\u5c3a\u5bf8\u83b7\u53d6\u8be5\u4fe1\u606f\u3002"]})}),"\n",(0,i.jsx)(n.p,{children:"3\ufe0f\u20e3 \u6309Facial Landmark Detection\u6a21\u578b\u7684\u201cExample & Usage\u201d\u7ae0\u8282\u4e0b\u7684\u8bf4\u660e\u8fdb\u884c\u64cd\u4f5c\uff1a"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:'# Install the example (add --no-build-isolation)\npip3 install --no-build-isolation "qai-hub-models[face-det-lite]"\n\n# Run the example\n#    Use --help to see all options\npython3 -m qai_hub_models.models.face_det_lite.demo --quantize w8a8 --image ./three-people-640-480.jpg --output-dir out/\n'})}),"\n",(0,i.jsxs)(n.p,{children:["\u5728 ",(0,i.jsx)(n.code,{children:"out/FaceDetLitebNet_output.png"})," \u4e2d\u627e\u5230\u8f93\u51fa\u56fe\u50cf\u3002"]}),"\n",(0,i.jsx)(n.p,{children:"\u5982\u679c\u901a\u8fc7 SSH \u8fde\u63a5\uff0c\u5219\u53ef\u4ee5\u901a\u8fc7\u4ee5\u4e0b\u65b9\u5f0f\u5c06\u8f93\u51fa\u56fe\u50cf\u590d\u5236\u56de\u4e3b\u673a\uff1a"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# Find IP via: ifconfig | grep -Eo 'inet (addr:)?([0-9]*\\.){3}[0-9]*' | grep -Eo '([0-9]*\\.){3}[0-9]*' | grep -v '127.0.0.1'\n# Then: (replace 192.168.1.148 by the IP address of your development kit)\n\nscp ubuntu@192.168.1.148:~/aihub-demo/out/FaceDetLitebNet_output.png ~/Downloads/FaceDetLitebNet_output.png\n"})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.img,{src:"https://3580193864-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxM5xrbdbelLSl7uN8oac%2Fuploads%2Fgit-blob-a7485166f25f0813e5ef1b94a8654353f644a8f7%2Faihub-three-people-annotated.png?alt=media",alt:"",title:"\u8f7b\u91cf\u7ea7\u4eba\u8138\u68c0\u6d4b\u8f93\u51fa"}),"4\ufe0f\u20e3 \u597d\u7684\uff0c\u73b0\u5728\u6211\u4eec\u5df2\u7ecf\u62e5\u6709\u4e86\u4e00\u4e2a\u53ef\u8fd0\u884c\u7684\u6a21\u578b\u3002\u4f8b\u5982\uff0c\u5728 RB3 Gen 2 Vision Kit \u4e0a\uff0c\u8fd0\u884c\u6b64\u6a21\u578b\u6bcf\u6b21\u63a8\u7406\u9700\u8981 189.7 \u6beb\u79d2\u3002"]}),"\n",(0,i.jsx)(n.h3,{id:"\u5c06\u6a21\u578b\u79fb\u690d\u5230npu",children:"\u5c06\u6a21\u578b\u79fb\u690d\u5230NPU"}),"\n",(0,i.jsx)(n.p,{children:"\u81f3\u6b64\uff0c\u6211\u4eec\u5df2\u83b7\u5f97\u4e00\u4e2a\u53ef\u8fd0\u884c\u7684\u53c2\u8003\u6a21\u578b\uff0c\u63a5\u4e0b\u6765\u5c06\u5728NPU\u4e0a\u8fd0\u884c\u8be5\u6a21\u578b\u3002\u60a8\u9700\u8981\u5b8c\u6210\u4ee5\u4e0b\u4e09\u4e2a\u90e8\u5206\u3002"}),"\n",(0,i.jsxs)(n.p,{children:["1\ufe0f\u20e3 \u5bf9\u6570\u636e\u8fdb\u884c\u9884\u5904\u7406\uff0c\u4f8b\u5982\u5c06\u56fe\u50cf\u8f6c\u6362\u4e3a\u53ef\u4ee5\u4f20\u9012\u7ed9\u795e\u7ecf\u7f51\u7edc\u7684\u7279\u5f81\u3002",(0,i.jsx)(n.br,{}),"\n","2\ufe0f\u20e3 \u5c06\u6a21\u578b\u5bfc\u51fa\u4e3a ONNX \u6216\u8005 TFLite\uff0c\u5e76\u901a\u8fc7 ",(0,i.jsx)(n.a,{href:"https://qc-ai-test.gitbook.io/qc-ai-test-docs/running-building-ai-models/lite-rt",children:"LiteRT"})," or ",(0,i.jsx)(n.a,{href:"https://qc-ai-test.gitbook.io/qc-ai-test-docs/running-building-ai-models/onnxruntime",children:"ONNX Runtime"})," \u8fd0\u884c\u6a21\u578b\u3002",(0,i.jsx)(n.br,{}),"\n","3\ufe0f\u20e3 \u5bf9\u8f93\u51fa\u8fdb\u884c\u540e\u5904\u7406\uff0c\u4f8b\u5982\u5c06\u795e\u7ecf\u7f51\u7edc\u7684\u8f93\u51fa\u8f6c\u6362\u4e3a\u9762\u90e8\u7684\u8fb9\u754c\u6846\u3002"]}),"\n",(0,i.jsxs)(n.p,{children:["\u8be5\u6a21\u578b\u8f83\u4e3a\u76f4\u89c2\uff0c\u53ef\u901a\u8fc7 ",(0,i.jsx)(n.a,{href:"https://qc-ai-test.gitbook.io/qc-ai-test-docs/running-building-ai-models/lite-rt",children:"LiteRT"})," \u548c ",(0,i.jsx)(n.a,{href:"https://qc-ai-test.gitbook.io/qc-ai-test-docs/running-building-ai-models/onnxruntime",children:"ONNX Runtime"})," \u8fdb\u884c\u4e86\u89e3\u3002\u7136\u800c\uff0c\u6570\u636e\u9884\u5904\u7406\u4e0e\u8f93\u51fa\u540e\u5904\u7406\u7684\u4ee3\u7801\u53ef\u80fd\u5e76\u975e\u5982\u6b64\u7b80\u5355\u3002"]}),"\n",(0,i.jsx)(n.h4,{id:"\u9884\u5904\u7406\u8f93\u5165",children:"\u9884\u5904\u7406\u8f93\u5165"}),"\n",(0,i.jsxs)(n.p,{children:["\u5bf9\u4e8e\u56fe\u50cf\u6a21\u578b\uff0c\u5927\u591a\u6570AI Hub\u6a21\u578b\u63a5\u53d7\u7684\u8f93\u5165\u77e9\u9635\u683c\u5f0f\u4e3a ",(0,i.jsx)(n.code,{children:"(HEIGHT, WIDTH, CHANNELS)"})," (LiteRT) \u6216 ",(0,i.jsx)(n.code,{children:"(CHANNELS, HEIGHT, WIDTH)"})," (ONNX)\uff0c\u4e14\u6570\u503c\u9700\u5f52\u4e00\u5316\u81f30\u52301\u4e4b\u95f4\u3002\u82e5\u8f93\u5165\u4e3a\u5355\u901a\u9053\u56fe\u50cf\uff0c\u9700\u5148\u5c06\u5176\u8f6c\u6362\u4e3a\u7070\u5ea6\u56fe\u3002\u82e5\u6a21\u578b\u5df2\u91cf\u5316\uff08\u7edd\u5927\u591a\u6570\u60c5\u51b5\u5982\u6b64\uff09\uff0c\u8fd8\u9700\u8bfb\u53d6zero_point\u548cscale\uff0c\u5e76\u5bf9\u50cf\u7d20\u503c\u8fdb\u884c\u76f8\u5e94\u7f29\u653e\uff08\u5728LiteRT\u4e2d\u64cd\u4f5c\u8f83\u4e3a\u7b80\u4fbf\uff0c\u56e0\u5176\u5185\u7f6e\u91cf\u5316\u53c2\u6570\uff0c\u800cONNX\u5219\u4e0d\u5305\u542b\u8fd9\u4e9b\u53c2\u6570\uff09\u3002\u901a\u5e38\u91cf\u5316\u6a21\u578b\u7684\u8f93\u5165\u6570\u636e\u4f1a\u7ebf\u6027\u7f29\u653e\u81f30..255\uff08uint8\uff09\u6216-128..127\uff08int8\uff09\u8303\u56f4\u3002\u8fd9\u90e8\u5206\u5904\u7406\u76f8\u5bf9\u7b80\u5355\u3002\u4e0b\u9762\u793a\u4f8b\u4ee3\u7801\u4e2d\u7684\uff08",(0,i.jsx)(n.code,{children:"def load_image_litert"}),"\uff09\u51fd\u6570\u5c55\u793a\u4e86\u5b8c\u6574\u7684Python\u5b9e\u73b0\u3002"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.em,{children:"\u7136\u800c\u9700\u8981\u7279\u522b\u6ce8\u610f\u7684\u662f"}),"\u4e0a\u8ff0\u5904\u7406\u65b9\u5f0f\u5e76\u975e\u7edd\u5bf9\u901a\u7528\uff0c\u6b63\u56e0\u5982\u6b64\u624d\u9700\u8981\u53c2\u8003AI Hub\u793a\u4f8b\u4ee3\u7801\u3002\u6bcf\u4e2a\u793a\u4f8b\u90fd\u63d0\u4f9b\u4e86\u7cbe\u786e\u7684\u8f93\u5165\u7f29\u653e\u5b9e\u73b0\u4ee3\u7801\u3002\u5728\u6211\u4eec\u5f53\u524d\u7684\u793a\u4f8bLightweight-Face-Detection\u4e2d\uff0c\u8f93\u5165\u7684\u5f20\u91cf\u7ef4\u5ea6\u4e3a",(0,i.jsx)(n.code,{children:"(480, 640, 1)"}),"\u3002\u4f46\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c",(0,i.jsx)(n.a,{href:"https://github.com/quic/ai-hub-models/blob/8cdeb11df6cc835b9b0b0cf9b602c7aa83ebfaf8/qai_hub_models/models/face_det_lite/app.py#L70",children:"\u9884\u5904\u7406\u4ee3\u7801"}),"\u5e76\u672a\u5c06\u56fe\u50cf\u8f6c\u6362\u4e3a\u7070\u5ea6\u56fe\uff0c\u800c\u662f\u76f4\u63a5\u63d0\u53d6\u4e86RGB\u56fe\u50cf\u7684\u84dd\u8272\u901a\u9053\uff1a"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'img_array = img_array.astype("float32") / 255.0\nimg_array = img_array[np.newaxis, ...]\nimg_tensor = torch.Tensor(img_array)\nimg_tensor = img_tensor[:, :, :, -1]        # HERE WE TAKE BLUE CHANNEL, NOT CONVERT TO GRAYSCALE\n'})}),"\n",(0,i.jsx)(n.p,{children:"\u8fd9\u7c7b\u7ec6\u8282\u6781\u6613\u51fa\u9519\u3002\u56e0\u6b64\uff0c\u5f53\u60a8\u53d1\u73b0\u81ea\u5df1\u7684\u5b9e\u73b0\u4e0eAI Hub\u793a\u4f8b\u7ed3\u679c\u4e0d\u4e00\u81f4\u65f6\uff1a\u8bf7\u4ed4\u7ec6\u9605\u8bfb\u793a\u4f8b\u4ee3\u7801\u3002\u5bf9\u4e8e\u975e\u56fe\u50cf\u7c7b\u8f93\u5165\uff08\u5982\u97f3\u9891\u6570\u636e\uff09\uff0c\u8fd9\u4e00\u539f\u5219\u66f4\u4e3a\u91cd\u8981\u3002\u8bf7\u52a1\u5fc5\u901a\u8fc7\u6f14\u793a\u4ee3\u7801\u6765\u7406\u89e3\u6a21\u578b\u771f\u6b63\u7684\u8f93\u5165\u9884\u671f\u3002"}),"\n",(0,i.jsx)(n.h4,{id:"\u540e\u5904\u7406\u8f93\u51fa",children:"\u540e\u5904\u7406\u8f93\u51fa"}),"\n",(0,i.jsxs)(n.p,{children:["\u8fd9\u540c\u6837\u9002\u7528\u4e8e\u540e\u5904\u7406\u3002\u4f8b\u5982\uff0c\u5c06\u795e\u7ecf\u7f51\u7edc\u7684\u8f93\u51fa\u6620\u5c04\u5230\u9762\u90e8\u68c0\u6d4b\u8fb9\u754c\u6846\u5e76\u4e0d\u5b58\u5728\u7edf\u4e00\u7684\u65b9\u6cd5\u3002\u5bf9\u4e8e Lightweight-Face-Detection\uff0c\u53ef\u4ee5\u5728",(0,i.jsx)(n.a,{href:"https://github.com/quic/ai-hub-models/blob/8cdeb11df6cc835b9b0b0cf9b602c7aa83ebfaf8/qai_hub_models/models/face_det_lite/app.py#L77",children:"face_det_lite/app.py#L77"}),"\u67e5\u770b\u5177\u4f53\u4ee3\u7801\u3002"]}),"\n",(0,i.jsx)(n.p,{children:"\u5982\u679c\u60a8\u7684\u76ee\u6807\u5e73\u53f0\u662f Python\uff0c\u90a3\u4e48\u6700\u7b80\u5355\u7684\u65b9\u6cd5\u662f\u5c06\u540e\u5904\u7406\u4ee3\u7801\u590d\u5236\u5230\u60a8\u7684\u5e94\u7528\u7a0b\u5e8f\u4e2d\uff0c\u56e0\u4e3a AI Hub \u5305\u542b\u5f88\u591a\u60a8\u53ef\u80fd\u4e0d\u60f3\u8981\u7684\u4f9d\u8d56\u9879\u3002\u6b64\u5916\u9700\u8981\u6ce8\u610f\uff0cAI Hub\u7684\u540e\u5904\u7406\u4ee3\u7801\u57fa\u4e8ePyTorch\u5f20\u91cf\u8fd0\u884c\uff0c\u800c\u60a8\u7684\u63a8\u7406\u73af\u5883\u662fLiteRT\u6216ONNX Runtime\uff0c\u56e0\u6b64\u9700\u8981\u5bf9\u5176\u4e2d\u7684\u90e8\u5206\u7ec6\u8282\u8fdb\u884c\u8c03\u6574\u3002\u6211\u4eec\u5c06\u5728\u4e0b\u9762\u7684\u7aef\u5230\u7aef\u793a\u4f8b\u4e2d\u5177\u4f53\u6f14\u793a\u8fd9\u4e00\u70b9\u3002"}),"\n",(0,i.jsx)(n.h3,{id:"\u7aef\u5230\u7aef\u793a\u4f8bpython",children:"\u7aef\u5230\u7aef\u793a\u4f8b\uff08Python\uff09"}),"\n",(0,i.jsx)(n.p,{children:"\u7ecf\u8fc7\u4ee5\u4e0a\u8bf4\u660e\uff0c\u63a5\u4e0b\u6765\u8ba9\u6211\u4eec\u67e5\u770b\u5177\u4f53\u4ee3\u7801\u5b9e\u73b0\u3002"}),"\n",(0,i.jsx)(n.p,{children:"1\ufe0f\u20e3 \u5728\u5f00\u53d1\u677f\u4e0a\u6253\u5f00\u4e00\u4e2a\u7ec8\u7aef\uff0c\u5e76\u8bbe\u7f6e\u6b64\u793a\u4f8b\u7684\u57fa\u672c\u73af\u5883\uff1a"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# Create a new fresh directory\nmkdir -p ~/aihub-npu\ncd ~/aihub-npu\n\n# Create a new venv\npython3 -m venv .venv\nsource .venv/bin/activate\n\n# Install the LiteRT runtime (to run models) and Pillow (to parse images)\npip3 install ai-edge-litert==1.3.0 Pillow\n\n# Download an example image\nwget https://cdn.edgeimpulse.com/qc-ai-docs/example-images/three-people-640-480.jpg\n"})}),"\n",(0,i.jsx)(n.p,{children:"2\ufe0f\u20e3 NPU\u4ec5\u652f\u6301uint8/int8\u91cf\u5316\u6a21\u578b\u3002\u503c\u5f97\u5e86\u5e78\u7684\u662f\uff0cAI Hub \u5df2\u63d0\u4f9b\u9884\u91cf\u5316\u4e14\u4f18\u5316\u8fc7\u7684\u6a21\u578b\u3002\u60a8\u53ef\u4ee5\uff1a"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"\u4e0b\u8f7d\u672c\u6559\u7a0b\u7684\u6a21\u578b\uff08\u5728 CDN \u4e0a\u955c\u50cf\uff09\uff1a"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"wget https://cdn.edgeimpulse.com/qc-ai-docs/models/face_det_lite-lightweight-face-detection-w8a8.tflite\n"})}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:["\u6216\u901a\u8fc7\u4ee5\u4e0b\u6b65\u9aa4\u4e0b\u8f7d\u4efb\u610fAI Hub\u6a21\u578b\u5e76\u63a8\u9001\u81f3\u5f00\u53d1\u677f\uff1aa. \u8bbf\u95ee",(0,i.jsx)(n.a,{href:"https://aihub.qualcomm.com/iot/models/face_det_lite",children:"Lightweight-Face-Detection"}),"\u3002b. \u70b9\u51fb\u201cDownload model\u201d\u3002c. \u9009\u62e9\u201cTFLite\u201d\u4e3a\u8fd0\u884c\u65f6\uff0c\u201cw8a8\u201d\u4e3a\u7cbe\u5ea6\u3002"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{src:"https://3580193864-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxM5xrbdbelLSl7uN8oac%2Fuploads%2Fgit-blob-a402e668a1082b6c0fc8dfdbe2cec20f204dee2d%2Faihub-download.png?alt=media",alt:"",title:"\u4ece AI Hub \u4e0b\u8f7d TFLite \u683c\u5f0f\u7684 w8a8 \u91cf\u5316\u6a21\u578b"})}),"\n",(0,i.jsxs)(n.p,{children:["\u5982\u679c\u60a8\u7684\u6a21\u578b\u4ec5\u9002\u7528\u4e8e ONNX \u683c\u5f0f\uff0c\u8bf7\u4ece",(0,i.jsx)(n.a,{href:"https://qc-ai-test.gitbook.io/qc-ai-test-docs/running-building-ai-models/onnxruntime",children:"Run models using ONNX Runtime"}),"\u83b7\u53d6\u8bf4\u660e\u3002\u539f\u7406\u4e0e\u672c\u6559\u7a0b\u76f8\u540c\u3002d. \u4e0b\u8f7d\u6a21\u578b\u3002e. \u5982\u679c\u60a8\u6ca1\u6709\u76f4\u63a5\u5728 Dragonwing \u5f00\u53d1\u677f\u4e0a\u4e0b\u8f7d\u6a21\u578b\uff0c\u5219\u9700\u8981\u901a\u8fc7 SSH \u63a8\u9001\u6a21\u578b\uff1ai. \u627e\u5230\u5f00\u53d1\u677f\u7684 IP \u5730\u5740\u3002\u5728\u60a8\u7684\u5f00\u53d1\u677f\u4e0a\u8fd0\u884c\uff1a"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"ifconfig | grep -Eo 'inet (addr:)?([0-9]*\\.){3}[0-9]*' | grep -Eo '([0-9]*\\.){3}[0-9]*' | grep -v '127.0.0.1'\n\n# ... Example:\n# 192.168.1.253\n"})}),"\n",(0,i.jsx)(n.p,{children:"ii.\u63a8\u9001 .tflite \u6587\u4ef6\u3002\u4ece\u60a8\u7684\u8ba1\u7b97\u673a\u8fd0\u884c\uff1a"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"scp face_det_lite-lightweight-face-detection-w8a8.tflite ubuntu@192.168.1.253:~/face_det_lite-lightweight-face-detection-w8a8.tflite\n"})}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:["3\ufe0f\u20e3 \u521b\u5efa\u4e00\u4e2a\u65b0\u6587\u4ef6 ",(0,i.jsx)(n.code,{children:"face_detection.py"}),"\u3002\u8be5\u6587\u4ef6\u5305\u542b\u6a21\u578b\u8c03\u7528\uff0c\u4ee5\u53ca\u6765\u81ea AI Hub \u793a\u4f8b\u7684\u9884\u5904\u7406\u548c\u540e\u5904\u7406\u4ee3\u7801\uff08\u53c2\u89c1\u5185\u8054\u6ce8\u91ca\uff09\u3002"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import numpy as np\nfrom ai_edge_litert.interpreter import Interpreter, load_delegate\nfrom PIL import Image, ImageDraw\nimport os, time, sys\n\ndef curr_ms():\n    return round(time.time() * 1000)\n\n# Paths\nIMAGE_IN = \'three-people-640-480.jpg\'\nIMAGE_OUT = \'three-people-640-480-overlay.jpg\'\nMODEL_PATH = \'face_det_lite-lightweight-face-detection-w8a8.tflite\'\n\n# If we pass in --use-qnn we offload to NPU\nuse_qnn = True if len(sys.argv) >= 2 and sys.argv[1] == \'--use-qnn\' else False\n\nexperimental_delegates = []\nif use_qnn:\n    experimental_delegates = [load_delegate("libQnnTFLiteDelegate.so", options={"backend_type":"htp"})]\n\n# Load TFLite model and allocate tensors\ninterpreter = Interpreter(\n    model_path=MODEL_PATH,\n    experimental_delegates=experimental_delegates\n)\ninterpreter.allocate_tensors()\n\n# Get input and output tensor details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# === BEGIN PREPROCESSING ===\n\n# Load an image (using Pillow) and make it in the right format that the interpreter expects (e.g. quantize)\n# All AI Hub image models use 0..1 inputs to start.\ndef load_image_litert(interpreter, path, single_channel_behavior: str = \'grayscale\'):\n    d = interpreter.get_input_details()[0]\n    shape = [int(x) for x in d["shape"]]  # e.g. [1, H, W, C] or [1, C, H, W]\n    dtype = d["dtype"]\n    scale, zp = d.get("quantization", (0.0, 0))\n\n    if len(shape) != 4 or shape[0] != 1:\n        raise ValueError(f"Unexpected input shape: {shape}")\n\n    # Detect layout\n    if shape[1] in (1, 3):   # [1, C, H, W]\n        layout, C, H, W = "NCHW", shape[1], shape[2], shape[3]\n    elif shape[3] in (1, 3): # [1, H, W, C]\n        layout, C, H, W = "NHWC", shape[3], shape[1], shape[2]\n    else:\n        raise ValueError(f"Cannot infer layout from shape {shape}")\n\n    # Load & resize\n    img = Image.open(path).convert("RGB").resize((W, H), Image.BILINEAR)\n    arr = np.array(img)\n    if C == 1:\n        if single_channel_behavior == \'grayscale\':\n            # Convert to luminance (H, W)\n            gray = np.asarray(Image.fromarray(arr).convert(\'L\'))\n        elif single_channel_behavior in (\'red\', \'green\', \'blue\'):\n            ch_idx = {\'red\': 0, \'green\': 1, \'blue\': 2}[single_channel_behavior]\n            gray = arr[:, :, ch_idx]\n        else:\n            raise ValueError(f"Invalid single_channel_behavior: {single_channel_behavior}")\n        # Keep shape as HWC with C=1\n        arr = gray[..., np.newaxis]\n\n    # HWC -> correct layout\n    if layout == "NCHW":\n        arr = np.transpose(arr, (2, 0, 1))  # (C,H,W)\n\n    # Scale 0..1 (all AI Hub image models use this)\n    arr = (arr / 255.0).astype(np.float32)\n\n    # Quantize if needed\n    if scale and float(scale) != 0.0:\n        q = np.rint(arr / float(scale) + int(zp))\n        if dtype == np.uint8:\n            arr = np.clip(q, 0, 255).astype(np.uint8)\n        else:\n            arr = np.clip(q, -128, 127).astype(np.int8)\n\n    return np.expand_dims(arr, 0)  # add batch\n\n# This model looks like grayscale, but AI Hub inference actually takes the BLUE channel\n# see https://github.com/quic/ai-hub-models/blob/8cdeb11df6cc835b9b0b0cf9b602c7aa83ebfaf8/qai_hub_models/models/face_det_lite/app.py#L70\ninput_data = load_image_litert(interpreter, IMAGE_IN, single_channel_behavior=\'blue\')\n\n# === END PREPROCESSING (input_data contains right data) ===\n\n# Set tensor and run inference\ninterpreter.set_tensor(input_details[0][\'index\'], input_data)\n\n# Run once to warmup\ninterpreter.invoke()\n\n# Then run 10x\nstart = curr_ms()\nfor i in range(0, 10):\n    interpreter.invoke()\nend = curr_ms()\n\n# === BEGIN POSTPROCESSING ===\n\n# Grab 3 output tensors and dequantize\nq_output_0 = interpreter.get_tensor(output_details[0][\'index\'])\nscale_0, zero_point_0 = output_details[0][\'quantization\']\nhm = ((q_output_0.astype(np.float32) - zero_point_0) * scale_0)[0]\n\nq_output_1 = interpreter.get_tensor(output_details[1][\'index\'])\nscale_1, zero_point_1 = output_details[1][\'quantization\']\nbox = ((q_output_1.astype(np.float32) - zero_point_1) * scale_1)[0]\n\nq_output_2 = interpreter.get_tensor(output_details[2][\'index\'])\nscale_2, zero_point_2 = output_details[2][\'quantization\']\nlandmark = ((q_output_2.astype(np.float32) - zero_point_2) * scale_2)[0]\n\n# Taken from https://github.com/quic/ai-hub-models/blob/8cdeb11df6cc835b9b0b0cf9b602c7aa83ebfaf8/qai_hub_models/utils/bounding_box_processing.py#L369\ndef get_iou(boxA: np.ndarray, boxB: np.ndarray) -> float:\n    """\n    Given two tensors of shape (4,) in xyxy format,\n    compute the iou between the two boxes.\n    """\n    xA = max(boxA[0], boxB[0])\n    yA = max(boxA[1], boxB[1])\n    xB = min(boxA[2], boxB[2])\n    yB = min(boxA[3], boxB[3])\n\n    inter_area = max(0, xB - xA + 1) * max(0, yB - yA + 1)\n    boxA_area = (boxA[2] - boxA[0] + 1) * (boxA[3] - boxA[1] + 1)\n    boxB_area = (boxB[2] - boxB[0] + 1) * (boxB[3] - boxB[1] + 1)\n\n    return inter_area / float(boxA_area + boxB_area - inter_area)\n\n# Taken from https://github.com/quic/ai-hub-models/blob/8cdeb11df6cc835b9b0b0cf9b602c7aa83ebfaf8/qai_hub_models/models/face_det_lite/utils.py\nclass BBox:\n    # Bounding Box\n    def __init__(\n        self,\n        label: str,\n        xyrb: list[int],\n        score: float = 0,\n        landmark: list | None = None,\n        rotate: bool = False,\n    ):\n        """\n        A bounding box plus landmarks structure to hold the hierarchical result.\n        parameters:\n            label:str the class label\n            xyrb: 4 list for bbox left, top,  right bottom coordinates\n            score:the score of the detection\n            landmark: 10x2 the landmark of the joints [[x1,y1], [x2,y2]...]\n        """\n        self.label = label\n        self.score = score\n        self.landmark = landmark\n        self.x, self.y, self.r, self.b = xyrb\n        self.rotate = rotate\n\n        minx = min(self.x, self.r)\n        maxx = max(self.x, self.r)\n        miny = min(self.y, self.b)\n        maxy = max(self.y, self.b)\n        self.x, self.y, self.r, self.b = minx, miny, maxx, maxy\n\n    @property\n    def width(self) -> int:\n        return self.r - self.x + 1\n\n    @property\n    def height(self) -> int:\n        return self.b - self.y + 1\n\n    @property\n    def box(self) -> list[int]:\n        return [self.x, self.y, self.r, self.b]\n\n    @box.setter\n    def box(self, newvalue: list[int]) -> None:\n        self.x, self.y, self.r, self.b = newvalue\n\n    @property\n    def haslandmark(self) -> bool:\n        return self.landmark is not None\n\n    @property\n    def xywh(self) -> list[int]:\n        return [self.x, self.y, self.width, self.height]\n\n# Taken from https://github.com/quic/ai-hub-models/blob/8cdeb11df6cc835b9b0b0cf9b602c7aa83ebfaf8/qai_hub_models/models/face_det_lite/utils.py\ndef nms(objs: list[BBox], iou: float = 0.5) -> list[BBox]:\n    """\n    nms function customized to work on the BBox objects list.\n    parameter:\n        objs: the list of the BBox objects.\n    return:\n        the rest of the BBox after nms operation.\n    """\n    if objs is None or len(objs) <= 1:\n        return objs\n\n    objs = sorted(objs, key=lambda obj: obj.score, reverse=True)\n    keep = []\n    flags = [0] * len(objs)\n    for index, obj in enumerate(objs):\n        if flags[index] != 0:\n            continue\n\n        keep.append(obj)\n        for j in range(index + 1, len(objs)):\n            # if flags[j] == 0 and obj.iou(objs[j]) > iou:\n            if (\n                flags[j] == 0\n                and get_iou(np.array(obj.box), np.array(objs[j].box)) > iou\n            ):\n                flags[j] = 1\n    return keep\n\n# Ported from https://github.com/quic/ai-hub-models/blob/8cdeb11df6cc835b9b0b0cf9b602c7aa83ebfaf8/qai_hub_models/models/face_det_lite/utils.py#L110\n# The original code uses torch.Tensor, this uses native numpy arrays\ndef detect(\n    hm: np.ndarray,           # (H, W, 1), float32\n    box: np.ndarray,          # (H, W, 4), float32\n    landmark: np.ndarray,     # (H, W, 10), float32\n    threshold: float = 0.2,\n    nms_iou: float = 0.2,\n    stride: int = 8,\n) -> list:\n    def _sigmoid(x: np.ndarray) -> np.ndarray:\n        # stable-ish sigmoid\n        out = np.empty_like(x, dtype=np.float32)\n        np.negative(x, out=out)\n        np.exp(out, out=out)\n        out += 1.0\n        np.divide(1.0, out, out=out)\n        return out\n\n    def _maxpool3x3_same(x_hw: np.ndarray) -> np.ndarray:\n        """\n        x_hw: (H, W) single-channel array.\n        3x3 max pool, stride=1, padding=1 (same as PyTorch F.max_pool2d(kernel=3,stride=1,padding=1))\n        Pure NumPy using stride tricks.\n        """\n        H, W = x_hw.shape\n        # pad with -inf so edges don\'t borrow smaller values\n        pad = 1\n        xpad = np.pad(x_hw, ((pad, pad), (pad, pad)), mode=\'constant\', constant_values=-np.inf)\n\n        # build 3x3 sliding windows using as_strided\n        s0, s1 = xpad.strides\n        shape = (H, W, 3, 3)\n        strides = (s0, s1, s0, s1)\n        windows = np.lib.stride_tricks.as_strided(xpad, shape=shape, strides=strides, writeable=False)\n        # max over the 3x3 window\n        return windows.max(axis=(2, 3))\n\n    def _topk_desc(values_flat: np.ndarray, k: int):\n        """Return (topk_values_sorted, topk_indices_sorted_desc)."""\n        if k <= 0:\n            return np.array([], dtype=values_flat.dtype), np.array([], dtype=np.int64)\n        k = min(k, values_flat.size)\n        # argpartition for top-k by value\n        idx_part = np.argpartition(-values_flat, k - 1)[:k]\n        # sort those k by value desc\n        order = np.argsort(-values_flat[idx_part])\n        idx_sorted = idx_part[order]\n        return values_flat[idx_sorted], idx_sorted\n\n    # 1) sigmoid heatmap\n    hm = _sigmoid(hm.astype(np.float32, copy=False))\n\n    # squeeze channel -> (H, W)\n    hm_hw = hm[..., 0]\n\n    # 2) 3x3 max-pool same\n    hm_pool = _maxpool3x3_same(hm_hw)\n\n    # 3) local maxima mask (keep equal to pooled)\n    # (like (hm == hm_pool).float() * hm in torch)\n    keep = (hm_hw >= hm_pool)  # >= to keep plateaus, mirrors torch equality on floats closely enough\n    candidate_scores = np.where(keep, hm_hw, 0.0).ravel()\n\n    # 4) topk up to 2000\n    num_candidates = int(keep.sum())\n    k = min(num_candidates, 2000)\n    scores_k, flat_idx_k = _topk_desc(candidate_scores, k)\n\n    H, W = hm_hw.shape\n    ys = (flat_idx_k // W).astype(np.int32)\n    xs = (flat_idx_k %  W).astype(np.int32)\n\n    # 5) gather boxes/landmarks and build outputs\n    objs = []\n    for cx, cy, score in zip(xs, ys, scores_k):\n        if score < threshold:\n            # because scores_k is sorted desc, we can break\n            break\n\n        # box offsets at (cy, cx): [x, y, r, b]\n        x, y, r, b = box[cy, cx].astype(np.float32, copy=False)\n\n        # convert to absolute xyrb in pixels (same math as torch code)\n        cxcycxcy = np.array([cx, cy, cx, cy], dtype=np.float32)\n        xyrb = (cxcycxcy + np.array([-x, -y,  r,  b], dtype=np.float32)) * float(stride)\n        xyrb = xyrb.astype(np.int32, copy=False).tolist()\n\n        # landmarks: first 5 x, next 5 y\n        x5y5 = landmark[cy, cx].astype(np.float32, copy=False)\n        x5y5 = x5y5 + np.array([cx]*5 + [cy]*5, dtype=np.float32)\n        x5y5 *= float(stride)\n        box_landmark = list(zip(x5y5[:5].tolist(), x5y5[5:].tolist()))\n\n        objs.append(BBox("0", xyrb=xyrb, score=float(score), landmark=box_landmark))\n\n    if nms_iou != -1:\n        return nms(objs, iou=nms_iou)\n    return objs\n\n# Detection code from https://github.com/quic/ai-hub-models/blob/8cdeb11df6cc835b9b0b0cf9b602c7aa83ebfaf8/qai_hub_models/models/face_det_lite/app.py#L77\ndets = detect(hm, box, landmark, threshold=0.55, nms_iou=-1, stride=8)\nres = []\nfor n in range(0, len(dets)):\n    xmin, ymin, w, h = dets[n].xywh\n    score = dets[n].score\n\n    L = int(xmin)\n    R = int(xmin + w)\n    T = int(ymin)\n    B = int(ymin + h)\n    W = int(w)\n    H = int(h)\n\n    if L < 0 or T < 0 or R >= 640 or B >= 480:\n        if L < 0:\n            L = 0\n        if T < 0:\n            T = 0\n        if R >= 640:\n            R = 640 - 1\n        if B >= 480:\n            B = 480 - 1\n\n    # Enlarge bounding box to cover more face area\n    b_Left = L - int(W * 0.05)\n    b_Top = T - int(H * 0.05)\n    b_Width = int(W * 1.1)\n    b_Height = int(H * 1.1)\n\n    if (\n        b_Left >= 0\n        and b_Top >= 0\n        and b_Width - 1 + b_Left < 640\n        and b_Height - 1 + b_Top < 480\n    ):\n        L = b_Left\n        T = b_Top\n        W = b_Width\n        H = b_Height\n        R = W - 1 + L\n        B = H - 1 + T\n\n    print(f\'Found face: x={L}, y={T}, w={W}, h={H}, score={score}\')\n\n    res.append([L, T, W, H, score])\n\n# === END POSTPROCESSING ===\n\n# Create new PIL image from the input data, stripping off the batch dim\ninput_reshaped = input_data.reshape(input_data.shape[1:])\nif input_reshaped.shape[2] == 1:\n    input_reshaped = np.squeeze(input_reshaped, axis=-1) # strip off the last dim if grayscale\n\n# And write to output image so we can debug\nimg_out = Image.fromarray(input_reshaped).convert("RGB")\ndraw = ImageDraw.Draw(img_out)\nfor bb in res:\n    L, T, W, H, score = bb\n    draw.rectangle([L, T, L + w, T + H], outline="#00FF00", width=3)\nimg_out.save(IMAGE_OUT)\n\nprint(\'\')\nprint(f\'Inference took (on average): {(end - start) / 10}ms. per image\')\n'})}),"\n",(0,i.jsx)(n.p,{children:"4\ufe0f\u20e3 \u5728 CPU \u4e0a\u8fd0\u884c\u6a21\u578b\uff1a"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"python3 face_detection.py\n\n# INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n# Found face: x=120, y=186, w=62, h=79, score=0.8306506276130676\n# Found face: x=311, y=125, w=66, h=81, score=0.8148472309112549\n# Found face: x=424, y=173, w=64, h=86, score=0.8093323111534119\n#\n# Inference took (on average): 35.6ms. per image\n"})}),"\n",(0,i.jsxs)(n.p,{children:["\u81f3\u6b64\uff0c\u6211\u4eec\u6bcf\u6b21\u63a8\u7406\u7684\u65f6\u95f4\u4ece 189.7 \u6beb\u79d2\u7f29\u77ed\u5230 35.6 \u6beb\u79d2\u3002",(0,i.jsx)(n.br,{}),"\n","5\ufe0f\u20e3 \u5728 NPU \u4e0a\u8fd0\u884c\u6a21\u578b\uff1a"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"python3 face_detection.py --use-qnn\n\n# INFO: TfLiteQnnDelegate delegate: 1382 nodes delegated out of 1633 nodes with 27 partitions.\n#\n# Found face: x=120, y=186, w=62, h=78, score=0.8255056142807007\n# Found face: x=311, y=125, w=66, h=81, score=0.8148472309112549\n# Found face: x=421, y=173, w=67, h=86, score=0.8093323111534119\n#\n# Inference took (on average): 2.4ms. per image\n"})}),"\n",(0,i.jsxs)(n.p,{children:["\u81f3\u6b64\u5df2\u5b8c\u6210\u4f18\u5316\u3002\u901a\u8fc7\u91cf\u5316\u8be5\u6a21\u578b\u5e76\u5c06\u5176\u79fb\u690d\u5230 NPU\uff0c\u6211\u4eec\u5c06\u6a21\u578b\u901f\u5ea6\u63d0\u9ad8\u4e86 79 \u500d\u3002\u76f8\u4fe1\u60a8\u5bf9 AI Hub \u7684\u8d44\u6e90\u4ef7\u503c\u4ee5\u53ca NPU \u548c AI Engine Direct SDK \u7684\u6f5c\u5728\u529f\u80fd\u6709\u5145\u5206\u7684\u4e86\u89e3\u3002\u60a8\u4e5f\u4e0d\u5c40\u9650\u4e8e\u4f7f\u7528 Python\u3002\u4f8b\u5982\uff0c ",(0,i.jsx)(n.a,{href:"https://qc-ai-test.gitbook.io/qc-ai-test-docs/running-building-ai-models/lite-rt",children:"LiteRT"})," \u4e5f\u63d0\u4f9b\u4e86C++\u7684\u8c03\u7528\u793a\u4f8b\u3002"]}),"\n",(0,i.jsx)(n.h2,{id:"\u5c06\u6a21\u578b\u90e8\u7f72\u5230-npuedge-impulse",children:"\u5c06\u6a21\u578b\u90e8\u7f72\u5230 NPU\uff08Edge Impulse\uff09"}),"\n",(0,i.jsx)(n.p,{children:"\u53ef\u4ee5\u901a\u8fc7 Edge Impulse \u90e8\u7f72\u56fe\u50cf\u5206\u7c7b\u3001\u89c6\u89c9\u56de\u5f52\u548c\u67d0\u4e9b\u5bf9\u8c61\u68c0\u6d4b\u6a21\u578b\u3002"})]})}function p(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(c,{...e})}):c(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>s,x:()=>r});var a=t(6540);const i={},o=a.createContext(i);function s(e){const n=a.useContext(o);return a.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:s(e.components),a.createElement(o.Provider,{value:n},e.children)}}}]);